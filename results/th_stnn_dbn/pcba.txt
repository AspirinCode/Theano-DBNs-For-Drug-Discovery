_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 0
RunningOn slot5@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 0
Running Theano Learn Deep Belief Net for pcba, target: aid1030.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -64.7780716934
Pre-training layer 0, epoch 1, cost  -50.6557432378
Pre-training layer 0, epoch 2, cost  -45.0959829024
Pre-training layer 0, epoch 3, cost  -41.4692298602
Pre-training layer 0, epoch 4, cost  -38.7967218797
Pre-training layer 0, epoch 5, cost  -36.684438359
Pre-training layer 0, epoch 6, cost  -34.9478955536
Pre-training layer 0, epoch 7, cost  -33.4502050856
Pre-training layer 1, epoch 0, cost  -1351.13234955
Pre-training layer 1, epoch 1, cost  -1347.43035215
Pre-training layer 1, epoch 2, cost  -1347.42905684
Pre-training layer 1, epoch 3, cost  -1347.42907083
Pre-training layer 1, epoch 4, cost  -1347.4287564
Pre-training layer 1, epoch 5, cost  -1347.42923414
Pre-training layer 1, epoch 6, cost  -1347.42850211
Pre-training layer 1, epoch 7, cost  -1347.42890208
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2352/2352, best error 48.750000 %, best_auc: 0.500000
Optimization complete with best validation score of 48.755102 %, obtained at iteration 2352, with test performance 48.750000 %, and best_auc: 0.500000
runtime: 267598.03 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 1
RunningOn slot4@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 1
Running Theano Learn Deep Belief Net for pcba, target: aid1379.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -69.7612142203
Pre-training layer 0, epoch 1, cost  -56.6078754833
Pre-training layer 0, epoch 2, cost  -51.0643064065
Pre-training layer 0, epoch 3, cost  -47.5135951824
Pre-training layer 0, epoch 4, cost  -44.8874321123
Pre-training layer 0, epoch 5, cost  -42.8049308941
Pre-training layer 0, epoch 6, cost  -41.099314203
Pre-training layer 0, epoch 7, cost  -39.6569651263
Pre-training layer 1, epoch 0, cost  -1353.16664771
Pre-training layer 1, epoch 1, cost  -1346.32458949
Pre-training layer 1, epoch 2, cost  -1346.30502323
Pre-training layer 1, epoch 3, cost  -1346.30463202
Pre-training layer 1, epoch 4, cost  -1346.30420658
Pre-training layer 1, epoch 5, cost  -1346.30402247
Pre-training layer 1, epoch 6, cost  -1346.30421167
Pre-training layer 1, epoch 7, cost  -1346.30383205
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1292/1292, best error 7.802326 %, best_auc: 0.500000
     new best AUC!: 0.500012594458
Optimization complete with best validation score of 7.800000 %, obtained at iteration 1292, with test performance 7.802326 %, and best_auc: 0.500013
runtime: 211870.21 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 10
RunningOn slot6@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 10
Running Theano Learn Deep Belief Net for pcba, target: aid1471.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -70.2025461028
Pre-training layer 0, epoch 1, cost  -57.1982860113
Pre-training layer 0, epoch 2, cost  -51.7385864821
Pre-training layer 0, epoch 3, cost  -48.1859366551
Pre-training layer 0, epoch 4, cost  -45.5797859741
Pre-training layer 0, epoch 5, cost  -43.4968590826
Pre-training layer 0, epoch 6, cost  -41.779623219
Pre-training layer 0, epoch 7, cost  -40.3239483304
Pre-training layer 1, epoch 0, cost  -1350.07639432
Pre-training layer 1, epoch 1, cost  -1343.22233897
Pre-training layer 1, epoch 2, cost  -1343.20917925
Pre-training layer 1, epoch 3, cost  -1343.20882947
Pre-training layer 1, epoch 4, cost  -1343.20878091
Pre-training layer 1, epoch 5, cost  -1343.20868542
Pre-training layer 1, epoch 6, cost  -1343.2086788
Pre-training layer 1, epoch 7, cost  -1343.20827971
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1393/1393, best error 3.750000 %, best_auc: 0.500000
Optimization complete with best validation score of 3.750000 %, obtained at iteration 1393, with test performance 3.750000 %, and best_auc: 0.500000
runtime: 184962.37 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 100
RunningOn slot11@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 100
Running Theano Learn Deep Belief Net for pcba, target: aid720579.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -68.5549754402
Pre-training layer 0, epoch 1, cost  -53.9710298188
Pre-training layer 0, epoch 2, cost  -48.0389621981
Pre-training layer 0, epoch 3, cost  -44.2620967424
Pre-training layer 0, epoch 4, cost  -41.4398408538
Pre-training layer 0, epoch 5, cost  -39.2191548029
Pre-training layer 0, epoch 6, cost  -37.3807258672
Pre-training layer 0, epoch 7, cost  -35.8430438682
Pre-training layer 1, epoch 0, cost  -1351.13117175
Pre-training layer 1, epoch 1, cost  -1347.09741942
Pre-training layer 1, epoch 2, cost  -1347.09630995
Pre-training layer 1, epoch 3, cost  -1347.09616716
Pre-training layer 1, epoch 4, cost  -1347.09617038
Pre-training layer 1, epoch 5, cost  -1347.09626708
Pre-training layer 1, epoch 6, cost  -1347.09607423
Pre-training layer 1, epoch 7, cost  -1347.09570327
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2172/2172, best error 15.785615 %, best_auc: 0.500000
Optimization complete with best validation score of 15.795297 %, obtained at iteration 2172, with test performance 15.785615 %, and best_auc: 0.500000
runtime: 36180.85 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 101
RunningOn slot3@opt-a010.discovery.wisc.edu
python th_deep_belief_net.py pcba 101
Running Theano Learn Deep Belief Net for pcba, target: aid720580.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -68.629009017
Pre-training layer 0, epoch 1, cost  -54.1107530883
Pre-training layer 0, epoch 2, cost  -48.2026931142
Pre-training layer 0, epoch 3, cost  -44.4425573428
Pre-training layer 0, epoch 4, cost  -41.6515488797
Pre-training layer 0, epoch 5, cost  -39.4769247719
Pre-training layer 0, epoch 6, cost  -37.6488491014
Pre-training layer 0, epoch 7, cost  -36.118339226
Pre-training layer 1, epoch 0, cost  -1350.5608093
Pre-training layer 1, epoch 1, cost  -1346.5833392
Pre-training layer 1, epoch 2, cost  -1346.58279176
Pre-training layer 1, epoch 3, cost  -1346.5824272
Pre-training layer 1, epoch 4, cost  -1346.58208489
Pre-training layer 1, epoch 5, cost  -1346.58278222
Pre-training layer 1, epoch 6, cost  -1346.58211157
Pre-training layer 1, epoch 7, cost  -1346.58218524
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2220/2220, best error 12.212449 %, best_auc: 0.500000
     new best AUC!: 0.506582490223
Optimization complete with best validation score of 12.197564 %, obtained at iteration 2220, with test performance 12.212449 %, and best_auc: 0.506582
runtime: 70559.94 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 102
RunningOn slot5@opt-a005.discovery.wisc.edu
python th_deep_belief_net.py pcba 102
Running Theano Learn Deep Belief Net for pcba, target: aid720707.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.3246511825
Pre-training layer 0, epoch 1, cost  -52.2117275088
Pre-training layer 0, epoch 2, cost  -46.5929642983
Pre-training layer 0, epoch 3, cost  -43.0385515601
Pre-training layer 0, epoch 4, cost  -40.3762216648
Pre-training layer 0, epoch 5, cost  -38.3155949551
Pre-training layer 0, epoch 6, cost  -36.6230328829
Pre-training layer 0, epoch 7, cost  -35.2066226029
Pre-training layer 1, epoch 0, cost  -1353.15521378
Pre-training layer 1, epoch 1, cost  -1349.46928727
Pre-training layer 1, epoch 2, cost  -1349.46816352
Pre-training layer 1, epoch 3, cost  -1349.46811426
Pre-training layer 1, epoch 4, cost  -1349.46782274
Pre-training layer 1, epoch 5, cost  -1349.46808697
Pre-training layer 1, epoch 6, cost  -1349.46783817
Pre-training layer 1, epoch 7, cost  -1349.467873
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2234/2234, best error 2.137097 %, best_auc: 0.500000
Optimization complete with best validation score of 2.137097 %, obtained at iteration 2234, with test performance 2.137097 %, and best_auc: 0.500000
runtime: 68011.16 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 103
RunningOn slot2@opt-a005.discovery.wisc.edu
python th_deep_belief_net.py pcba 103
Running Theano Learn Deep Belief Net for pcba, target: aid720708.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.0121492425
Pre-training layer 0, epoch 1, cost  -51.9897319146
Pre-training layer 0, epoch 2, cost  -46.3669109947
Pre-training layer 0, epoch 3, cost  -42.7913095879
Pre-training layer 0, epoch 4, cost  -40.1452432146
Pre-training layer 0, epoch 5, cost  -38.0902454965
Pre-training layer 0, epoch 6, cost  -36.3859706121
Pre-training layer 0, epoch 7, cost  -34.9141837063
Pre-training layer 1, epoch 0, cost  -1352.98258355
Pre-training layer 1, epoch 1, cost  -1349.39402166
Pre-training layer 1, epoch 2, cost  -1349.39290575
Pre-training layer 1, epoch 3, cost  -1349.39282009
Pre-training layer 1, epoch 4, cost  -1349.39251775
Pre-training layer 1, epoch 5, cost  -1349.39288781
Pre-training layer 1, epoch 6, cost  -1349.39242229
Pre-training layer 1, epoch 7, cost  -1349.3925545
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2302/2302, best error 5.159061 %, best_auc: 0.500000
Optimization complete with best validation score of 5.157757 %, obtained at iteration 2302, with test performance 5.159061 %, and best_auc: 0.500000
runtime: 55317.12 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 104
RunningOn slot17@roy-exec-1.discovery.wisc.edu
python th_deep_belief_net.py pcba 104
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 105
RunningOn slot4@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 105
Running Theano Learn Deep Belief Net for pcba, target: aid720711.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.3231862637
Pre-training layer 0, epoch 1, cost  -52.258602394
Pre-training layer 0, epoch 2, cost  -46.6537894041
Pre-training layer 0, epoch 3, cost  -43.0492712044
Pre-training layer 0, epoch 4, cost  -40.4394140722
Pre-training layer 0, epoch 5, cost  -38.376496898
Pre-training layer 0, epoch 6, cost  -36.6666577131
Pre-training layer 0, epoch 7, cost  -35.2424768777
Pre-training layer 1, epoch 0, cost  -1352.59663154
Pre-training layer 1, epoch 1, cost  -1348.86367245
Pre-training layer 1, epoch 2, cost  -1348.86245853
Pre-training layer 1, epoch 3, cost  -1348.86235779
Pre-training layer 1, epoch 4, cost  -1348.86238427
Pre-training layer 1, epoch 5, cost  -1348.86238173
Pre-training layer 1, epoch 6, cost  -1348.86210489
Pre-training layer 1, epoch 7, cost  -1348.86225794
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2238/2238, best error 2.332440 %, best_auc: 0.500000
Optimization complete with best validation score of 2.332440 %, obtained at iteration 2238, with test performance 2.332440 %, and best_auc: 0.500000
runtime: 240423.77 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 106
RunningOn slot1@opt-a012.discovery.wisc.edu
python th_deep_belief_net.py pcba 106
Running Theano Learn Deep Belief Net for pcba, target: aid743255.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.4732322042
Pre-training layer 0, epoch 1, cost  -51.1909612771
Pre-training layer 0, epoch 2, cost  -45.5034522604
Pre-training layer 0, epoch 3, cost  -41.8793342526
Pre-training layer 0, epoch 4, cost  -39.2263414917
Pre-training layer 0, epoch 5, cost  -37.1338355207
Pre-training layer 0, epoch 6, cost  -35.3917111165
Pre-training layer 0, epoch 7, cost  -33.9345801939
Pre-training layer 1, epoch 0, cost  -1353.28459551
Pre-training layer 1, epoch 1, cost  -1350.0399517
Pre-training layer 1, epoch 2, cost  -1350.03938058
Pre-training layer 1, epoch 3, cost  -1350.03933038
Pre-training layer 1, epoch 4, cost  -1350.03907145
Pre-training layer 1, epoch 5, cost  -1350.03914107
Pre-training layer 1, epoch 6, cost  -1350.03878408
Pre-training layer 1, epoch 7, cost  -1350.03916097
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2494/2494, best error 6.496992 %, best_auc: 0.500000
Optimization complete with best validation score of 6.496992 %, obtained at iteration 2494, with test performance 6.496992 %, and best_auc: 0.500000
runtime: 57572.85 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 107
RunningOn slot1@opt-a006.discovery.wisc.edu
python th_deep_belief_net.py pcba 107
Running Theano Learn Deep Belief Net for pcba, target: aid743266.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.597559307
Pre-training layer 0, epoch 1, cost  -51.3485209797
Pre-training layer 0, epoch 2, cost  -45.7281759943
Pre-training layer 0, epoch 3, cost  -42.120781645
Pre-training layer 0, epoch 4, cost  -39.4872609914
Pre-training layer 0, epoch 5, cost  -37.3945923817
Pre-training layer 0, epoch 6, cost  -35.7278946004
Pre-training layer 0, epoch 7, cost  -34.2802380705
Pre-training layer 1, epoch 0, cost  -1353.11075038
Pre-training layer 1, epoch 1, cost  -1349.82928203
Pre-training layer 1, epoch 2, cost  -1349.82825808
Pre-training layer 1, epoch 3, cost  -1349.82879143
Pre-training layer 1, epoch 4, cost  -1349.82839627
Pre-training layer 1, epoch 5, cost  -1349.8285602
Pre-training layer 1, epoch 6, cost  -1349.82808756
Pre-training layer 1, epoch 7, cost  -1349.82825042
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2487/2487, best error 2.207479 %, best_auc: 0.500000
Optimization complete with best validation score of 2.207479 %, obtained at iteration 2487, with test performance 2.207479 %, and best_auc: 0.500000
runtime: 65411.15 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 108
RunningOn slot4@wid-004.discovery.wisc.edu
python th_deep_belief_net.py pcba 108
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 109
RunningOn slot3@opt-a016.discovery.wisc.edu
python th_deep_belief_net.py pcba 109
Running Theano Learn Deep Belief Net for pcba, target: aid881.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -74.1598105973
Pre-training layer 0, epoch 1, cost  -61.4661337915
Pre-training layer 0, epoch 2, cost  -56.0562533271
Pre-training layer 0, epoch 3, cost  -52.3398329703
Pre-training layer 0, epoch 4, cost  -49.5360688096
Pre-training layer 0, epoch 5, cost  -47.2846930769
Pre-training layer 0, epoch 6, cost  -45.4767049501
Pre-training layer 0, epoch 7, cost  -43.9671650831
Pre-training layer 1, epoch 0, cost  -1350.44412528
Pre-training layer 1, epoch 1, cost  -1339.24866072
Pre-training layer 1, epoch 2, cost  -1338.65709349
Pre-training layer 1, epoch 3, cost  -1338.24147092
Pre-training layer 1, epoch 4, cost  -1337.89995087
Pre-training layer 1, epoch 5, cost  -1337.6353758
Pre-training layer 1, epoch 6, cost  -1337.43744806
Pre-training layer 1, epoch 7, cost  -1337.29508116
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 746/746, best error 14.241935 %, best_auc: 0.623849
     new best AUC!: 0.626119858978
     new best AUC!: 0.634862009537
     new best AUC!: 0.646327776317
     new best AUC!: 0.651736868995
     new best AUC!: 0.664842248371
     new best AUC!: 0.675623485831
     new best AUC!: 0.683474715322
     new best AUC!: 0.705895334951
     new best AUC!: 0.706065374735
     new best AUC!: 0.719302852775
     epoch 11, minibatch 746/746, best error 14.080645 %, best_auc: 0.719303
     new best AUC!: 0.737030096247
     epoch 13, minibatch 746/746, best error 13.431452 %, best_auc: 0.737030
     new best AUC!: 0.743195428949
     epoch 14, minibatch 746/746, best error 13.064516 %, best_auc: 0.743195
     new best AUC!: 0.749305935739
     epoch 15, minibatch 746/746, best error 12.544355 %, best_auc: 0.749306
     new best AUC!: 0.7504310588
     new best AUC!: 0.758730231882
     epoch 17, minibatch 746/746, best error 11.846774 %, best_auc: 0.758730
     new best AUC!: 0.762361852607
     epoch 18, minibatch 746/746, best error 11.762097 %, best_auc: 0.762362
     new best AUC!: 0.794724634871
     epoch 19, minibatch 746/746, best error 11.112903 %, best_auc: 0.794725
     new best AUC!: 0.865049196324
     epoch 20, minibatch 746/746, best error 10.104839 %, best_auc: 0.865049
     epoch 21, minibatch 746/746, best error 9.387097 %, best_auc: 0.865049
     epoch 29, minibatch 746/746, best error 9.612903 %, best_auc: 0.865049
     epoch 36, minibatch 746/746, best error 9.955645 %, best_auc: 0.865049
Optimization complete with best validation score of 8.955645 %, obtained at iteration 26856, with test performance 9.955645 %, and best_auc: 0.865049
runtime: 79986.44 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 11
RunningOn slot10@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 11
Running Theano Learn Deep Belief Net for pcba, target: aid1479.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -69.0646305267
Pre-training layer 0, epoch 1, cost  -55.2453043237
Pre-training layer 0, epoch 2, cost  -49.531010126
Pre-training layer 0, epoch 3, cost  -45.8221609333
Pre-training layer 0, epoch 4, cost  -43.1046392958
Pre-training layer 0, epoch 5, cost  -40.9573885562
Pre-training layer 0, epoch 6, cost  -39.220977712
Pre-training layer 0, epoch 7, cost  -37.6952667429
Pre-training layer 1, epoch 0, cost  -1349.31391172
Pre-training layer 1, epoch 1, cost  -1344.06297449
Pre-training layer 1, epoch 2, cost  -1344.0603748
Pre-training layer 1, epoch 3, cost  -1344.06008524
Pre-training layer 1, epoch 4, cost  -1344.06003781
Pre-training layer 1, epoch 5, cost  -1344.05972616
Pre-training layer 1, epoch 6, cost  -1344.05981862
Pre-training layer 1, epoch 7, cost  -1344.0598791
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1795/1795, best error 7.919732 %, best_auc: 0.500000
Optimization complete with best validation score of 7.919732 %, obtained at iteration 1795, with test performance 7.919732 %, and best_auc: 0.500000
runtime: 30255.73 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 110
RunningOn slot15@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 110
Running Theano Learn Deep Belief Net for pcba, target: aid883.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -100.544320289
Pre-training layer 0, epoch 1, cost  -70.9928921198
Pre-training layer 0, epoch 2, cost  -66.6114425207
Pre-training layer 0, epoch 3, cost  -63.232709413
Pre-training layer 0, epoch 4, cost  -60.4792139503
Pre-training layer 0, epoch 5, cost  -58.267988965
Pre-training layer 0, epoch 6, cost  -56.509516301
Pre-training layer 0, epoch 7, cost  -54.5924477321
Pre-training layer 1, epoch 0, cost  -1383.75818874
Pre-training layer 1, epoch 1, cost  -1364.06560791
Pre-training layer 1, epoch 2, cost  -1354.84555186
Pre-training layer 1, epoch 3, cost  -1350.0937159
Pre-training layer 1, epoch 4, cost  -1347.45564103
Pre-training layer 1, epoch 5, cost  -1345.77376193
Pre-training layer 1, epoch 6, cost  -1344.47791284
Pre-training layer 1, epoch 7, cost  -1343.46242933
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 92/92, best error 40.200000 %, best_auc: 0.626423
     new best AUC!: 0.633055624857
     epoch 2, minibatch 92/92, best error 37.833333 %, best_auc: 0.633056
     new best AUC!: 0.636563521404
     epoch 3, minibatch 92/92, best error 37.666667 %, best_auc: 0.636564
     new best AUC!: 0.63953968938
     epoch 4, minibatch 92/92, best error 37.300000 %, best_auc: 0.639540
     new best AUC!: 0.640562825873
     epoch 5, minibatch 92/92, best error 37.366667 %, best_auc: 0.640563
     new best AUC!: 0.641652743442
     epoch 6, minibatch 92/92, best error 36.966667 %, best_auc: 0.641653
     new best AUC!: 0.643177368019
     epoch 7, minibatch 92/92, best error 36.633333 %, best_auc: 0.643177
     new best AUC!: 0.644694432474
     epoch 8, minibatch 92/92, best error 36.466667 %, best_auc: 0.644694
     new best AUC!: 0.646411840159
     epoch 9, minibatch 92/92, best error 36.033333 %, best_auc: 0.646412
     new best AUC!: 0.647617679597
     epoch 10, minibatch 92/92, best error 35.866667 %, best_auc: 0.647618
     new best AUC!: 0.649265786164
     new best AUC!: 0.65106635519
     epoch 12, minibatch 92/92, best error 36.033333 %, best_auc: 0.651066
     new best AUC!: 0.652541838974
     epoch 13, minibatch 92/92, best error 36.133333 %, best_auc: 0.652542
     new best AUC!: 0.654160965075
     new best AUC!: 0.656148017106
     epoch 15, minibatch 92/92, best error 35.800000 %, best_auc: 0.656148
     new best AUC!: 0.658261071168
     epoch 16, minibatch 92/92, best error 35.533333 %, best_auc: 0.658261
     new best AUC!: 0.660646289618
     epoch 17, minibatch 92/92, best error 35.100000 %, best_auc: 0.660646
     new best AUC!: 0.663157510099
     epoch 18, minibatch 92/92, best error 34.433333 %, best_auc: 0.663158
     new best AUC!: 0.665312144832
     epoch 19, minibatch 92/92, best error 34.766667 %, best_auc: 0.665312
     new best AUC!: 0.667514660336
     epoch 20, minibatch 92/92, best error 33.800000 %, best_auc: 0.667515
     new best AUC!: 0.669853258035
     epoch 21, minibatch 92/92, best error 33.600000 %, best_auc: 0.669853
     new best AUC!: 0.672191855733
     epoch 22, minibatch 92/92, best error 33.266667 %, best_auc: 0.672192
     new best AUC!: 0.674703076214
     epoch 23, minibatch 92/92, best error 32.933333 %, best_auc: 0.674703
     new best AUC!: 0.677030333729
     epoch 24, minibatch 92/92, best error 32.766667 %, best_auc: 0.677030
     new best AUC!: 0.679494933458
     epoch 25, minibatch 92/92, best error 32.666667 %, best_auc: 0.679495
     new best AUC!: 0.681749109796
     epoch 26, minibatch 92/92, best error 32.866667 %, best_auc: 0.681749
     new best AUC!: 0.683984385828
     epoch 27, minibatch 92/92, best error 32.533333 %, best_auc: 0.683984
     new best AUC!: 0.687520002822
     epoch 28, minibatch 92/92, best error 32.566667 %, best_auc: 0.687520
     new best AUC!: 0.688888384881
     epoch 29, minibatch 92/92, best error 32.300000 %, best_auc: 0.688888
     new best AUC!: 0.690683913825
     new best AUC!: 0.692643245409
     new best AUC!: 0.694894901706
     epoch 32, minibatch 92/92, best error 31.200000 %, best_auc: 0.694895
     new best AUC!: 0.695826056716
     new best AUC!: 0.697375881699
     new best AUC!: 0.699132350013
     epoch 35, minibatch 92/92, best error 30.700000 %, best_auc: 0.699132
     new best AUC!: 0.701365106006
     new best AUC!: 0.702728447983
     epoch 37, minibatch 92/92, best error 30.900000 %, best_auc: 0.702728
     new best AUC!: 0.704081709797
     epoch 38, minibatch 92/92, best error 30.766667 %, best_auc: 0.704082
     new best AUC!: 0.705271168971
     new best AUC!: 0.706838634239
     epoch 40, minibatch 92/92, best error 30.533333 %, best_auc: 0.706839
     new best AUC!: 0.707913431565
     new best AUC!: 0.708927747915
     new best AUC!: 0.710104606886
     new best AUC!: 0.7109677208
     epoch 44, minibatch 92/92, best error 29.800000 %, best_auc: 0.710968
     new best AUC!: 0.711823274591
     new best AUC!: 0.71279097019
     epoch 46, minibatch 92/92, best error 29.500000 %, best_auc: 0.712791
     new best AUC!: 0.713554542499
     epoch 47, minibatch 92/92, best error 29.500000 %, best_auc: 0.713555
     new best AUC!: 0.714145492025
     new best AUC!: 0.714718801267
     new best AUC!: 0.715343771342
     new best AUC!: 0.715956141213
     new best AUC!: 0.716877216061
     new best AUC!: 0.717542506785
     epoch 53, minibatch 92/92, best error 29.400000 %, best_auc: 0.717543
     new best AUC!: 0.718263238403
     new best AUC!: 0.719082251606
     new best AUC!: 0.719997026352
     new best AUC!: 0.721515350827
     new best AUC!: 0.722808131667
     new best AUC!: 0.724094612405
     new best AUC!: 0.725573876251
     new best AUC!: 0.726561732175
     epoch 61, minibatch 92/92, best error 29.633333 %, best_auc: 0.726562
     new best AUC!: 0.727949014538
     epoch 62, minibatch 92/92, best error 29.433333 %, best_auc: 0.727949
     new best AUC!: 0.729032632006
     epoch 63, minibatch 92/92, best error 29.466667 %, best_auc: 0.729033
     new best AUC!: 0.730523236035
     new best AUC!: 0.731831137118
     new best AUC!: 0.733036976556
     new best AUC!: 0.734557821072
     new best AUC!: 0.73641005093
     new best AUC!: 0.738177859427
     new best AUC!: 0.739897787152
     new best AUC!: 0.742523669482
     new best AUC!: 0.7446216033
     new best AUC!: 0.74676237781
     new best AUC!: 0.748657448358
     new best AUC!: 0.75044415716
     new best AUC!: 0.751884360376
     new best AUC!: 0.753164541012
     epoch 77, minibatch 92/92, best error 28.600000 %, best_auc: 0.753165
     new best AUC!: 0.754278398968
     new best AUC!: 0.755472898223
     new best AUC!: 0.756373812746
     epoch 80, minibatch 92/92, best error 28.100000 %, best_auc: 0.756374
     new best AUC!: 0.75744356999
     epoch 81, minibatch 92/92, best error 27.933333 %, best_auc: 0.757444
     new best AUC!: 0.758562468027
     new best AUC!: 0.759782167689
     epoch 83, minibatch 92/92, best error 27.800000 %, best_auc: 0.759782
     new best AUC!: 0.761010687492
     epoch 84, minibatch 92/92, best error 27.266667 %, best_auc: 0.761011
     new best AUC!: 0.762697854689
     new best AUC!: 0.764495903674
     new best AUC!: 0.765788684514
     new best AUC!: 0.767667374798
     new best AUC!: 0.769078597547
     new best AUC!: 0.770201275645
     new best AUC!: 0.770716623952
     new best AUC!: 0.771206771853
     new best AUC!: 0.771497836545
     new best AUC!: 0.771717080079
     new best AUC!: 0.771772520973
     epoch 95, minibatch 92/92, best error 26.000000 %, best_auc: 0.771773
     new best AUC!: 0.772129106721
     new best AUC!: 0.772463012104
     new best AUC!: 0.772562553708
     new best AUC!: 0.772804477608
     new best AUC!: 0.773041361427
     new best AUC!: 0.773438267825
     new best AUC!: 0.773689011867
     new best AUC!: 0.774093478387
     epoch 103, minibatch 92/92, best error 26.666667 %, best_auc: 0.774093
     new best AUC!: 0.77458992639
     new best AUC!: 0.774868390878
     new best AUC!: 0.775271597378
     new best AUC!: 0.77543918008
     new best AUC!: 0.775629443147
     new best AUC!: 0.775740324934
     new best AUC!: 0.775799545889
     epoch 111, minibatch 92/92, best error 27.066667 %, best_auc: 0.775800
     epoch 113, minibatch 92/92, best error 27.100000 %, best_auc: 0.775800
     epoch 114, minibatch 92/92, best error 27.000000 %, best_auc: 0.775800
     epoch 116, minibatch 92/92, best error 26.966667 %, best_auc: 0.775800
     epoch 124, minibatch 92/92, best error 26.066667 %, best_auc: 0.775800
     epoch 125, minibatch 92/92, best error 26.733333 %, best_auc: 0.775800
     epoch 126, minibatch 92/92, best error 26.566667 %, best_auc: 0.775800
     new best AUC!: 0.775870107026
     new best AUC!: 0.776051549951
     new best AUC!: 0.776101950763
     new best AUC!: 0.776144791454
     new best AUC!: 0.776151091556
Optimization complete with best validation score of 23.533333 %, obtained at iteration 11592, with test performance 26.566667 %, and best_auc: 0.776151
runtime: 12219.67 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 111
RunningOn slot6@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 111
Running Theano Learn Deep Belief Net for pcba, target: aid884.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -99.520858267
Pre-training layer 0, epoch 1, cost  -73.0006553178
Pre-training layer 0, epoch 2, cost  -69.2417648867
Pre-training layer 0, epoch 3, cost  -66.0796523931
Pre-training layer 0, epoch 4, cost  -63.6592291658
Pre-training layer 0, epoch 5, cost  -61.4221209444
Pre-training layer 0, epoch 6, cost  -59.761046011
Pre-training layer 0, epoch 7, cost  -57.8960632752
Pre-training layer 1, epoch 0, cost  -1383.66078075
Pre-training layer 1, epoch 1, cost  -1367.07039299
Pre-training layer 1, epoch 2, cost  -1359.9541166
Pre-training layer 1, epoch 3, cost  -1355.48750713
Pre-training layer 1, epoch 4, cost  -1352.57305146
Pre-training layer 1, epoch 5, cost  -1350.59513856
Pre-training layer 1, epoch 6, cost  -1349.21489192
Pre-training layer 1, epoch 7, cost  -1348.18252619
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 98/98, best error 41.500000 %, best_auc: 0.675827
     new best AUC!: 0.698500606988
     epoch 2, minibatch 98/98, best error 34.781250 %, best_auc: 0.698501
     new best AUC!: 0.719141616528
     epoch 3, minibatch 98/98, best error 32.656250 %, best_auc: 0.719142
     new best AUC!: 0.735958412775
     epoch 4, minibatch 98/98, best error 30.625000 %, best_auc: 0.735958
     new best AUC!: 0.749804964741
     epoch 5, minibatch 98/98, best error 30.093750 %, best_auc: 0.749805
     new best AUC!: 0.762332126969
     epoch 6, minibatch 98/98, best error 29.187500 %, best_auc: 0.762332
     new best AUC!: 0.774199023492
     epoch 7, minibatch 98/98, best error 28.187500 %, best_auc: 0.774199
     new best AUC!: 0.786319370712
     epoch 8, minibatch 98/98, best error 27.125000 %, best_auc: 0.786319
     new best AUC!: 0.797510018153
     epoch 9, minibatch 98/98, best error 26.500000 %, best_auc: 0.797510
     new best AUC!: 0.806810440951
     epoch 10, minibatch 98/98, best error 26.406250 %, best_auc: 0.806810
     new best AUC!: 0.816677893086
     epoch 11, minibatch 98/98, best error 25.718750 %, best_auc: 0.816678
     new best AUC!: 0.829511783935
     epoch 12, minibatch 98/98, best error 24.718750 %, best_auc: 0.829512
     new best AUC!: 0.841875306824
     epoch 13, minibatch 98/98, best error 23.843750 %, best_auc: 0.841875
     new best AUC!: 0.848999326415
     epoch 14, minibatch 98/98, best error 22.843750 %, best_auc: 0.848999
     new best AUC!: 0.853989945695
     epoch 15, minibatch 98/98, best error 22.875000 %, best_auc: 0.853990
     new best AUC!: 0.857680583622
     epoch 16, minibatch 98/98, best error 22.187500 %, best_auc: 0.857681
     new best AUC!: 0.860495941364
     epoch 17, minibatch 98/98, best error 22.125000 %, best_auc: 0.860496
     new best AUC!: 0.86273627808
     epoch 18, minibatch 98/98, best error 21.500000 %, best_auc: 0.862736
     new best AUC!: 0.864419479931
     epoch 19, minibatch 98/98, best error 21.375000 %, best_auc: 0.864419
     new best AUC!: 0.865728594643
     epoch 20, minibatch 98/98, best error 20.906250 %, best_auc: 0.865729
     new best AUC!: 0.866810897619
     epoch 21, minibatch 98/98, best error 21.562500 %, best_auc: 0.866811
     new best AUC!: 0.867795016992
     new best AUC!: 0.868582008045
     new best AUC!: 0.869288321098
     new best AUC!: 0.869922328398
     new best AUC!: 0.870549105121
     new best AUC!: 0.871166748486
     new best AUC!: 0.871726166691
     new best AUC!: 0.872240679217
     new best AUC!: 0.872761280649
     new best AUC!: 0.873256004232
     new best AUC!: 0.873634277494
     new best AUC!: 0.874046800851
     new best AUC!: 0.87434591834
     new best AUC!: 0.874949481111
     new best AUC!: 0.87519265678
     new best AUC!: 0.876060706389
     epoch 37, minibatch 98/98, best error 20.625000 %, best_auc: 0.876061
     new best AUC!: 0.876634585745
     new best AUC!: 0.877072986951
     new best AUC!: 0.877308170931
     new best AUC!: 0.877554010496
     new best AUC!: 0.877853889098
     epoch 42, minibatch 98/98, best error 20.343750 %, best_auc: 0.877854
     new best AUC!: 0.878029706248
     new best AUC!: 0.878044547956
     new best AUC!: 0.878406837841
     new best AUC!: 0.878565529944
     epoch 47, minibatch 98/98, best error 20.281250 %, best_auc: 0.878566
     new best AUC!: 0.878581893878
     epoch 51, minibatch 98/98, best error 19.843750 %, best_auc: 0.878582
     new best AUC!: 0.878736019302
     new best AUC!: 0.878743630434
     epoch 252, minibatch 98/98, best error 20.343750 %, best_auc: 0.878744
Optimization complete with best validation score of 20.750000 %, obtained at iteration 24696, with test performance 20.343750 %, and best_auc: 0.878744
runtime: 55819.06 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 112
RunningOn slot5@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 112
Running Theano Learn Deep Belief Net for pcba, target: aid885.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -93.4994336773
Pre-training layer 0, epoch 1, cost  -67.7281920895
Pre-training layer 0, epoch 2, cost  -63.5084344377
Pre-training layer 0, epoch 3, cost  -60.2715994275
Pre-training layer 0, epoch 4, cost  -57.6872719743
Pre-training layer 0, epoch 5, cost  -55.5109529212
Pre-training layer 0, epoch 6, cost  -53.5953540868
Pre-training layer 0, epoch 7, cost  -51.8697942577
Pre-training layer 1, epoch 0, cost  -1382.26225183
Pre-training layer 1, epoch 1, cost  -1362.95196059
Pre-training layer 1, epoch 2, cost  -1354.95690019
Pre-training layer 1, epoch 3, cost  -1351.26727461
Pre-training layer 1, epoch 4, cost  -1349.39071227
Pre-training layer 1, epoch 5, cost  -1348.269614
Pre-training layer 1, epoch 6, cost  -1347.48943321
Pre-training layer 1, epoch 7, cost  -1346.86118949
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 106/106, best error 27.142857 %, best_auc: 0.752203
     epoch 3, minibatch 106/106, best error 23.057143 %, best_auc: 0.752203
     epoch 5, minibatch 106/106, best error 22.228571 %, best_auc: 0.752203
     epoch 6, minibatch 106/106, best error 21.485714 %, best_auc: 0.752203
     epoch 7, minibatch 106/106, best error 21.028571 %, best_auc: 0.752203
     epoch 8, minibatch 106/106, best error 20.771429 %, best_auc: 0.752203
     epoch 9, minibatch 106/106, best error 20.457143 %, best_auc: 0.752203
     epoch 11, minibatch 106/106, best error 19.657143 %, best_auc: 0.752203
     epoch 12, minibatch 106/106, best error 19.114286 %, best_auc: 0.752203
     epoch 13, minibatch 106/106, best error 18.200000 %, best_auc: 0.752203
     epoch 14, minibatch 106/106, best error 17.257143 %, best_auc: 0.752203
     epoch 15, minibatch 106/106, best error 16.914286 %, best_auc: 0.752203
     epoch 16, minibatch 106/106, best error 17.228571 %, best_auc: 0.752203
     epoch 17, minibatch 106/106, best error 17.028571 %, best_auc: 0.752203
     epoch 18, minibatch 106/106, best error 16.885714 %, best_auc: 0.752203
     epoch 19, minibatch 106/106, best error 16.657143 %, best_auc: 0.752203
     epoch 20, minibatch 106/106, best error 16.571429 %, best_auc: 0.752203
     epoch 21, minibatch 106/106, best error 16.514286 %, best_auc: 0.752203
     epoch 22, minibatch 106/106, best error 16.485714 %, best_auc: 0.752203
     epoch 24, minibatch 106/106, best error 16.285714 %, best_auc: 0.752203
     epoch 25, minibatch 106/106, best error 16.257143 %, best_auc: 0.752203
     epoch 26, minibatch 106/106, best error 16.200000 %, best_auc: 0.752203
     epoch 27, minibatch 106/106, best error 16.200000 %, best_auc: 0.752203
     epoch 28, minibatch 106/106, best error 16.200000 %, best_auc: 0.752203
     epoch 29, minibatch 106/106, best error 16.142857 %, best_auc: 0.752203
     epoch 31, minibatch 106/106, best error 16.085714 %, best_auc: 0.752203
     new best AUC!: 0.752347986057
     new best AUC!: 0.752856312936
     epoch 33, minibatch 106/106, best error 16.057143 %, best_auc: 0.752856
     new best AUC!: 0.753903224245
     epoch 36, minibatch 106/106, best error 16.028571 %, best_auc: 0.753903
     new best AUC!: 0.754399448102
     epoch 37, minibatch 106/106, best error 16.028571 %, best_auc: 0.754399
     new best AUC!: 0.755053011232
     epoch 38, minibatch 106/106, best error 16.000000 %, best_auc: 0.755053
     new best AUC!: 0.7555552866
     new best AUC!: 0.755912325716
     new best AUC!: 0.766641653757
     epoch 43, minibatch 106/106, best error 15.942857 %, best_auc: 0.766642
     new best AUC!: 0.768759682417
     epoch 47, minibatch 106/106, best error 15.942857 %, best_auc: 0.768760
     new best AUC!: 0.769582687839
     new best AUC!: 0.771501016654
     epoch 51, minibatch 106/106, best error 15.000000 %, best_auc: 0.771501
     new best AUC!: 0.781280257552
     new best AUC!: 0.78243609605
     new best AUC!: 0.782581332301
     new best AUC!: 0.816723954299
     new best AUC!: 0.821867738187
     new best AUC!: 0.822817825329
     new best AUC!: 0.822902546476
     new best AUC!: 0.8230901433
     new best AUC!: 0.823108297831
     new best AUC!: 0.823332203718
     new best AUC!: 0.823550058095
     new best AUC!: 0.823780015492
     new best AUC!: 0.8240160244
     new best AUC!: 0.835090288536
     new best AUC!: 0.835447327653
     new best AUC!: 0.835495739737
     new best AUC!: 0.835622821456
     new best AUC!: 0.836028272657
     new best AUC!: 0.836300590627
     new best AUC!: 0.836609217661
     new best AUC!: 0.838696988768
     new best AUC!: 0.839435273044
     new best AUC!: 0.83978020914
     new best AUC!: 0.840264329977
     new best AUC!: 0.840379308675
     new best AUC!: 0.840385360186
     new best AUC!: 0.840960253679
     new best AUC!: 0.841063129357
     new best AUC!: 0.841940598373
     new best AUC!: 0.842884634005
     new best AUC!: 0.847284082107
     new best AUC!: 0.847786357475
Optimization complete with best validation score of 14.085714 %, obtained at iteration 5406, with test performance 15.000000 %, and best_auc: 0.847786
runtime: 130897.95 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 113
RunningOn slot6@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 113
Running Theano Learn Deep Belief Net for pcba, target: aid887.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -75.986844328
Pre-training layer 0, epoch 1, cost  -62.5931817557
Pre-training layer 0, epoch 2, cost  -56.7129366332
Pre-training layer 0, epoch 3, cost  -52.6204170549
Pre-training layer 0, epoch 4, cost  -49.5262180904
Pre-training layer 0, epoch 5, cost  -47.0027388374
Pre-training layer 0, epoch 6, cost  -44.9270650635
Pre-training layer 0, epoch 7, cost  -43.2280666746
Pre-training layer 1, epoch 0, cost  -1353.54501998
Pre-training layer 1, epoch 1, cost  -1339.14270442
Pre-training layer 1, epoch 2, cost  -1338.37622407
Pre-training layer 1, epoch 3, cost  -1338.02760564
Pre-training layer 1, epoch 4, cost  -1337.78987271
Pre-training layer 1, epoch 5, cost  -1337.6074892
Pre-training layer 1, epoch 6, cost  -1337.46180333
Pre-training layer 1, epoch 7, cost  -1337.33864198
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 616/616, best error 29.658537 %, best_auc: 0.643982
     new best AUC!: 0.650361350361
     new best AUC!: 0.656810353362
     new best AUC!: 0.662441752097
     new best AUC!: 0.668637622086
     new best AUC!: 0.672297894712
     new best AUC!: 0.674321081218
     epoch 7, minibatch 616/616, best error 30.702439 %, best_auc: 0.674321
     new best AUC!: 0.677940558975
     epoch 8, minibatch 616/616, best error 30.092683 %, best_auc: 0.677941
     new best AUC!: 0.679642677919
     epoch 9, minibatch 616/616, best error 29.760976 %, best_auc: 0.679643
     new best AUC!: 0.685692608106
     epoch 10, minibatch 616/616, best error 28.917073 %, best_auc: 0.685693
     new best AUC!: 0.694959017373
     epoch 11, minibatch 616/616, best error 28.385366 %, best_auc: 0.694959
     new best AUC!: 0.704073683384
     epoch 12, minibatch 616/616, best error 27.746341 %, best_auc: 0.704074
     new best AUC!: 0.711676208228
     epoch 13, minibatch 616/616, best error 26.697561 %, best_auc: 0.711676
     new best AUC!: 0.716703706359
     epoch 14, minibatch 616/616, best error 26.170732 %, best_auc: 0.716704
     new best AUC!: 0.724627810835
     epoch 15, minibatch 616/616, best error 25.780488 %, best_auc: 0.724628
     new best AUC!: 0.735981415292
     epoch 16, minibatch 616/616, best error 24.839024 %, best_auc: 0.735981
     new best AUC!: 0.744474941027
     epoch 17, minibatch 616/616, best error 24.409756 %, best_auc: 0.744475
     new best AUC!: 0.752335036818
     epoch 18, minibatch 616/616, best error 23.619512 %, best_auc: 0.752335
     new best AUC!: 0.755925838684
     epoch 19, minibatch 616/616, best error 23.434146 %, best_auc: 0.755926
     new best AUC!: 0.757619764516
     epoch 20, minibatch 616/616, best error 23.278049 %, best_auc: 0.757620
     new best AUC!: 0.764495833461
     new best AUC!: 0.768793461897
     new best AUC!: 0.770637594776
     epoch 23, minibatch 616/616, best error 23.282927 %, best_auc: 0.770638
     new best AUC!: 0.771043665871
     new best AUC!: 0.772058928955
     new best AUC!: 0.772244980866
Optimization complete with best validation score of 23.580488 %, obtained at iteration 14168, with test performance 23.282927 %, and best_auc: 0.772245
runtime: 109478.57 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 114
RunningOn slot9@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 114
Running Theano Learn Deep Belief Net for pcba, target: aid891.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -102.334150002
Pre-training layer 0, epoch 1, cost  -72.2113543792
Pre-training layer 0, epoch 2, cost  -68.19584791
Pre-training layer 0, epoch 3, cost  -65.2696063323
Pre-training layer 0, epoch 4, cost  -62.5610022599
Pre-training layer 0, epoch 5, cost  -60.3446075261
Pre-training layer 0, epoch 6, cost  -58.4760267124
Pre-training layer 0, epoch 7, cost  -56.7467060241
Pre-training layer 1, epoch 0, cost  -1386.35590656
Pre-training layer 1, epoch 1, cost  -1371.37157012
Pre-training layer 1, epoch 2, cost  -1365.3311991
Pre-training layer 1, epoch 3, cost  -1361.34406532
Pre-training layer 1, epoch 4, cost  -1358.68321645
Pre-training layer 1, epoch 5, cost  -1356.91500955
Pre-training layer 1, epoch 6, cost  -1355.73668055
Pre-training layer 1, epoch 7, cost  -1354.95393726
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 84/84, best error 49.935484 %, best_auc: 0.504156
     new best AUC!: 0.510164600957
     new best AUC!: 0.510169724244
     new best AUC!: 0.517964292741
     new best AUC!: 0.51796839137
     new best AUC!: 0.541308036593
     new best AUC!: 0.541933077579
     new best AUC!: 0.544970161978
     new best AUC!: 0.566222580169
     new best AUC!: 0.566229752771
     new best AUC!: 0.569245319365
     new best AUC!: 0.569305774149
     new best AUC!: 0.57354273231
     new best AUC!: 0.573550929569
     new best AUC!: 0.573591915863
     new best AUC!: 0.573591915863
     new best AUC!: 0.573596014493
     new best AUC!: 0.574667806086
     new best AUC!: 0.575067422454
     new best AUC!: 0.575067422454
     new best AUC!: 0.575072545741
     new best AUC!: 0.575080743
     new best AUC!: 0.576328775657
     new best AUC!: 0.576335948259
     new best AUC!: 0.576359515378
     new best AUC!: 0.576851350908
Optimization complete with best validation score of 49.903226 %, obtained at iteration 84, with test performance 49.935484 %, and best_auc: 0.576851
runtime: 11976.72 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 115
RunningOn slot6@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 115
Running Theano Learn Deep Belief Net for pcba, target: aid899.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -100.962952995
Pre-training layer 0, epoch 1, cost  -71.2249225902
Pre-training layer 0, epoch 2, cost  -66.8365598007
Pre-training layer 0, epoch 3, cost  -63.5113342646
Pre-training layer 0, epoch 4, cost  -60.7018718249
Pre-training layer 0, epoch 5, cost  -58.603199586
Pre-training layer 0, epoch 6, cost  -56.6781155535
Pre-training layer 0, epoch 7, cost  -54.9282100666
Pre-training layer 1, epoch 0, cost  -1385.6080565
Pre-training layer 1, epoch 1, cost  -1370.09240428
Pre-training layer 1, epoch 2, cost  -1363.70937001
Pre-training layer 1, epoch 3, cost  -1359.563604
Pre-training layer 1, epoch 4, cost  -1356.85445087
Pre-training layer 1, epoch 5, cost  -1355.08418244
Pre-training layer 1, epoch 6, cost  -1353.92402995
Pre-training layer 1, epoch 7, cost  -1353.1662796
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 88/88, best error 48.206897 %, best_auc: 0.523639
     new best AUC!: 0.523640095919
     new best AUC!: 0.539986970757
     new best AUC!: 0.539995322836
     new best AUC!: 0.541902380899
     new best AUC!: 0.541927437137
     new best AUC!: 0.568431368182
     new best AUC!: 0.572264972494
     new best AUC!: 0.57243665412
     new best AUC!: 0.57245985434
     new best AUC!: 0.573821243235
     new best AUC!: 0.579306703193
     new best AUC!: 0.580064886375
     new best AUC!: 0.580179031456
     new best AUC!: 0.580606843508
     new best AUC!: 0.584169469253
     new best AUC!: 0.604828800938
     new best AUC!: 0.605184228305
     new best AUC!: 0.615681863739
     new best AUC!: 0.616035435088
     new best AUC!: 0.616181132468
     new best AUC!: 0.616245165074
     new best AUC!: 0.618014877837
     new best AUC!: 0.618468674135
     new best AUC!: 0.619826350995
     new best AUC!: 0.620212402652
     new best AUC!: 0.623362064485
     new best AUC!: 0.624524859499
     new best AUC!: 0.624642716616
     new best AUC!: 0.625441732184
     epoch 170, minibatch 88/88, best error 47.793103 %, best_auc: 0.625442
     epoch 171, minibatch 88/88, best error 46.482759 %, best_auc: 0.625442
     epoch 172, minibatch 88/88, best error 46.275862 %, best_auc: 0.625442
     epoch 173, minibatch 88/88, best error 46.379310 %, best_auc: 0.625442
     new best AUC!: 0.729633919092
     epoch 177, minibatch 88/88, best error 33.517241 %, best_auc: 0.729634
     new best AUC!: 0.763689985672
     epoch 178, minibatch 88/88, best error 29.862069 %, best_auc: 0.763690
     new best AUC!: 0.776984639599
     epoch 179, minibatch 88/88, best error 29.448276 %, best_auc: 0.776985
     new best AUC!: 0.787216864518
     epoch 180, minibatch 88/88, best error 29.275862 %, best_auc: 0.787217
     new best AUC!: 0.795680304684
     epoch 181, minibatch 88/88, best error 28.827586 %, best_auc: 0.795680
     new best AUC!: 0.802738739541
     new best AUC!: 0.808613035183
     new best AUC!: 0.813130581973
     new best AUC!: 0.816844473151
     new best AUC!: 0.819719444383
     new best AUC!: 0.822087722815
     new best AUC!: 0.823997564905
     new best AUC!: 0.825927823188
     new best AUC!: 0.827544414501
     new best AUC!: 0.82881021849
     new best AUC!: 0.82985330037
     new best AUC!: 0.830727484651
     new best AUC!: 0.831256449661
     new best AUC!: 0.831621157116
     new best AUC!: 0.831826247058
     epoch 196, minibatch 88/88, best error 28.793103 %, best_auc: 0.831826
     new best AUC!: 0.832050825185
     epoch 197, minibatch 88/88, best error 28.793103 %, best_auc: 0.832051
     new best AUC!: 0.832266123225
     epoch 198, minibatch 88/88, best error 28.172414 %, best_auc: 0.832266
     new best AUC!: 0.832371916227
     epoch 199, minibatch 88/88, best error 28.275862 %, best_auc: 0.832372
     epoch 200, minibatch 88/88, best error 27.862069 %, best_auc: 0.832372
     new best AUC!: 0.832527821704
     epoch 201, minibatch 88/88, best error 27.310345 %, best_auc: 0.832528
     new best AUC!: 0.832655886917
     epoch 202, minibatch 88/88, best error 27.000000 %, best_auc: 0.832656
     epoch 203, minibatch 88/88, best error 26.482759 %, best_auc: 0.832656
     epoch 205, minibatch 88/88, best error 25.586207 %, best_auc: 0.832656
     epoch 206, minibatch 88/88, best error 25.482759 %, best_auc: 0.832656
     epoch 221, minibatch 88/88, best error 25.034483 %, best_auc: 0.832656
     epoch 224, minibatch 88/88, best error 24.310345 %, best_auc: 0.832656
Optimization complete with best validation score of 24.310345 %, obtained at iteration 19712, with test performance 24.310345 %, and best_auc: 0.832656
runtime: 98166.97 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 116
RunningOn slot14@roy-exec-1.discovery.wisc.edu
python th_deep_belief_net.py pcba 116
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 117
RunningOn slot12@roy-exec-1.discovery.wisc.edu
python th_deep_belief_net.py pcba 117
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 118
RunningOn slot3@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 118
Running Theano Learn Deep Belief Net for pcba, target: aid904.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -82.2355312059
Pre-training layer 0, epoch 1, cost  -65.9294025238
Pre-training layer 0, epoch 2, cost  -58.8577666406
Pre-training layer 0, epoch 3, cost  -53.8122984358
Pre-training layer 0, epoch 4, cost  -50.0590826509
Pre-training layer 0, epoch 5, cost  -47.0824661318
Pre-training layer 0, epoch 6, cost  -44.6381835876
Pre-training layer 0, epoch 7, cost  -42.6199704045
Pre-training layer 1, epoch 0, cost  -1368.24362942
Pre-training layer 1, epoch 1, cost  -1354.51072059
Pre-training layer 1, epoch 2, cost  -1352.80773133
Pre-training layer 1, epoch 3, cost  -1352.55794859
Pre-training layer 1, epoch 4, cost  -1352.52011944
Pre-training layer 1, epoch 5, cost  -1352.5136592
Pre-training layer 1, epoch 6, cost  -1352.51374836
Pre-training layer 1, epoch 7, cost  -1352.51257936
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 419/419, best error 22.575540 %, best_auc: 0.500000
     new best AUC!: 0.504715591312
Optimization complete with best validation score of 22.611511 %, obtained at iteration 419, with test performance 22.575540 %, and best_auc: 0.504716
runtime: 61758.58 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 119
RunningOn slot1@opt-a012.discovery.wisc.edu
python th_deep_belief_net.py pcba 119
Running Theano Learn Deep Belief Net for pcba, target: aid912.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -78.0697890526
Pre-training layer 0, epoch 1, cost  -64.9126586123
Pre-training layer 0, epoch 2, cost  -59.3258983188
Pre-training layer 0, epoch 3, cost  -55.371199828
Pre-training layer 0, epoch 4, cost  -52.3676495697
Pre-training layer 0, epoch 5, cost  -50.0058441501
Pre-training layer 0, epoch 6, cost  -47.8863084947
Pre-training layer 0, epoch 7, cost  -46.1476902016
Pre-training layer 1, epoch 0, cost  -1356.95515633
Pre-training layer 1, epoch 1, cost  -1339.51591069
Pre-training layer 1, epoch 2, cost  -1338.43565734
Pre-training layer 1, epoch 3, cost  -1338.05299658
Pre-training layer 1, epoch 4, cost  -1337.76706343
Pre-training layer 1, epoch 5, cost  -1337.52465508
Pre-training layer 1, epoch 6, cost  -1337.31450721
Pre-training layer 1, epoch 7, cost  -1337.13279762
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 492/492, best error 16.463415 %, best_auc: 0.569595
     new best AUC!: 0.570164639092
     new best AUC!: 0.574435117599
     new best AUC!: 0.577813463098
     new best AUC!: 0.579226277372
     new best AUC!: 0.584647201946
     new best AUC!: 0.584698296837
     new best AUC!: 0.588508515815
     new best AUC!: 0.590315085158
     new best AUC!: 0.591302514193
     new best AUC!: 0.592436739659
     new best AUC!: 0.597952554745
     new best AUC!: 0.598106244931
     new best AUC!: 0.60096674777
     new best AUC!: 0.603649635036
     new best AUC!: 0.604700729927
     new best AUC!: 0.606616788321
     new best AUC!: 0.60696431468
     new best AUC!: 0.607368207624
     new best AUC!: 0.608378751014
     epoch 26, minibatch 492/492, best error 16.060976 %, best_auc: 0.608379
     epoch 29, minibatch 492/492, best error 15.817073 %, best_auc: 0.608379
     epoch 30, minibatch 492/492, best error 15.932927 %, best_auc: 0.608379
     new best AUC!: 0.608738848337
     epoch 45, minibatch 492/492, best error 15.628049 %, best_auc: 0.608739
     epoch 46, minibatch 492/492, best error 15.591463 %, best_auc: 0.608739
     epoch 47, minibatch 492/492, best error 15.591463 %, best_auc: 0.608739
     epoch 48, minibatch 492/492, best error 15.573171 %, best_auc: 0.608739
     epoch 49, minibatch 492/492, best error 15.548780 %, best_auc: 0.608739
Optimization complete with best validation score of 16.250000 %, obtained at iteration 24108, with test performance 15.548780 %, and best_auc: 0.608739
runtime: 24919.13 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 12
RunningOn slot15@wid-002.discovery.wisc.edu
python th_deep_belief_net.py pcba 12
Running Theano Learn Deep Belief Net for pcba, target: aid1631.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -67.1445504058
Pre-training layer 0, epoch 1, cost  -53.2296936752
Pre-training layer 0, epoch 2, cost  -47.6017201116
Pre-training layer 0, epoch 3, cost  -43.9622230226
Pre-training layer 0, epoch 4, cost  -41.3707985703
Pre-training layer 0, epoch 5, cost  -39.2409884683
Pre-training layer 0, epoch 6, cost  -37.5280986901
Pre-training layer 0, epoch 7, cost  -36.1018251186
Pre-training layer 1, epoch 0, cost  -1355.76088005
Pre-training layer 1, epoch 1, cost  -1351.25771382
Pre-training layer 1, epoch 2, cost  -1351.25506244
Pre-training layer 1, epoch 3, cost  -1351.254865
Pre-training layer 1, epoch 4, cost  -1351.25493049
Pre-training layer 1, epoch 5, cost  -1351.25474171
Pre-training layer 1, epoch 6, cost  -1351.25499077
Pre-training layer 1, epoch 7, cost  -1351.25502439
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1737/1737, best error 9.224913 %, best_auc: 0.500000
Optimization complete with best validation score of 9.217993 %, obtained at iteration 1737, with test performance 9.224913 %, and best_auc: 0.500000
runtime: 30271.74 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 120
RunningOn slot2@opt-a005.discovery.wisc.edu
python th_deep_belief_net.py pcba 120
Running Theano Learn Deep Belief Net for pcba, target: aid914.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -92.7347560157
Pre-training layer 0, epoch 1, cost  -64.4935351931
Pre-training layer 0, epoch 2, cost  -59.8576946154
Pre-training layer 0, epoch 3, cost  -56.2825821198
Pre-training layer 0, epoch 4, cost  -53.5780560207
Pre-training layer 0, epoch 5, cost  -51.2298447279
Pre-training layer 0, epoch 6, cost  -49.2024939902
Pre-training layer 0, epoch 7, cost  -47.6210989593
Pre-training layer 1, epoch 0, cost  -1381.62674829
Pre-training layer 1, epoch 1, cost  -1360.91073208
Pre-training layer 1, epoch 2, cost  -1352.26374059
Pre-training layer 1, epoch 3, cost  -1347.34351842
Pre-training layer 1, epoch 4, cost  -1343.8907265
Pre-training layer 1, epoch 5, cost  -1341.18326796
Pre-training layer 1, epoch 6, cost  -1339.05488115
Pre-training layer 1, epoch 7, cost  -1337.35984307
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 103/103, best error 18.882353 %, best_auc: 0.818642
     new best AUC!: 0.819926776003
     new best AUC!: 0.828389286173
     new best AUC!: 0.830059307155
     new best AUC!: 0.83893396994
     new best AUC!: 0.840769922494
     new best AUC!: 0.841711985612
     new best AUC!: 0.843927974993
     new best AUC!: 0.854874106111
     epoch 39, minibatch 103/103, best error 18.617647 %, best_auc: 0.854874
     epoch 40, minibatch 103/103, best error 18.500000 %, best_auc: 0.854874
     epoch 41, minibatch 103/103, best error 18.352941 %, best_auc: 0.854874
     epoch 42, minibatch 103/103, best error 18.264706 %, best_auc: 0.854874
     epoch 43, minibatch 103/103, best error 17.970588 %, best_auc: 0.854874
     epoch 44, minibatch 103/103, best error 17.735294 %, best_auc: 0.854874
     epoch 45, minibatch 103/103, best error 18.294118 %, best_auc: 0.854874
     epoch 83, minibatch 103/103, best error 18.911765 %, best_auc: 0.854874
     epoch 87, minibatch 103/103, best error 19.558824 %, best_auc: 0.854874
     epoch 88, minibatch 103/103, best error 19.558824 %, best_auc: 0.854874
     epoch 100, minibatch 103/103, best error 18.558824 %, best_auc: 0.854874
     epoch 102, minibatch 103/103, best error 18.558824 %, best_auc: 0.854874
     new best AUC!: 0.856303258682
     epoch 103, minibatch 103/103, best error 18.529412 %, best_auc: 0.856303
     new best AUC!: 0.856313963945
     new best AUC!: 0.856790348135
     epoch 116, minibatch 103/103, best error 18.588235 %, best_auc: 0.856790
     epoch 120, minibatch 103/103, best error 18.500000 %, best_auc: 0.856790
     new best AUC!: 0.856827816555
     new best AUC!: 0.85684922708
     new best AUC!: 0.856854579711
     new best AUC!: 0.856859932343
     new best AUC!: 0.85696698497
     new best AUC!: 0.857090095491
     new best AUC!: 0.857100800754
     new best AUC!: 0.8572453218
     new best AUC!: 0.857266732326
     new best AUC!: 0.857320258639
     new best AUC!: 0.857347021796
     new best AUC!: 0.857373784953
     new best AUC!: 0.863481137327
     new best AUC!: 0.874106110564
     new best AUC!: 0.87417034214
     new best AUC!: 0.874202457928
     new best AUC!: 0.874213163191
     new best AUC!: 0.874363036869
     new best AUC!: 0.880748726074
     new best AUC!: 0.88109129448
     new best AUC!: 0.881267931315
     new best AUC!: 0.881305399735
     new best AUC!: 0.881353573417
     new best AUC!: 0.882172526014
     new best AUC!: 0.882183231276
Optimization complete with best validation score of 16.352941 %, obtained at iteration 12360, with test performance 18.500000 %, and best_auc: 0.882183
runtime: 36226.79 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 121
RunningOn slot5@opt-a006.discovery.wisc.edu
python th_deep_belief_net.py pcba 121
Running Theano Learn Deep Belief Net for pcba, target: aid915.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -94.842032048
Pre-training layer 0, epoch 1, cost  -71.9771493695
Pre-training layer 0, epoch 2, cost  -67.2839786043
Pre-training layer 0, epoch 3, cost  -63.2011351402
Pre-training layer 0, epoch 4, cost  -59.9288582741
Pre-training layer 0, epoch 5, cost  -57.1222110202
Pre-training layer 0, epoch 6, cost  -54.4537919529
Pre-training layer 0, epoch 7, cost  -52.3738095903
Pre-training layer 1, epoch 0, cost  -1382.47016988
Pre-training layer 1, epoch 1, cost  -1367.57413804
Pre-training layer 1, epoch 2, cost  -1361.64124865
Pre-training layer 1, epoch 3, cost  -1358.3759219
Pre-training layer 1, epoch 4, cost  -1356.5744585
Pre-training layer 1, epoch 5, cost  -1355.5852441
Pre-training layer 1, epoch 6, cost  -1355.02350232
Pre-training layer 1, epoch 7, cost  -1354.7127464
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 123/123, best error 49.150000 %, best_auc: 0.510950
     new best AUC!: 0.516174016414
     new best AUC!: 0.516179731614
     new best AUC!: 0.52186921336
     new best AUC!: 0.537077360949
     new best AUC!: 0.53710307935
     new best AUC!: 0.539083396201
     new best AUC!: 0.556674782251
     new best AUC!: 0.56210993759
     new best AUC!: 0.562318542395
     new best AUC!: 0.569411105777
     new best AUC!: 0.569436824178
     new best AUC!: 0.569439681778
     new best AUC!: 0.579518437236
     new best AUC!: 0.579575589237
     new best AUC!: 0.585207918981
     new best AUC!: 0.58556226139
     new best AUC!: 0.585573691791
     new best AUC!: 0.585693710994
     new best AUC!: 0.586268088608
     new best AUC!: 0.586836751023
     new best AUC!: 0.593054888782
     new best AUC!: 0.595961068057
     new best AUC!: 0.60740289875
     new best AUC!: 0.60740575635
     new best AUC!: 0.607477196351
     new best AUC!: 0.612923782091
     new best AUC!: 0.617724550214
     new best AUC!: 0.617764556615
     new best AUC!: 0.617767414215
     new best AUC!: 0.618673273438
     new best AUC!: 0.624971423999
     new best AUC!: 0.630386576138
     new best AUC!: 0.630438012939
     new best AUC!: 0.635053037057
     new best AUC!: 0.636081773084
     new best AUC!: 0.636556134696
     new best AUC!: 0.640425325195
     new best AUC!: 0.645463274124
     new best AUC!: 0.647923667787
     epoch 328, minibatch 123/123, best error 47.475000 %, best_auc: 0.647924
     epoch 329, minibatch 123/123, best error 46.675000 %, best_auc: 0.647924
     epoch 330, minibatch 123/123, best error 45.200000 %, best_auc: 0.647924
     epoch 331, minibatch 123/123, best error 39.200000 %, best_auc: 0.647924
     new best AUC!: 0.713734197472
     epoch 332, minibatch 123/123, best error 35.625000 %, best_auc: 0.713734
     new best AUC!: 0.745422124683
     new best AUC!: 0.766765539629
     new best AUC!: 0.783779690465
     new best AUC!: 0.792878289098
     new best AUC!: 0.799187870059
     epoch 337, minibatch 123/123, best error 33.150000 %, best_auc: 0.799188
     new best AUC!: 0.800296618888
     new best AUC!: 0.803482842969
     epoch 340, minibatch 123/123, best error 29.750000 %, best_auc: 0.803483
     epoch 344, minibatch 123/123, best error 27.550000 %, best_auc: 0.803483
     epoch 377, minibatch 123/123, best error 30.325000 %, best_auc: 0.803483
Optimization complete with best validation score of 28.550000 %, obtained at iteration 46371, with test performance 30.325000 %, and best_auc: 0.803483
runtime: 40559.96 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 122
RunningOn slot15@wid-002.discovery.wisc.edu
python th_deep_belief_net.py pcba 122
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 123
RunningOn slot7@wid-002.discovery.wisc.edu
python th_deep_belief_net.py pcba 123
Running Theano Learn Deep Belief Net for pcba, target: aid925.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -80.0614884252
Pre-training layer 0, epoch 1, cost  -67.1044209143
Pre-training layer 0, epoch 2, cost  -62.1108127333
Pre-training layer 0, epoch 3, cost  -58.4337635415
Pre-training layer 0, epoch 4, cost  -55.5910674386
Pre-training layer 0, epoch 5, cost  -53.3176995388
Pre-training layer 0, epoch 6, cost  -51.3855292653
Pre-training layer 0, epoch 7, cost  -49.7354998295
Pre-training layer 1, epoch 0, cost  -1357.733287
Pre-training layer 1, epoch 1, cost  -1339.39045532
Pre-training layer 1, epoch 2, cost  -1338.04623113
Pre-training layer 1, epoch 3, cost  -1337.71561602
Pre-training layer 1, epoch 4, cost  -1337.50630301
Pre-training layer 1, epoch 5, cost  -1337.32713936
Pre-training layer 1, epoch 6, cost  -1337.16461479
Pre-training layer 1, epoch 7, cost  -1337.02283187
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 393/393, best error 1.615385 %, best_auc: 0.645044
     new best AUC!: 0.648417384356
Optimization complete with best validation score of 1.607692 %, obtained at iteration 393, with test performance 1.615385 %, and best_auc: 0.648417
runtime: 15013.92 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 124
RunningOn slot3@opt-a016.discovery.wisc.edu
python th_deep_belief_net.py pcba 124
Running Theano Learn Deep Belief Net for pcba, target: aid926.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -78.0905106485
Pre-training layer 0, epoch 1, cost  -64.9215686453
Pre-training layer 0, epoch 2, cost  -59.4203068464
Pre-training layer 0, epoch 3, cost  -55.4962126522
Pre-training layer 0, epoch 4, cost  -52.5018786676
Pre-training layer 0, epoch 5, cost  -50.1144032014
Pre-training layer 0, epoch 6, cost  -48.0844326174
Pre-training layer 0, epoch 7, cost  -46.4169352204
Pre-training layer 1, epoch 0, cost  -1356.31262673
Pre-training layer 1, epoch 1, cost  -1339.56670616
Pre-training layer 1, epoch 2, cost  -1338.59547849
Pre-training layer 1, epoch 3, cost  -1338.24777697
Pre-training layer 1, epoch 4, cost  -1337.97786701
Pre-training layer 1, epoch 5, cost  -1337.74604702
Pre-training layer 1, epoch 6, cost  -1337.5376543
Pre-training layer 1, epoch 7, cost  -1337.36070404
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 492/492, best error 12.621951 %, best_auc: 0.510491
     new best AUC!: 0.510806630169
     new best AUC!: 0.511728161033
     new best AUC!: 0.512108298833
     new best AUC!: 0.514202595249
     new best AUC!: 0.516077503627
     new best AUC!: 0.519287949328
     new best AUC!: 0.519553338085
     new best AUC!: 0.520857028758
     new best AUC!: 0.520919711055
     new best AUC!: 0.526815385471
     new best AUC!: 0.527387108678
     new best AUC!: 0.528677150786
     new best AUC!: 0.533294915151
     new best AUC!: 0.534054179747
     epoch 51, minibatch 492/492, best error 13.000000 %, best_auc: 0.534054
     epoch 52, minibatch 492/492, best error 12.975610 %, best_auc: 0.534054
     epoch 53, minibatch 492/492, best error 12.957317 %, best_auc: 0.534054
     epoch 54, minibatch 492/492, best error 12.939024 %, best_auc: 0.534054
     epoch 55, minibatch 492/492, best error 12.908537 %, best_auc: 0.534054
     epoch 56, minibatch 492/492, best error 12.896341 %, best_auc: 0.534054
     epoch 57, minibatch 492/492, best error 12.884146 %, best_auc: 0.534054
     new best AUC!: 0.535879043387
     epoch 60, minibatch 492/492, best error 12.884146 %, best_auc: 0.535879
     new best AUC!: 0.538623415882
     epoch 62, minibatch 492/492, best error 12.859756 %, best_auc: 0.538623
     new best AUC!: 0.53938116397
     new best AUC!: 0.540258716125
     epoch 70, minibatch 492/492, best error 12.835366 %, best_auc: 0.540259
     epoch 71, minibatch 492/492, best error 12.835366 %, best_auc: 0.540259
     epoch 75, minibatch 492/492, best error 12.810976 %, best_auc: 0.540259
     epoch 77, minibatch 492/492, best error 12.804878 %, best_auc: 0.540259
     epoch 81, minibatch 492/492, best error 12.810976 %, best_auc: 0.540259
     new best AUC!: 0.540274386699
     epoch 82, minibatch 492/492, best error 12.810976 %, best_auc: 0.540274
     new best AUC!: 0.540868352012
Optimization complete with best validation score of 12.487805 %, obtained at iteration 40344, with test performance 12.810976 %, and best_auc: 0.540868
runtime: 114425.00 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 125
RunningOn slot4@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 125
Running Theano Learn Deep Belief Net for pcba, target: aid927.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -80.5083996719
Pre-training layer 0, epoch 1, cost  -67.30624458
Pre-training layer 0, epoch 2, cost  -62.4643359266
Pre-training layer 0, epoch 3, cost  -58.7655182527
Pre-training layer 0, epoch 4, cost  -55.9422973106
Pre-training layer 0, epoch 5, cost  -53.6995435182
Pre-training layer 0, epoch 6, cost  -51.755799194
Pre-training layer 0, epoch 7, cost  -50.0864587599
Pre-training layer 1, epoch 0, cost  -1364.53134363
Pre-training layer 1, epoch 1, cost  -1346.13586308
Pre-training layer 1, epoch 2, cost  -1343.13513943
Pre-training layer 1, epoch 3, cost  -1342.56869001
Pre-training layer 1, epoch 4, cost  -1342.4572191
Pre-training layer 1, epoch 5, cost  -1342.4332876
Pre-training layer 1, epoch 6, cost  -1342.42900775
Pre-training layer 1, epoch 7, cost  -1342.42847394
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 365/365, best error 2.958678 %, best_auc: 0.500000
Optimization complete with best validation score of 2.958678 %, obtained at iteration 365, with test performance 2.958678 %, and best_auc: 0.500000
runtime: 100914.99 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 126
RunningOn slot3@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 126
Running Theano Learn Deep Belief Net for pcba, target: aid938.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -74.0076832664
Pre-training layer 0, epoch 1, cost  -58.7288867229
Pre-training layer 0, epoch 2, cost  -52.048304218
Pre-training layer 0, epoch 3, cost  -47.6583151939
Pre-training layer 0, epoch 4, cost  -44.4432177504
Pre-training layer 0, epoch 5, cost  -41.8796971485
Pre-training layer 0, epoch 6, cost  -39.8466501639
Pre-training layer 0, epoch 7, cost  -38.0912065031
Pre-training layer 1, epoch 0, cost  -1356.6408845
Pre-training layer 1, epoch 1, cost  -1344.89115156
Pre-training layer 1, epoch 2, cost  -1344.52027182
Pre-training layer 1, epoch 3, cost  -1344.50469628
Pre-training layer 1, epoch 4, cost  -1344.50337762
Pre-training layer 1, epoch 5, cost  -1344.50319062
Pre-training layer 1, epoch 6, cost  -1344.5028512
Pre-training layer 1, epoch 7, cost  -1344.50245645
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 742/742, best error 43.178138 %, best_auc: 0.499929
     new best AUC!: 0.501261915256
     new best AUC!: 0.501262115226
     new best AUC!: 0.512073332464
     new best AUC!: 0.512073732406
     new best AUC!: 0.514358297945
Optimization complete with best validation score of 43.186235 %, obtained at iteration 742, with test performance 43.178138 %, and best_auc: 0.514358
runtime: 119662.42 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 127
RunningOn slot6@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 127
Running Theano Learn Deep Belief Net for pcba, target: aid995.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -77.3381223297
Pre-training layer 0, epoch 1, cost  -63.6682454988
Pre-training layer 0, epoch 2, cost  -57.8276021514
Pre-training layer 0, epoch 3, cost  -53.7492912749
Pre-training layer 0, epoch 4, cost  -50.6614753306
Pre-training layer 0, epoch 5, cost  -48.2225176037
Pre-training layer 0, epoch 6, cost  -46.0868673789
Pre-training layer 0, epoch 7, cost  -44.3621673402
Pre-training layer 1, epoch 0, cost  -1358.0896134
Pre-training layer 1, epoch 1, cost  -1342.4261555
Pre-training layer 1, epoch 2, cost  -1341.26256567
Pre-training layer 1, epoch 3, cost  -1341.15790319
Pre-training layer 1, epoch 4, cost  -1341.14860401
Pre-training layer 1, epoch 5, cost  -1341.14692981
Pre-training layer 1, epoch 6, cost  -1341.14589732
Pre-training layer 1, epoch 7, cost  -1341.14593078
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 547/547, best error 22.895604 %, best_auc: 0.500000
Optimization complete with best validation score of 22.890110 %, obtained at iteration 547, with test performance 22.895604 %, and best_auc: 0.500000
runtime: 67730.24 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 13
RunningOn slot6@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 13
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 14
RunningOn slot4@opt-a004.discovery.wisc.edu
python th_deep_belief_net.py pcba 14
Running Theano Learn Deep Belief Net for pcba, target: aid1688.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -67.6673254387
Pre-training layer 0, epoch 1, cost  -53.6400407011
Pre-training layer 0, epoch 2, cost  -47.8899737524
Pre-training layer 0, epoch 3, cost  -44.1485740104
Pre-training layer 0, epoch 4, cost  -41.4008048975
Pre-training layer 0, epoch 5, cost  -39.216327381
Pre-training layer 0, epoch 6, cost  -37.4531362509
Pre-training layer 0, epoch 7, cost  -35.8804571621
Pre-training layer 1, epoch 0, cost  -1353.65566959
Pre-training layer 1, epoch 1, cost  -1348.84060084
Pre-training layer 1, epoch 2, cost  -1348.83802833
Pre-training layer 1, epoch 3, cost  -1348.83750781
Pre-training layer 1, epoch 4, cost  -1348.83765794
Pre-training layer 1, epoch 5, cost  -1348.8372688
Pre-training layer 1, epoch 6, cost  -1348.83731774
Pre-training layer 1, epoch 7, cost  -1348.83742516
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1736/1736, best error 24.615917 %, best_auc: 0.500000
Optimization complete with best validation score of 24.619377 %, obtained at iteration 1736, with test performance 24.615917 %, and best_auc: 0.500000
runtime: 221818.98 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 15
RunningOn slot4@wid-004.discovery.wisc.edu
python th_deep_belief_net.py pcba 15
Running Theano Learn Deep Belief Net for pcba, target: aid1721.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.4979522048
Pre-training layer 0, epoch 1, cost  -51.7327701436
Pre-training layer 0, epoch 2, cost  -46.1304164001
Pre-training layer 0, epoch 3, cost  -42.5915481739
Pre-training layer 0, epoch 4, cost  -39.9762407606
Pre-training layer 0, epoch 5, cost  -37.9489934178
Pre-training layer 0, epoch 6, cost  -36.2366827981
Pre-training layer 0, epoch 7, cost  -34.8269362221
Pre-training layer 1, epoch 0, cost  -1355.47208614
Pre-training layer 1, epoch 1, cost  -1351.46821083
Pre-training layer 1, epoch 2, cost  -1351.46691192
Pre-training layer 1, epoch 3, cost  -1351.46658785
Pre-training layer 1, epoch 4, cost  -1351.46660799
Pre-training layer 1, epoch 5, cost  -1351.46622766
Pre-training layer 1, epoch 6, cost  -1351.46635145
Pre-training layer 1, epoch 7, cost  -1351.46615627
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1945/1945, best error 10.041667 %, best_auc: 0.500000
     new best AUC!: 0.500008571918
Optimization complete with best validation score of 10.044753 %, obtained at iteration 1945, with test performance 10.041667 %, and best_auc: 0.500009
runtime: 44171.65 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 16
RunningOn slot10@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 16
Running Theano Learn Deep Belief Net for pcba, target: aid2100.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.1470671181
Pre-training layer 0, epoch 1, cost  -52.1430773245
Pre-training layer 0, epoch 2, cost  -46.5327431291
Pre-training layer 0, epoch 3, cost  -42.9650821632
Pre-training layer 0, epoch 4, cost  -40.3496987849
Pre-training layer 0, epoch 5, cost  -38.2494101097
Pre-training layer 0, epoch 6, cost  -36.5366046725
Pre-training layer 0, epoch 7, cost  -35.1018989493
Pre-training layer 1, epoch 0, cost  -1354.31299012
Pre-training layer 1, epoch 1, cost  -1350.33431984
Pre-training layer 1, epoch 2, cost  -1350.33316352
Pre-training layer 1, epoch 3, cost  -1350.33290291
Pre-training layer 1, epoch 4, cost  -1350.33287684
Pre-training layer 1, epoch 5, cost  -1350.33244624
Pre-training layer 1, epoch 6, cost  -1350.33284503
Pre-training layer 1, epoch 7, cost  -1350.33240873
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2015/2015, best error 10.321908 %, best_auc: 0.500000
     new best AUC!: 0.500008301649
Optimization complete with best validation score of 10.318927 %, obtained at iteration 2015, with test performance 10.321908 %, and best_auc: 0.500008
runtime: 38128.61 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 17
RunningOn slot7@wid-004.discovery.wisc.edu
python th_deep_belief_net.py pcba 17
Running Theano Learn Deep Belief Net for pcba, target: aid2101.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -67.1292911414
Pre-training layer 0, epoch 1, cost  -53.2382280914
Pre-training layer 0, epoch 2, cost  -47.6613709391
Pre-training layer 0, epoch 3, cost  -44.1175398755
Pre-training layer 0, epoch 4, cost  -41.5175285643
Pre-training layer 0, epoch 5, cost  -39.4780911653
Pre-training layer 0, epoch 6, cost  -37.7687714869
Pre-training layer 0, epoch 7, cost  -36.3347819778
Pre-training layer 1, epoch 0, cost  -1349.91337911
Pre-training layer 1, epoch 1, cost  -1345.29623625
Pre-training layer 1, epoch 2, cost  -1345.29490505
Pre-training layer 1, epoch 3, cost  -1345.29482075
Pre-training layer 1, epoch 4, cost  -1345.29447732
Pre-training layer 1, epoch 5, cost  -1345.29441665
Pre-training layer 1, epoch 6, cost  -1345.29470092
Pre-training layer 1, epoch 7, cost  -1345.29397171
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1979/1979, best error 2.593323 %, best_auc: 0.500000
Optimization complete with best validation score of 2.593323 %, obtained at iteration 1979, with test performance 2.593323 %, and best_auc: 0.500000
runtime: 30667.90 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 18
RunningOn slot12@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 18
Running Theano Learn Deep Belief Net for pcba, target: aid2147.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.4390023875
Pre-training layer 0, epoch 1, cost  -52.2923814275
Pre-training layer 0, epoch 2, cost  -46.5715422859
Pre-training layer 0, epoch 3, cost  -42.8152538362
Pre-training layer 0, epoch 4, cost  -40.1147492231
Pre-training layer 0, epoch 5, cost  -37.9059010029
Pre-training layer 0, epoch 6, cost  -36.1370684425
Pre-training layer 0, epoch 7, cost  -34.6132339819
Pre-training layer 1, epoch 0, cost  -1352.18993758
Pre-training layer 1, epoch 1, cost  -1347.84103078
Pre-training layer 1, epoch 2, cost  -1347.83966189
Pre-training layer 1, epoch 3, cost  -1347.83964566
Pre-training layer 1, epoch 4, cost  -1347.83937091
Pre-training layer 1, epoch 5, cost  -1347.83934902
Pre-training layer 1, epoch 6, cost  -1347.83956884
Pre-training layer 1, epoch 7, cost  -1347.83881803
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1966/1966, best error 31.780153 %, best_auc: 0.500000
Optimization complete with best validation score of 31.783206 %, obtained at iteration 1966, with test performance 31.780153 %, and best_auc: 0.500000
runtime: 33513.50 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 19
RunningOn slot14@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 19
Running Theano Learn Deep Belief Net for pcba, target: aid2242.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -69.2366685134
Pre-training layer 0, epoch 1, cost  -55.5865429463
Pre-training layer 0, epoch 2, cost  -49.9502233227
Pre-training layer 0, epoch 3, cost  -46.3400622009
Pre-training layer 0, epoch 4, cost  -43.6474684651
Pre-training layer 0, epoch 5, cost  -41.5446034251
Pre-training layer 0, epoch 6, cost  -39.8279635648
Pre-training layer 0, epoch 7, cost  -38.3578351091
Pre-training layer 1, epoch 0, cost  -1356.10498381
Pre-training layer 1, epoch 1, cost  -1349.98416208
Pre-training layer 1, epoch 2, cost  -1349.96910398
Pre-training layer 1, epoch 3, cost  -1349.96852644
Pre-training layer 1, epoch 4, cost  -1349.96830465
Pre-training layer 1, epoch 5, cost  -1349.96804339
Pre-training layer 1, epoch 6, cost  -1349.96840121
Pre-training layer 1, epoch 7, cost  -1349.96794416
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1319/1319, best error 9.760820 %, best_auc: 0.500000
Optimization complete with best validation score of 9.753986 %, obtained at iteration 1319, with test performance 9.760820 %, and best_auc: 0.500000
runtime: 24746.31 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 2
RunningOn slot6@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 2
Running Theano Learn Deep Belief Net for pcba, target: aid1452.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -73.1630429386
Pre-training layer 0, epoch 1, cost  -60.3763189428
Pre-training layer 0, epoch 2, cost  -54.9762611274
Pre-training layer 0, epoch 3, cost  -51.3827386427
Pre-training layer 0, epoch 4, cost  -48.7211231135
Pre-training layer 0, epoch 5, cost  -46.6061747786
Pre-training layer 0, epoch 6, cost  -44.8308575787
Pre-training layer 0, epoch 7, cost  -43.3642691139
Pre-training layer 1, epoch 0, cost  -1349.47367194
Pre-training layer 1, epoch 1, cost  -1339.53362147
Pre-training layer 1, epoch 2, cost  -1338.87012863
Pre-training layer 1, epoch 3, cost  -1338.43265322
Pre-training layer 1, epoch 4, cost  -1338.1279708
Pre-training layer 1, epoch 5, cost  -1337.91523597
Pre-training layer 1, epoch 6, cost  -1337.77583808
Pre-training layer 1, epoch 7, cost  -1337.68430623
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 942/942, best error 3.348243 %, best_auc: 0.481963
     new best AUC!: 0.488265524827
     new best AUC!: 0.493052395555
     new best AUC!: 0.495519786706
     new best AUC!: 0.499480896325
     new best AUC!: 0.502579502471
     new best AUC!: 0.507394636557
     new best AUC!: 0.512562591562
     new best AUC!: 0.517020665225
     new best AUC!: 0.521716622152
     new best AUC!: 0.524574989755
     new best AUC!: 0.528111206892
     new best AUC!: 0.530153705562
     new best AUC!: 0.534807267451
     new best AUC!: 0.544251468517
     new best AUC!: 0.549986103849
     new best AUC!: 0.552889221768
     new best AUC!: 0.560573086688
     new best AUC!: 0.561777576794
     new best AUC!: 0.563268939983
     new best AUC!: 0.566677500954
     epoch 24, minibatch 942/942, best error 3.444089 %, best_auc: 0.566678
     epoch 25, minibatch 942/942, best error 3.421725 %, best_auc: 0.566678
     epoch 26, minibatch 942/942, best error 3.402556 %, best_auc: 0.566678
     epoch 27, minibatch 942/942, best error 3.376997 %, best_auc: 0.566678
     epoch 28, minibatch 942/942, best error 3.357827 %, best_auc: 0.566678
     epoch 29, minibatch 942/942, best error 3.351438 %, best_auc: 0.566678
     new best AUC!: 0.567310129116
     epoch 30, minibatch 942/942, best error 3.341853 %, best_auc: 0.567310
     epoch 31, minibatch 942/942, best error 3.338658 %, best_auc: 0.567310
     epoch 32, minibatch 942/942, best error 3.329073 %, best_auc: 0.567310
     new best AUC!: 0.567503262063
     epoch 33, minibatch 942/942, best error 3.325879 %, best_auc: 0.567503
     new best AUC!: 0.567868330436
     epoch 34, minibatch 942/942, best error 3.322684 %, best_auc: 0.567868
     epoch 35, minibatch 942/942, best error 3.316294 %, best_auc: 0.567868
     new best AUC!: 0.568771344723
     epoch 36, minibatch 942/942, best error 3.313099 %, best_auc: 0.568771
     new best AUC!: 0.570083235589
     new best AUC!: 0.571039479201
     epoch 39, minibatch 942/942, best error 3.303514 %, best_auc: 0.571039
     new best AUC!: 0.572441812812
     epoch 40, minibatch 942/942, best error 3.303514 %, best_auc: 0.572442
     epoch 45, minibatch 942/942, best error 3.300319 %, best_auc: 0.572442
     new best AUC!: 0.573331166476
     new best AUC!: 0.57388889674
     new best AUC!: 0.575765583709
     epoch 50, minibatch 942/942, best error 3.284345 %, best_auc: 0.575766
     new best AUC!: 0.576313421798
     new best AUC!: 0.576352519443
     epoch 53, minibatch 942/942, best error 3.277955 %, best_auc: 0.576353
     new best AUC!: 0.576947463128
     epoch 56, minibatch 942/942, best error 3.277955 %, best_auc: 0.576947
     new best AUC!: 0.57725788901
     new best AUC!: 0.577390726792
     new best AUC!: 0.578675296412
     new best AUC!: 0.57869884921
     new best AUC!: 0.579959394976
     new best AUC!: 0.580214707309
     epoch 90, minibatch 942/942, best error 3.274760 %, best_auc: 0.580215
     epoch 104, minibatch 942/942, best error 3.274760 %, best_auc: 0.580215
     epoch 108, minibatch 942/942, best error 3.274760 %, best_auc: 0.580215
     epoch 125, minibatch 942/942, best error 3.271565 %, best_auc: 0.580215
     epoch 132, minibatch 942/942, best error 3.271565 %, best_auc: 0.580215
Optimization complete with best validation score of 2.993610 %, obtained at iteration 124344, with test performance 3.271565 %, and best_auc: 0.580215
runtime: 423607.14 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 20
RunningOn slot16@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 20
Running Theano Learn Deep Belief Net for pcba, target: aid2326.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -67.6378991085
Pre-training layer 0, epoch 1, cost  -53.5514579983
Pre-training layer 0, epoch 2, cost  -47.7924163934
Pre-training layer 0, epoch 3, cost  -44.1333327012
Pre-training layer 0, epoch 4, cost  -41.4078894837
Pre-training layer 0, epoch 5, cost  -39.2619257792
Pre-training layer 0, epoch 6, cost  -37.491511143
Pre-training layer 0, epoch 7, cost  -36.0102198261
Pre-training layer 1, epoch 0, cost  -1354.14942204
Pre-training layer 1, epoch 1, cost  -1349.60673941
Pre-training layer 1, epoch 2, cost  -1349.60482591
Pre-training layer 1, epoch 3, cost  -1349.60415746
Pre-training layer 1, epoch 4, cost  -1349.60429813
Pre-training layer 1, epoch 5, cost  -1349.6040277
Pre-training layer 1, epoch 6, cost  -1349.60449834
Pre-training layer 1, epoch 7, cost  -1349.60420023
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1802/1802, best error 10.633333 %, best_auc: 0.500000
Optimization complete with best validation score of 10.626667 %, obtained at iteration 1802, with test performance 10.633333 %, and best_auc: 0.500000
runtime: 31458.77 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 21
RunningOn slot9@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 21
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 22
RunningOn slot11@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 22
Running Theano Learn Deep Belief Net for pcba, target: aid2517.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.25856348
Pre-training layer 0, epoch 1, cost  -52.1752091844
Pre-training layer 0, epoch 2, cost  -46.5444315586
Pre-training layer 0, epoch 3, cost  -42.9059208155
Pre-training layer 0, epoch 4, cost  -40.2826472392
Pre-training layer 0, epoch 5, cost  -38.1670619262
Pre-training layer 0, epoch 6, cost  -36.4350750214
Pre-training layer 0, epoch 7, cost  -34.9707318014
Pre-training layer 1, epoch 0, cost  -1348.09100472
Pre-training layer 1, epoch 1, cost  -1343.92905747
Pre-training layer 1, epoch 2, cost  -1343.92814193
Pre-training layer 1, epoch 3, cost  -1343.92800641
Pre-training layer 1, epoch 4, cost  -1343.92779353
Pre-training layer 1, epoch 5, cost  -1343.92794959
Pre-training layer 1, epoch 6, cost  -1343.92746969
Pre-training layer 1, epoch 7, cost  -1343.92747223
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2273/2273, best error 8.992074 %, best_auc: 0.500000
Optimization complete with best validation score of 8.986790 %, obtained at iteration 2273, with test performance 8.992074 %, and best_auc: 0.500000
runtime: 37296.18 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 23
RunningOn slot13@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 23
Running Theano Learn Deep Belief Net for pcba, target: aid2528.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.8397218924
Pre-training layer 0, epoch 1, cost  -52.8672318745
Pre-training layer 0, epoch 2, cost  -47.2173119417
Pre-training layer 0, epoch 3, cost  -43.5828204769
Pre-training layer 0, epoch 4, cost  -40.947564036
Pre-training layer 0, epoch 5, cost  -38.8398492499
Pre-training layer 0, epoch 6, cost  -37.1191356737
Pre-training layer 0, epoch 7, cost  -35.6546078403
Pre-training layer 1, epoch 0, cost  -1350.49199355
Pre-training layer 1, epoch 1, cost  -1346.45319003
Pre-training layer 1, epoch 2, cost  -1346.45199974
Pre-training layer 1, epoch 3, cost  -1346.4517118
Pre-training layer 1, epoch 4, cost  -1346.45153561
Pre-training layer 1, epoch 5, cost  -1346.45182829
Pre-training layer 1, epoch 6, cost  -1346.45140653
Pre-training layer 1, epoch 7, cost  -1346.45142548
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2201/2201, best error 5.319236 %, best_auc: 0.500000
Optimization complete with best validation score of 5.319236 %, obtained at iteration 2201, with test performance 5.319236 %, and best_auc: 0.500000
runtime: 34842.08 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 24
RunningOn slot6@opt-a004.discovery.wisc.edu
python th_deep_belief_net.py pcba 24
Running Theano Learn Deep Belief Net for pcba, target: aid2546.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -59.4057189488
Pre-training layer 0, epoch 1, cost  -44.7304424684
Pre-training layer 0, epoch 2, cost  -39.1472713569
Pre-training layer 0, epoch 3, cost  -35.578538582
Pre-training layer 0, epoch 4, cost  -32.9671791754
Pre-training layer 0, epoch 5, cost  -30.9193785521
Pre-training layer 0, epoch 6, cost  -29.2322107197
Pre-training layer 0, epoch 7, cost  -27.8084430584
Pre-training layer 1, epoch 0, cost  -1353.13038332
Pre-training layer 1, epoch 1, cost  -1350.85735301
Pre-training layer 1, epoch 2, cost  -1350.85707711
Pre-training layer 1, epoch 3, cost  -1350.85729427
Pre-training layer 1, epoch 4, cost  -1350.85674455
Pre-training layer 1, epoch 5, cost  -1350.85708815
Pre-training layer 1, epoch 6, cost  -1350.85698484
Pre-training layer 1, epoch 7, cost  -1350.85697409
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3471/3471, best error 49.262111 %, best_auc: 0.500000
     new best AUC!: 0.500017035485
Optimization complete with best validation score of 49.269896 %, obtained at iteration 3471, with test performance 49.262111 %, and best_auc: 0.500017
runtime: 392378.87 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 25
RunningOn slot4@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 25
Running Theano Learn Deep Belief Net for pcba, target: aid2549.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -69.4234230289
Pre-training layer 0, epoch 1, cost  -55.9307680423
Pre-training layer 0, epoch 2, cost  -50.2811527218
Pre-training layer 0, epoch 3, cost  -46.6215343272
Pre-training layer 0, epoch 4, cost  -43.9585924991
Pre-training layer 0, epoch 5, cost  -41.8162851066
Pre-training layer 0, epoch 6, cost  -40.0313786257
Pre-training layer 0, epoch 7, cost  -38.5089528136
Pre-training layer 1, epoch 0, cost  -1349.61793244
Pre-training layer 1, epoch 1, cost  -1343.77430487
Pre-training layer 1, epoch 2, cost  -1343.76877908
Pre-training layer 1, epoch 3, cost  -1343.76827273
Pre-training layer 1, epoch 4, cost  -1343.76822396
Pre-training layer 1, epoch 5, cost  -1343.76811918
Pre-training layer 1, epoch 6, cost  -1343.76786242
Pre-training layer 1, epoch 7, cost  -1343.76857275
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1620/1620, best error 13.442593 %, best_auc: 0.500000
Optimization complete with best validation score of 13.442593 %, obtained at iteration 1620, with test performance 13.442593 %, and best_auc: 0.500000
runtime: 28230.63 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 26
RunningOn slot6@opt-a005.discovery.wisc.edu
python th_deep_belief_net.py pcba 26
Running Theano Learn Deep Belief Net for pcba, target: aid2551.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -59.4359186959
Pre-training layer 0, epoch 1, cost  -45.0828883691
Pre-training layer 0, epoch 2, cost  -39.6755078344
Pre-training layer 0, epoch 3, cost  -36.2130160329
Pre-training layer 0, epoch 4, cost  -33.6887906576
Pre-training layer 0, epoch 5, cost  -31.6671268183
Pre-training layer 0, epoch 6, cost  -30.0363873311
Pre-training layer 0, epoch 7, cost  -28.628330223
Pre-training layer 1, epoch 0, cost  -1353.02282514
Pre-training layer 1, epoch 1, cost  -1350.71774178
Pre-training layer 1, epoch 2, cost  -1350.717328
Pre-training layer 1, epoch 3, cost  -1350.71735147
Pre-training layer 1, epoch 4, cost  -1350.71682734
Pre-training layer 1, epoch 5, cost  -1350.71722041
Pre-training layer 1, epoch 6, cost  -1350.71707879
Pre-training layer 1, epoch 7, cost  -1350.71722198
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3433/3433, best error 49.529720 %, best_auc: 0.500000
     new best AUC!: 0.500017314818
     new best AUC!: 0.500043287045
Optimization complete with best validation score of 49.529720 %, obtained at iteration 3433, with test performance 49.529720 %, and best_auc: 0.500043
runtime: 67813.02 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 27
RunningOn slot6@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 27
Running Theano Learn Deep Belief Net for pcba, target: aid2662.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -67.358921017
Pre-training layer 0, epoch 1, cost  -53.7012725949
Pre-training layer 0, epoch 2, cost  -48.1408719608
Pre-training layer 0, epoch 3, cost  -44.6355128555
Pre-training layer 0, epoch 4, cost  -42.0232371588
Pre-training layer 0, epoch 5, cost  -39.9771671445
Pre-training layer 0, epoch 6, cost  -38.2974157228
Pre-training layer 0, epoch 7, cost  -36.8755088246
Pre-training layer 1, epoch 0, cost  -1352.8991092
Pre-training layer 1, epoch 1, cost  -1348.12466584
Pre-training layer 1, epoch 2, cost  -1348.12214788
Pre-training layer 1, epoch 3, cost  -1348.12148275
Pre-training layer 1, epoch 4, cost  -1348.12168728
Pre-training layer 1, epoch 5, cost  -1348.12139408
Pre-training layer 1, epoch 6, cost  -1348.12164625
Pre-training layer 1, epoch 7, cost  -1348.12176147
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1783/1783, best error 1.111111 %, best_auc: 0.500000
Optimization complete with best validation score of 1.102694 %, obtained at iteration 1783, with test performance 1.111111 %, and best_auc: 0.500000
runtime: 143939.23 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 28
RunningOn slot19@roy-exec-1.discovery.wisc.edu
python th_deep_belief_net.py pcba 28
Running Theano Learn Deep Belief Net for pcba, target: aid2675.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -67.6097155367
Pre-training layer 0, epoch 1, cost  -53.9418330118
Pre-training layer 0, epoch 2, cost  -48.3764862525
Pre-training layer 0, epoch 3, cost  -44.8098912406
Pre-training layer 0, epoch 4, cost  -42.1797346634
Pre-training layer 0, epoch 5, cost  -40.1222868136
Pre-training layer 0, epoch 6, cost  -38.4148856898
Pre-training layer 0, epoch 7, cost  -36.9625623862
Pre-training layer 1, epoch 0, cost  -1354.41976437
Pre-training layer 1, epoch 1, cost  -1349.5941157
Pre-training layer 1, epoch 2, cost  -1349.59144283
Pre-training layer 1, epoch 3, cost  -1349.59097977
Pre-training layer 1, epoch 4, cost  -1349.5911383
Pre-training layer 1, epoch 5, cost  -1349.59089089
Pre-training layer 1, epoch 6, cost  -1349.5907341
Pre-training layer 1, epoch 7, cost  -1349.59118467
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1694/1694, best error 1.010638 %, best_auc: 0.500000
Optimization complete with best validation score of 1.008865 %, obtained at iteration 1694, with test performance 1.010638 %, and best_auc: 0.500000
runtime: 88399.49 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 29
RunningOn slot4@opt-a003.discovery.wisc.edu
python th_deep_belief_net.py pcba 29
Running Theano Learn Deep Belief Net for pcba, target: aid2676.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.2130684623
Pre-training layer 0, epoch 1, cost  -50.541832737
Pre-training layer 0, epoch 2, cost  -44.7991089342
Pre-training layer 0, epoch 3, cost  -41.1517162348
Pre-training layer 0, epoch 4, cost  -38.4883828743
Pre-training layer 0, epoch 5, cost  -36.4180842669
Pre-training layer 0, epoch 6, cost  -34.7069870274
Pre-training layer 0, epoch 7, cost  -33.2429655553
Pre-training layer 1, epoch 0, cost  -1355.51699629
Pre-training layer 1, epoch 1, cost  -1352.28737248
Pre-training layer 1, epoch 2, cost  -1352.28668726
Pre-training layer 1, epoch 3, cost  -1352.28674286
Pre-training layer 1, epoch 4, cost  -1352.28620891
Pre-training layer 1, epoch 5, cost  -1352.28670812
Pre-training layer 1, epoch 6, cost  -1352.2860609
Pre-training layer 1, epoch 7, cost  -1352.28641266
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2361/2361, best error 8.232529 %, best_auc: 0.500000
Optimization complete with best validation score of 8.233799 %, obtained at iteration 2361, with test performance 8.232529 %, and best_auc: 0.500000
runtime: 289626.52 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 3
RunningOn slot3@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 3
Running Theano Learn Deep Belief Net for pcba, target: aid1454.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -74.6657701642
Pre-training layer 0, epoch 1, cost  -61.8448686412
Pre-training layer 0, epoch 2, cost  -56.2375362356
Pre-training layer 0, epoch 3, cost  -52.4673570023
Pre-training layer 0, epoch 4, cost  -49.6702021766
Pre-training layer 0, epoch 5, cost  -47.4064734635
Pre-training layer 0, epoch 6, cost  -45.5761504567
Pre-training layer 0, epoch 7, cost  -43.9922601645
Pre-training layer 1, epoch 0, cost  -1355.53079163
Pre-training layer 1, epoch 1, cost  -1345.48466303
Pre-training layer 1, epoch 2, cost  -1345.31311595
Pre-training layer 1, epoch 3, cost  -1345.30985332
Pre-training layer 1, epoch 4, cost  -1345.30921461
Pre-training layer 1, epoch 5, cost  -1345.30895334
Pre-training layer 1, epoch 6, cost  -1345.30852726
Pre-training layer 1, epoch 7, cost  -1345.30855246
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 877/877, best error 10.476027 %, best_auc: 0.500000
     new best AUC!: 0.504844250979
Optimization complete with best validation score of 10.476027 %, obtained at iteration 877, with test performance 10.476027 %, and best_auc: 0.504844
runtime: 144406.44 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 30
RunningOn slot3@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 30
Running Theano Learn Deep Belief Net for pcba, target: aid411.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -71.6030022984
Pre-training layer 0, epoch 1, cost  -57.849229484
Pre-training layer 0, epoch 2, cost  -51.8117911941
Pre-training layer 0, epoch 3, cost  -47.7320809143
Pre-training layer 0, epoch 4, cost  -44.6379977019
Pre-training layer 0, epoch 5, cost  -42.1570239298
Pre-training layer 0, epoch 6, cost  -40.1711083007
Pre-training layer 0, epoch 7, cost  -38.5158563681
Pre-training layer 1, epoch 0, cost  -1360.65013523
Pre-training layer 1, epoch 1, cost  -1349.7017841
Pre-training layer 1, epoch 2, cost  -1349.30893396
Pre-training layer 1, epoch 3, cost  -1349.29140603
Pre-training layer 1, epoch 4, cost  -1349.29077591
Pre-training layer 1, epoch 5, cost  -1349.28987827
Pre-training layer 1, epoch 6, cost  -1349.28994587
Pre-training layer 1, epoch 7, cost  -1349.28915195
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 700/700, best error 40.163090 %, best_auc: 0.500036
     new best AUC!: 0.500143410297
     new best AUC!: 0.500286820594
     new best AUC!: 0.500394378316
     new best AUC!: 0.500430230891
Optimization complete with best validation score of 40.167382 %, obtained at iteration 700, with test performance 40.163090 %, and best_auc: 0.500430
runtime: 126347.70 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 31
RunningOn slot2@roy-exec-1.discovery.wisc.edu
python th_deep_belief_net.py pcba 31
Running Theano Learn Deep Belief Net for pcba, target: aid463254.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.6125385411
Pre-training layer 0, epoch 1, cost  -52.5106940892
Pre-training layer 0, epoch 2, cost  -46.9099518076
Pre-training layer 0, epoch 3, cost  -43.3480409111
Pre-training layer 0, epoch 4, cost  -40.7091303531
Pre-training layer 0, epoch 5, cost  -38.686860878
Pre-training layer 0, epoch 6, cost  -36.9760384426
Pre-training layer 0, epoch 7, cost  -35.5389000239
Pre-training layer 1, epoch 0, cost  -1354.28111655
Pre-training layer 1, epoch 1, cost  -1350.22496277
Pre-training layer 1, epoch 2, cost  -1350.2234289
Pre-training layer 1, epoch 3, cost  -1350.22327796
Pre-training layer 1, epoch 4, cost  -1350.22339394
Pre-training layer 1, epoch 5, cost  -1350.22289048
Pre-training layer 1, epoch 6, cost  -1350.22336797
Pre-training layer 1, epoch 7, cost  -1350.22275078
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1991/1991, best error 0.360483 %, best_auc: 0.500000
Optimization complete with best validation score of 0.360483 %, obtained at iteration 1991, with test performance 0.360483 %, and best_auc: 0.500000
runtime: 104498.22 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 32
RunningOn slot15@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 32
Running Theano Learn Deep Belief Net for pcba, target: aid485281.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.7205141306
Pre-training layer 0, epoch 1, cost  -52.7200042746
Pre-training layer 0, epoch 2, cost  -47.137437694
Pre-training layer 0, epoch 3, cost  -43.5258335693
Pre-training layer 0, epoch 4, cost  -40.9009210833
Pre-training layer 0, epoch 5, cost  -38.8408395599
Pre-training layer 0, epoch 6, cost  -37.1492520083
Pre-training layer 0, epoch 7, cost  -35.7057802446
Pre-training layer 1, epoch 0, cost  -1352.6623406
Pre-training layer 1, epoch 1, cost  -1348.65046973
Pre-training layer 1, epoch 2, cost  -1348.64938868
Pre-training layer 1, epoch 3, cost  -1348.64898654
Pre-training layer 1, epoch 4, cost  -1348.64889184
Pre-training layer 1, epoch 5, cost  -1348.64885525
Pre-training layer 1, epoch 6, cost  -1348.64906584
Pre-training layer 1, epoch 7, cost  -1348.64805939
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2093/2093, best error 2.147776 %, best_auc: 0.500000
Optimization complete with best validation score of 2.150646 %, obtained at iteration 2093, with test performance 2.147776 %, and best_auc: 0.500000
runtime: 34888.35 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 33
RunningOn slot17@roy-exec-1.discovery.wisc.edu
python th_deep_belief_net.py pcba 33
Running Theano Learn Deep Belief Net for pcba, target: aid485290.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.8587915697
Pre-training layer 0, epoch 1, cost  -52.5659652686
Pre-training layer 0, epoch 2, cost  -46.8332564633
Pre-training layer 0, epoch 3, cost  -43.1503355466
Pre-training layer 0, epoch 4, cost  -40.4815998188
Pre-training layer 0, epoch 5, cost  -38.3568244809
Pre-training layer 0, epoch 6, cost  -36.6091625914
Pre-training layer 0, epoch 7, cost  -35.1166961801
Pre-training layer 1, epoch 0, cost  -1348.82185369
Pre-training layer 1, epoch 1, cost  -1344.6533689
Pre-training layer 1, epoch 2, cost  -1344.65239524
Pre-training layer 1, epoch 3, cost  -1344.65240141
Pre-training layer 1, epoch 4, cost  -1344.65190928
Pre-training layer 1, epoch 5, cost  -1344.65240585
Pre-training layer 1, epoch 6, cost  -1344.65180724
Pre-training layer 1, epoch 7, cost  -1344.65194956
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2230/2230, best error 7.550471 %, best_auc: 0.500000
Optimization complete with best validation score of 7.550471 %, obtained at iteration 2230, with test performance 7.550471 %, and best_auc: 0.500000
runtime: 113141.71 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 34
RunningOn slot3@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 34
Running Theano Learn Deep Belief Net for pcba, target: aid485294.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.9907518544
Pre-training layer 0, epoch 1, cost  -51.8127446304
Pre-training layer 0, epoch 2, cost  -46.1654371891
Pre-training layer 0, epoch 3, cost  -42.5606021263
Pre-training layer 0, epoch 4, cost  -39.9353593714
Pre-training layer 0, epoch 5, cost  -37.8453858488
Pre-training layer 0, epoch 6, cost  -36.1312135821
Pre-training layer 0, epoch 7, cost  -34.6974246624
Pre-training layer 1, epoch 0, cost  -1353.96806803
Pre-training layer 1, epoch 1, cost  -1350.31059923
Pre-training layer 1, epoch 2, cost  -1350.30985808
Pre-training layer 1, epoch 3, cost  -1350.30970795
Pre-training layer 1, epoch 4, cost  -1350.30954591
Pre-training layer 1, epoch 5, cost  -1350.30967844
Pre-training layer 1, epoch 6, cost  -1350.30943434
Pre-training layer 1, epoch 7, cost  -1350.30937149
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2199/2199, best error 1.187158 %, best_auc: 0.500000
Optimization complete with best validation score of 1.185792 %, obtained at iteration 2199, with test performance 1.187158 %, and best_auc: 0.500000
runtime: 160388.89 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 35
RunningOn slot3@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 35
Running Theano Learn Deep Belief Net for pcba, target: aid485297.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -56.0990783949
Pre-training layer 0, epoch 1, cost  -41.5546550687
Pre-training layer 0, epoch 2, cost  -36.1313506111
Pre-training layer 0, epoch 3, cost  -32.7518484425
Pre-training layer 0, epoch 4, cost  -30.3064697133
Pre-training layer 0, epoch 5, cost  -28.4046948574
Pre-training layer 0, epoch 6, cost  -26.8488457707
Pre-training layer 0, epoch 7, cost  -25.5421807373
Pre-training layer 1, epoch 0, cost  -1358.68844571
Pre-training layer 1, epoch 1, cost  -1356.80346473
Pre-training layer 1, epoch 2, cost  -1356.80347757
Pre-training layer 1, epoch 3, cost  -1356.80363205
Pre-training layer 1, epoch 4, cost  -1356.80299143
Pre-training layer 1, epoch 5, cost  -1356.80333395
Pre-training layer 1, epoch 6, cost  -1356.8030288
Pre-training layer 1, epoch 7, cost  -1356.80309667
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3512/3512, best error 46.781197 %, best_auc: 0.500000
     new best AUC!: 0.500024078593
Optimization complete with best validation score of 46.777778 %, obtained at iteration 3512, with test performance 46.781197 %, and best_auc: 0.500024
runtime: 356825.87 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 36
RunningOn slot6@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 36
Running Theano Learn Deep Belief Net for pcba, target: aid485313.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -57.3576872537
Pre-training layer 0, epoch 1, cost  -42.8832549239
Pre-training layer 0, epoch 2, cost  -37.3884519403
Pre-training layer 0, epoch 3, cost  -33.9802145797
Pre-training layer 0, epoch 4, cost  -31.5102663592
Pre-training layer 0, epoch 5, cost  -29.5830973592
Pre-training layer 0, epoch 6, cost  -27.9953101606
Pre-training layer 0, epoch 7, cost  -26.6911379022
Pre-training layer 1, epoch 0, cost  -1358.2759767
Pre-training layer 1, epoch 1, cost  -1356.1837497
Pre-training layer 1, epoch 2, cost  -1356.18347284
Pre-training layer 1, epoch 3, cost  -1356.18349386
Pre-training layer 1, epoch 4, cost  -1356.18337637
Pre-training layer 1, epoch 5, cost  -1356.18316404
Pre-training layer 1, epoch 6, cost  -1356.18348328
Pre-training layer 1, epoch 7, cost  -1356.18326686
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3241/3241, best error 42.023148 %, best_auc: 0.500000
     new best AUC!: 0.500023952478
     new best AUC!: 0.500055889116
Optimization complete with best validation score of 42.020370 %, obtained at iteration 3241, with test performance 42.023148 %, and best_auc: 0.500056
runtime: 220646.73 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 37
RunningOn slot4@opt-a004.discovery.wisc.edu
python th_deep_belief_net.py pcba 37
Running Theano Learn Deep Belief Net for pcba, target: aid485314.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -63.8449698708
Pre-training layer 0, epoch 1, cost  -48.741366541
Pre-training layer 0, epoch 2, cost  -42.7863572272
Pre-training layer 0, epoch 3, cost  -39.0146682536
Pre-training layer 0, epoch 4, cost  -36.2474171867
Pre-training layer 0, epoch 5, cost  -34.0935346922
Pre-training layer 0, epoch 6, cost  -32.327639286
Pre-training layer 0, epoch 7, cost  -30.8394870729
Pre-training layer 1, epoch 0, cost  -1356.28652703
Pre-training layer 1, epoch 1, cost  -1353.67049641
Pre-training layer 1, epoch 2, cost  -1353.66985905
Pre-training layer 1, epoch 3, cost  -1353.66980239
Pre-training layer 1, epoch 4, cost  -1353.66980684
Pre-training layer 1, epoch 5, cost  -1353.66922641
Pre-training layer 1, epoch 6, cost  -1353.66944224
Pre-training layer 1, epoch 7, cost  -1353.6694005
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2788/2788, best error 28.988159 %, best_auc: 0.500000
Optimization complete with best validation score of 28.992465 %, obtained at iteration 2788, with test performance 28.988159 %, and best_auc: 0.500000
runtime: 309890.84 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 38
RunningOn slot1@opt-a006.discovery.wisc.edu
python th_deep_belief_net.py pcba 38
Running Theano Learn Deep Belief Net for pcba, target: aid485341.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.4197224462
Pre-training layer 0, epoch 1, cost  -50.9704599202
Pre-training layer 0, epoch 2, cost  -45.1846985292
Pre-training layer 0, epoch 3, cost  -41.5338298924
Pre-training layer 0, epoch 4, cost  -38.7920056878
Pre-training layer 0, epoch 5, cost  -36.684645884
Pre-training layer 0, epoch 6, cost  -34.9687721808
Pre-training layer 0, epoch 7, cost  -33.5006770838
Pre-training layer 1, epoch 0, cost  -1355.20329926
Pre-training layer 1, epoch 1, cost  -1351.83341131
Pre-training layer 1, epoch 2, cost  -1351.8325971
Pre-training layer 1, epoch 3, cost  -1351.83240676
Pre-training layer 1, epoch 4, cost  -1351.83207516
Pre-training layer 1, epoch 5, cost  -1351.83244144
Pre-training layer 1, epoch 6, cost  -1351.83185696
Pre-training layer 1, epoch 7, cost  -1351.83195024
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2285/2285, best error 13.591327 %, best_auc: 0.500000
Optimization complete with best validation score of 13.591327 %, obtained at iteration 2285, with test performance 13.591327 %, and best_auc: 0.500000
runtime: 72021.54 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 39
RunningOn slot9@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 39
Running Theano Learn Deep Belief Net for pcba, target: aid485349.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.451630318
Pre-training layer 0, epoch 1, cost  -52.2761269403
Pre-training layer 0, epoch 2, cost  -46.5803127623
Pre-training layer 0, epoch 3, cost  -42.9985321903
Pre-training layer 0, epoch 4, cost  -40.3365684806
Pre-training layer 0, epoch 5, cost  -38.2472567117
Pre-training layer 0, epoch 6, cost  -36.5137909539
Pre-training layer 0, epoch 7, cost  -35.0547973321
Pre-training layer 1, epoch 0, cost  -1354.74274651
Pre-training layer 1, epoch 1, cost  -1350.85925017
Pre-training layer 1, epoch 2, cost  -1350.858047
Pre-training layer 1, epoch 3, cost  -1350.85808295
Pre-training layer 1, epoch 4, cost  -1350.85777479
Pre-training layer 1, epoch 5, cost  -1350.85779724
Pre-training layer 1, epoch 6, cost  -1350.85780093
Pre-training layer 1, epoch 7, cost  -1350.85738749
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2042/2042, best error 5.423529 %, best_auc: 0.500000
Optimization complete with best validation score of 5.420588 %, obtained at iteration 2042, with test performance 5.423529 %, and best_auc: 0.500000
runtime: 33434.26 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 4
RunningOn slot5@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 4
Running Theano Learn Deep Belief Net for pcba, target: aid1457.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -70.0188204337
Pre-training layer 0, epoch 1, cost  -56.5623947746
Pre-training layer 0, epoch 2, cost  -50.9048778091
Pre-training layer 0, epoch 3, cost  -47.3026284252
Pre-training layer 0, epoch 4, cost  -44.6315313559
Pre-training layer 0, epoch 5, cost  -42.5492023443
Pre-training layer 0, epoch 6, cost  -40.7807763777
Pre-training layer 0, epoch 7, cost  -39.2929147663
Pre-training layer 1, epoch 0, cost  -1351.32321231
Pre-training layer 1, epoch 1, cost  -1344.48512278
Pre-training layer 1, epoch 2, cost  -1344.4688705
Pre-training layer 1, epoch 3, cost  -1344.46856512
Pre-training layer 1, epoch 4, cost  -1344.46819727
Pre-training layer 1, epoch 5, cost  -1344.46796617
Pre-training layer 1, epoch 6, cost  -1344.46832954
Pre-training layer 1, epoch 7, cost  -1344.46752996
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1358/1358, best error 9.544248 %, best_auc: 0.500000
Optimization complete with best validation score of 9.539823 %, obtained at iteration 1358, with test performance 9.544248 %, and best_auc: 0.500000
runtime: 192526.44 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 40
RunningOn slot3@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 40
Running Theano Learn Deep Belief Net for pcba, target: aid485353.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.1970540592
Pre-training layer 0, epoch 1, cost  -51.9932329418
Pre-training layer 0, epoch 2, cost  -46.3720352161
Pre-training layer 0, epoch 3, cost  -42.7127303816
Pre-training layer 0, epoch 4, cost  -40.0917771111
Pre-training layer 0, epoch 5, cost  -38.0176356431
Pre-training layer 0, epoch 6, cost  -36.3008413794
Pre-training layer 0, epoch 7, cost  -34.8526586547
Pre-training layer 1, epoch 0, cost  -1354.41687641
Pre-training layer 1, epoch 1, cost  -1350.56485072
Pre-training layer 1, epoch 2, cost  -1350.56359956
Pre-training layer 1, epoch 3, cost  -1350.56338433
Pre-training layer 1, epoch 4, cost  -1350.563213
Pre-training layer 1, epoch 5, cost  -1350.56324715
Pre-training layer 1, epoch 6, cost  -1350.56338935
Pre-training layer 1, epoch 7, cost  -1350.56288779
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2077/2077, best error 5.202312 %, best_auc: 0.500000
Optimization complete with best validation score of 5.200867 %, obtained at iteration 2077, with test performance 5.202312 %, and best_auc: 0.500000
runtime: 236695.27 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 41
RunningOn slot6@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 41
Running Theano Learn Deep Belief Net for pcba, target: aid485360.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -68.6408182511
Pre-training layer 0, epoch 1, cost  -54.4428573125
Pre-training layer 0, epoch 2, cost  -48.5534997002
Pre-training layer 0, epoch 3, cost  -44.8325638995
Pre-training layer 0, epoch 4, cost  -42.0279550635
Pre-training layer 0, epoch 5, cost  -39.870223457
Pre-training layer 0, epoch 6, cost  -38.0676349783
Pre-training layer 0, epoch 7, cost  -36.5670964312
Pre-training layer 1, epoch 0, cost  -1354.82234543
Pre-training layer 1, epoch 1, cost  -1349.75579448
Pre-training layer 1, epoch 2, cost  -1349.75100552
Pre-training layer 1, epoch 3, cost  -1349.7509625
Pre-training layer 1, epoch 4, cost  -1349.75095374
Pre-training layer 1, epoch 5, cost  -1349.75045854
Pre-training layer 1, epoch 6, cost  -1349.75035499
Pre-training layer 1, epoch 7, cost  -1349.75053739
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1610/1610, best error 16.593284 %, best_auc: 0.500000
     new best AUC!: 0.500022338382
Optimization complete with best validation score of 16.595149 %, obtained at iteration 1610, with test performance 16.593284 %, and best_auc: 0.500022
runtime: 215300.23 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 42
RunningOn slot5@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 42
Running Theano Learn Deep Belief Net for pcba, target: aid485364.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -59.9203559079
Pre-training layer 0, epoch 1, cost  -44.5623433267
Pre-training layer 0, epoch 2, cost  -38.6691192072
Pre-training layer 0, epoch 3, cost  -34.9760219968
Pre-training layer 0, epoch 4, cost  -32.2758599553
Pre-training layer 0, epoch 5, cost  -30.1418279779
Pre-training layer 0, epoch 6, cost  -28.373336406
Pre-training layer 0, epoch 7, cost  -26.9235916864
Pre-training layer 1, epoch 0, cost  -1357.12564843
Pre-training layer 1, epoch 1, cost  -1355.39588553
Pre-training layer 1, epoch 2, cost  -1355.39573277
Pre-training layer 1, epoch 3, cost  -1355.39546576
Pre-training layer 1, epoch 4, cost  -1355.39550805
Pre-training layer 1, epoch 5, cost  -1355.39549041
Pre-training layer 1, epoch 6, cost  -1355.39565532
Pre-training layer 1, epoch 7, cost  -1355.39528806
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 4001/4001, best error 48.117779 %, best_auc: 0.500000
Optimization complete with best validation score of 48.116279 %, obtained at iteration 4001, with test performance 48.117779 %, and best_auc: 0.500000
runtime: 352576.03 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 43
RunningOn slot12@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 43
Running Theano Learn Deep Belief Net for pcba, target: aid485367.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.9242243602
Pre-training layer 0, epoch 1, cost  -51.8433041767
Pre-training layer 0, epoch 2, cost  -46.2398628934
Pre-training layer 0, epoch 3, cost  -42.6181806584
Pre-training layer 0, epoch 4, cost  -40.0070010878
Pre-training layer 0, epoch 5, cost  -37.9823554451
Pre-training layer 0, epoch 6, cost  -36.2290451048
Pre-training layer 0, epoch 7, cost  -34.8251703065
Pre-training layer 1, epoch 0, cost  -1354.50069727
Pre-training layer 1, epoch 1, cost  -1350.66838688
Pre-training layer 1, epoch 2, cost  -1350.66746158
Pre-training layer 1, epoch 3, cost  -1350.66724139
Pre-training layer 1, epoch 4, cost  -1350.6668003
Pre-training layer 1, epoch 5, cost  -1350.66706672
Pre-training layer 1, epoch 6, cost  -1350.66701935
Pre-training layer 1, epoch 7, cost  -1350.66655317
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2081/2081, best error 4.796537 %, best_auc: 0.500000
Optimization complete with best validation score of 4.802309 %, obtained at iteration 2081, with test performance 4.796537 %, and best_auc: 0.500000
runtime: 34663.47 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 44
RunningOn slot4@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 44
Running Theano Learn Deep Belief Net for pcba, target: aid492947.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.5417379757
Pre-training layer 0, epoch 1, cost  -52.4889593271
Pre-training layer 0, epoch 2, cost  -46.862535007
Pre-training layer 0, epoch 3, cost  -43.2526586225
Pre-training layer 0, epoch 4, cost  -40.6447610396
Pre-training layer 0, epoch 5, cost  -38.5935906728
Pre-training layer 0, epoch 6, cost  -36.9158213114
Pre-training layer 0, epoch 7, cost  -35.4583895255
Pre-training layer 1, epoch 0, cost  -1354.24068296
Pre-training layer 1, epoch 1, cost  -1350.19828478
Pre-training layer 1, epoch 2, cost  -1350.19723268
Pre-training layer 1, epoch 3, cost  -1350.19710669
Pre-training layer 1, epoch 4, cost  -1350.19671502
Pre-training layer 1, epoch 5, cost  -1350.19674976
Pre-training layer 1, epoch 6, cost  -1350.19684352
Pre-training layer 1, epoch 7, cost  -1350.19617405
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1998/1998, best error 0.720721 %, best_auc: 0.500000
Optimization complete with best validation score of 0.720721 %, obtained at iteration 1998, with test performance 0.720721 %, and best_auc: 0.500000
runtime: 224853.28 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 45
RunningOn slot6@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 45
Running Theano Learn Deep Belief Net for pcba, target: aid493208.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -85.4488054232
Pre-training layer 0, epoch 1, cost  -68.1424385052
Pre-training layer 0, epoch 2, cost  -60.9310922269
Pre-training layer 0, epoch 3, cost  -55.7352226744
Pre-training layer 0, epoch 4, cost  -51.6708277258
Pre-training layer 0, epoch 5, cost  -48.324968251
Pre-training layer 0, epoch 6, cost  -45.8072703932
Pre-training layer 0, epoch 7, cost  -43.4583113006
Pre-training layer 1, epoch 0, cost  -1373.74801429
Pre-training layer 1, epoch 1, cost  -1360.76811246
Pre-training layer 1, epoch 2, cost  -1358.37382992
Pre-training layer 1, epoch 3, cost  -1357.85134827
Pre-training layer 1, epoch 4, cost  -1357.73461193
Pre-training layer 1, epoch 5, cost  -1357.70765258
Pre-training layer 1, epoch 6, cost  -1357.69996371
Pre-training layer 1, epoch 7, cost  -1357.69903429
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 323/323, best error 18.953271 %, best_auc: 0.500000
     new best AUC!: 0.50005728033
     new best AUC!: 0.50022912132
Optimization complete with best validation score of 18.925234 %, obtained at iteration 323, with test performance 18.953271 %, and best_auc: 0.500229
runtime: 130997.03 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 46
RunningOn slot6@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 46
Running Theano Learn Deep Belief Net for pcba, target: aid504327.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.0122994711
Pre-training layer 0, epoch 1, cost  -51.8587638692
Pre-training layer 0, epoch 2, cost  -46.1861701064
Pre-training layer 0, epoch 3, cost  -42.5619360895
Pre-training layer 0, epoch 4, cost  -39.9091640648
Pre-training layer 0, epoch 5, cost  -37.7771127521
Pre-training layer 0, epoch 6, cost  -36.0458409982
Pre-training layer 0, epoch 7, cost  -34.5598740049
Pre-training layer 1, epoch 0, cost  -1350.29227051
Pre-training layer 1, epoch 1, cost  -1346.62854694
Pre-training layer 1, epoch 2, cost  -1346.62736518
Pre-training layer 1, epoch 3, cost  -1346.62778379
Pre-training layer 1, epoch 4, cost  -1346.62712941
Pre-training layer 1, epoch 5, cost  -1346.62724792
Pre-training layer 1, epoch 6, cost  -1346.62679959
Pre-training layer 1, epoch 7, cost  -1346.62694608
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2422/2422, best error 5.680297 %, best_auc: 0.500000
Optimization complete with best validation score of 5.680297 %, obtained at iteration 2422, with test performance 5.680297 %, and best_auc: 0.500000
runtime: 37675.05 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 47
RunningOn slot4@opt-a003.discovery.wisc.edu
python th_deep_belief_net.py pcba 47
Running Theano Learn Deep Belief Net for pcba, target: aid504332.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -61.1131176406
Pre-training layer 0, epoch 1, cost  -46.6277445755
Pre-training layer 0, epoch 2, cost  -41.0664617579
Pre-training layer 0, epoch 3, cost  -37.5284694298
Pre-training layer 0, epoch 4, cost  -34.9362986192
Pre-training layer 0, epoch 5, cost  -32.9065055481
Pre-training layer 0, epoch 6, cost  -31.2206167591
Pre-training layer 0, epoch 7, cost  -29.7589091245
Pre-training layer 1, epoch 0, cost  -1353.36681202
Pre-training layer 1, epoch 1, cost  -1351.26455599
Pre-training layer 1, epoch 2, cost  -1351.26441115
Pre-training layer 1, epoch 3, cost  -1351.26440136
Pre-training layer 1, epoch 4, cost  -1351.26405404
Pre-training layer 1, epoch 5, cost  -1351.26429976
Pre-training layer 1, epoch 6, cost  -1351.26411198
Pre-training layer 1, epoch 7, cost  -1351.26411633
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3724/3724, best error 48.755842 %, best_auc: 0.500000
Optimization complete with best validation score of 48.750201 %, obtained at iteration 3724, with test performance 48.755842 %, and best_auc: 0.500000
runtime: 375015.63 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 48
RunningOn slot5@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 48
Running Theano Learn Deep Belief Net for pcba, target: aid504333.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -59.4118357574
Pre-training layer 0, epoch 1, cost  -44.3356884585
Pre-training layer 0, epoch 2, cost  -38.6231002333
Pre-training layer 0, epoch 3, cost  -34.9800475438
Pre-training layer 0, epoch 4, cost  -32.3240320148
Pre-training layer 0, epoch 5, cost  -30.258298674
Pre-training layer 0, epoch 6, cost  -28.566666482
Pre-training layer 0, epoch 7, cost  -27.1247614428
Pre-training layer 1, epoch 0, cost  -1356.86664035
Pre-training layer 1, epoch 1, cost  -1355.12777207
Pre-training layer 1, epoch 2, cost  -1355.12757455
Pre-training layer 1, epoch 3, cost  -1355.12743255
Pre-training layer 1, epoch 4, cost  -1355.12723699
Pre-training layer 1, epoch 5, cost  -1355.12721061
Pre-training layer 1, epoch 6, cost  -1355.12735703
Pre-training layer 1, epoch 7, cost  -1355.12719245
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 4022/4022, best error 49.098507 %, best_auc: 0.500000
Optimization complete with best validation score of 49.097761 %, obtained at iteration 4022, with test performance 49.098507 %, and best_auc: 0.500000
runtime: 391640.58 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 49
RunningOn slot3@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 49
Running Theano Learn Deep Belief Net for pcba, target: aid504339.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -58.5259142151
Pre-training layer 0, epoch 1, cost  -43.7897563654
Pre-training layer 0, epoch 2, cost  -38.2533852516
Pre-training layer 0, epoch 3, cost  -34.7040662461
Pre-training layer 0, epoch 4, cost  -32.1149749769
Pre-training layer 0, epoch 5, cost  -30.0823218562
Pre-training layer 0, epoch 6, cost  -28.4154457666
Pre-training layer 0, epoch 7, cost  -27.0069177822
Pre-training layer 1, epoch 0, cost  -1354.31581214
Pre-training layer 1, epoch 1, cost  -1352.57725098
Pre-training layer 1, epoch 2, cost  -1352.57701273
Pre-training layer 1, epoch 3, cost  -1352.57684147
Pre-training layer 1, epoch 4, cost  -1352.57690066
Pre-training layer 1, epoch 5, cost  -1352.57685247
Pre-training layer 1, epoch 6, cost  -1352.57694317
Pre-training layer 1, epoch 7, cost  -1352.57666738
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 4330/4330, best error 49.050589 %, best_auc: 0.500000
     new best AUC!: 0.500006799761
     new best AUC!: 0.500444965767
Optimization complete with best validation score of 49.047817 %, obtained at iteration 4330, with test performance 49.050589 %, and best_auc: 0.500445
runtime: 293239.67 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 5
RunningOn slot10@wid-002.discovery.wisc.edu
python th_deep_belief_net.py pcba 5
Running Theano Learn Deep Belief Net for pcba, target: aid1458.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -60.8735239446
Pre-training layer 0, epoch 1, cost  -46.5207760325
Pre-training layer 0, epoch 2, cost  -40.9028728033
Pre-training layer 0, epoch 3, cost  -37.3113774095
Pre-training layer 0, epoch 4, cost  -34.6975332695
Pre-training layer 0, epoch 5, cost  -32.6569872353
Pre-training layer 0, epoch 6, cost  -30.9767210566
Pre-training layer 0, epoch 7, cost  -29.5715927698
Pre-training layer 1, epoch 0, cost  -1354.67612862
Pre-training layer 1, epoch 1, cost  -1351.2041837
Pre-training layer 1, epoch 2, cost  -1351.20332631
Pre-training layer 1, epoch 3, cost  -1351.20319251
Pre-training layer 1, epoch 4, cost  -1351.20301652
Pre-training layer 1, epoch 5, cost  -1351.20331365
Pre-training layer 1, epoch 6, cost  -1351.20292006
Pre-training layer 1, epoch 7, cost  -1351.20285506
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2256/2256, best error 46.090546 %, best_auc: 0.500000
     new best AUC!: 0.500024665779
     new best AUC!: 0.500036998668
Optimization complete with best validation score of 46.079893 %, obtained at iteration 2256, with test performance 46.090546 %, and best_auc: 0.500037
runtime: 33869.59 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 50
RunningOn slot1@opt-a006.discovery.wisc.edu
python th_deep_belief_net.py pcba 50
Running Theano Learn Deep Belief Net for pcba, target: aid504444.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -62.0006505059
Pre-training layer 0, epoch 1, cost  -46.5246812163
Pre-training layer 0, epoch 2, cost  -40.560737753
Pre-training layer 0, epoch 3, cost  -36.7954818221
Pre-training layer 0, epoch 4, cost  -34.0325577543
Pre-training layer 0, epoch 5, cost  -31.8940510887
Pre-training layer 0, epoch 6, cost  -30.1277679248
Pre-training layer 0, epoch 7, cost  -28.6669830908
Pre-training layer 1, epoch 0, cost  -1356.0320648
Pre-training layer 1, epoch 1, cost  -1353.94090397
Pre-training layer 1, epoch 2, cost  -1353.94077331
Pre-training layer 1, epoch 3, cost  -1353.94073479
Pre-training layer 1, epoch 4, cost  -1353.94023545
Pre-training layer 1, epoch 5, cost  -1353.94066723
Pre-training layer 1, epoch 6, cost  -1353.9405647
Pre-training layer 1, epoch 7, cost  -1353.94051398
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3451/3451, best error 38.528696 %, best_auc: 0.500000
Optimization complete with best validation score of 38.527826 %, obtained at iteration 3451, with test performance 38.528696 %, and best_auc: 0.500000
runtime: 85472.02 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 51
RunningOn slot3@opt-a016.discovery.wisc.edu
python th_deep_belief_net.py pcba 51
Running Theano Learn Deep Belief Net for pcba, target: aid504466.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -61.2525242545
Pre-training layer 0, epoch 1, cost  -46.7923116082
Pre-training layer 0, epoch 2, cost  -41.1665132201
Pre-training layer 0, epoch 3, cost  -37.623079829
Pre-training layer 0, epoch 4, cost  -35.04022778
Pre-training layer 0, epoch 5, cost  -33.0110046909
Pre-training layer 0, epoch 6, cost  -31.3601738802
Pre-training layer 0, epoch 7, cost  -30.0115395023
Pre-training layer 1, epoch 0, cost  -1357.05183163
Pre-training layer 1, epoch 1, cost  -1354.4030681
Pre-training layer 1, epoch 2, cost  -1354.40261126
Pre-training layer 1, epoch 3, cost  -1354.40233506
Pre-training layer 1, epoch 4, cost  -1354.40243816
Pre-training layer 1, epoch 5, cost  -1354.40245019
Pre-training layer 1, epoch 6, cost  -1354.40241934
Pre-training layer 1, epoch 7, cost  -1354.40242212
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2706/2706, best error 27.713651 %, best_auc: 0.500000
     new best AUC!: 0.500015340011
     new best AUC!: 0.500030680023
Optimization complete with best validation score of 27.698113 %, obtained at iteration 2706, with test performance 27.713651 %, and best_auc: 0.500031
runtime: 167469.95 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 52
RunningOn slot5@opt-a006.discovery.wisc.edu
python th_deep_belief_net.py pcba 52
Running Theano Learn Deep Belief Net for pcba, target: aid504467.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -61.850129615
Pre-training layer 0, epoch 1, cost  -46.0830145673
Pre-training layer 0, epoch 2, cost  -40.1082946793
Pre-training layer 0, epoch 3, cost  -36.3006344399
Pre-training layer 0, epoch 4, cost  -33.5548797093
Pre-training layer 0, epoch 5, cost  -31.4248070545
Pre-training layer 0, epoch 6, cost  -29.6417321213
Pre-training layer 0, epoch 7, cost  -28.1888948044
Pre-training layer 1, epoch 0, cost  -1357.14558197
Pre-training layer 1, epoch 1, cost  -1355.03567475
Pre-training layer 1, epoch 2, cost  -1355.03546029
Pre-training layer 1, epoch 3, cost  -1355.03521977
Pre-training layer 1, epoch 4, cost  -1355.03501905
Pre-training layer 1, epoch 5, cost  -1355.03515941
Pre-training layer 1, epoch 6, cost  -1355.03512871
Pre-training layer 1, epoch 7, cost  -1355.03535303
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3311/3311, best error 41.567543 %, best_auc: 0.500000
Optimization complete with best validation score of 41.559383 %, obtained at iteration 3311, with test performance 41.567543 %, and best_auc: 0.500000
runtime: 100716.49 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 53
RunningOn slot3@opt-a003.discovery.wisc.edu
python th_deep_belief_net.py pcba 53
Running Theano Learn Deep Belief Net for pcba, target: aid504706.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.6752699996
Pre-training layer 0, epoch 1, cost  -52.6603401705
Pre-training layer 0, epoch 2, cost  -47.0292193768
Pre-training layer 0, epoch 3, cost  -43.4300169707
Pre-training layer 0, epoch 4, cost  -40.8179417801
Pre-training layer 0, epoch 5, cost  -38.7403443302
Pre-training layer 0, epoch 6, cost  -37.0315462844
Pre-training layer 0, epoch 7, cost  -35.6115542384
Pre-training layer 1, epoch 0, cost  -1354.23871432
Pre-training layer 1, epoch 1, cost  -1350.10954147
Pre-training layer 1, epoch 2, cost  -1350.10800262
Pre-training layer 1, epoch 3, cost  -1350.10780198
Pre-training layer 1, epoch 4, cost  -1350.1077814
Pre-training layer 1, epoch 5, cost  -1350.10738006
Pre-training layer 1, epoch 6, cost  -1350.10767484
Pre-training layer 1, epoch 7, cost  -1350.10749207
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1963/1963, best error 1.834862 %, best_auc: 0.500000
Optimization complete with best validation score of 1.831804 %, obtained at iteration 1963, with test performance 1.834862 %, and best_auc: 0.500000
runtime: 241220.68 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 54
RunningOn slot6@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 54
Running Theano Learn Deep Belief Net for pcba, target: aid504842.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.4320050802
Pre-training layer 0, epoch 1, cost  -52.398499427
Pre-training layer 0, epoch 2, cost  -46.7872410411
Pre-training layer 0, epoch 3, cost  -43.1941043683
Pre-training layer 0, epoch 4, cost  -40.5882893613
Pre-training layer 0, epoch 5, cost  -38.5582405428
Pre-training layer 0, epoch 6, cost  -36.837021627
Pre-training layer 0, epoch 7, cost  -35.4568970895
Pre-training layer 1, epoch 0, cost  -1354.3393932
Pre-training layer 1, epoch 1, cost  -1350.31523338
Pre-training layer 1, epoch 2, cost  -1350.3142359
Pre-training layer 1, epoch 3, cost  -1350.31425687
Pre-training layer 1, epoch 4, cost  -1350.3139847
Pre-training layer 1, epoch 5, cost  -1350.31379713
Pre-training layer 1, epoch 6, cost  -1350.31396899
Pre-training layer 1, epoch 7, cost  -1350.31361671
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1995/1995, best error 0.902256 %, best_auc: 0.500000
Optimization complete with best validation score of 0.900752 %, obtained at iteration 1995, with test performance 0.902256 %, and best_auc: 0.500000
runtime: 216903.66 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 55
RunningOn slot4@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 55
Running Theano Learn Deep Belief Net for pcba, target: aid504845.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.8454169964
Pre-training layer 0, epoch 1, cost  -51.7362220714
Pre-training layer 0, epoch 2, cost  -46.141262858
Pre-training layer 0, epoch 3, cost  -42.5597768675
Pre-training layer 0, epoch 4, cost  -39.9363551017
Pre-training layer 0, epoch 5, cost  -37.8876217268
Pre-training layer 0, epoch 6, cost  -36.167292568
Pre-training layer 0, epoch 7, cost  -34.7456387059
Pre-training layer 1, epoch 0, cost  -1351.72233283
Pre-training layer 1, epoch 1, cost  -1348.06012073
Pre-training layer 1, epoch 2, cost  -1348.05905138
Pre-training layer 1, epoch 3, cost  -1348.05927274
Pre-training layer 1, epoch 4, cost  -1348.0588305
Pre-training layer 1, epoch 5, cost  -1348.05909345
Pre-training layer 1, epoch 6, cost  -1348.05849364
Pre-training layer 1, epoch 7, cost  -1348.05880449
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2330/2330, best error 0.771907 %, best_auc: 0.500000
Optimization complete with best validation score of 0.771907 %, obtained at iteration 2330, with test performance 0.771907 %, and best_auc: 0.500000
runtime: 247144.64 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 56
RunningOn slot16@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 56
Running Theano Learn Deep Belief Net for pcba, target: aid504847.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -63.9987397142
Pre-training layer 0, epoch 1, cost  -49.9399332933
Pre-training layer 0, epoch 2, cost  -44.3142479744
Pre-training layer 0, epoch 3, cost  -40.6850861643
Pre-training layer 0, epoch 4, cost  -38.0076139429
Pre-training layer 0, epoch 5, cost  -35.8658106004
Pre-training layer 0, epoch 6, cost  -34.1228824668
Pre-training layer 0, epoch 7, cost  -32.6423264776
Pre-training layer 1, epoch 0, cost  -1348.86629166
Pre-training layer 1, epoch 1, cost  -1345.82622831
Pre-training layer 1, epoch 2, cost  -1345.82607301
Pre-training layer 1, epoch 3, cost  -1345.82572828
Pre-training layer 1, epoch 4, cost  -1345.82579363
Pre-training layer 1, epoch 5, cost  -1345.8254832
Pre-training layer 1, epoch 6, cost  -1345.82590624
Pre-training layer 1, epoch 7, cost  -1345.82559863
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2975/2975, best error 21.215943 %, best_auc: 0.500000
     new best AUC!: 0.500006401475
Optimization complete with best validation score of 21.214934 %, obtained at iteration 2975, with test performance 21.215943 %, and best_auc: 0.500006
runtime: 48457.52 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 57
RunningOn slot14@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 57
Running Theano Learn Deep Belief Net for pcba, target: aid504891.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.8315533435
Pre-training layer 0, epoch 1, cost  -51.6854298634
Pre-training layer 0, epoch 2, cost  -46.0584567512
Pre-training layer 0, epoch 3, cost  -42.4756470943
Pre-training layer 0, epoch 4, cost  -39.8758610323
Pre-training layer 0, epoch 5, cost  -37.776914298
Pre-training layer 0, epoch 6, cost  -36.1087274875
Pre-training layer 0, epoch 7, cost  -34.6662470673
Pre-training layer 1, epoch 0, cost  -1353.59167372
Pre-training layer 1, epoch 1, cost  -1350.08782807
Pre-training layer 1, epoch 2, cost  -1350.08711506
Pre-training layer 1, epoch 3, cost  -1350.08718897
Pre-training layer 1, epoch 4, cost  -1350.08661162
Pre-training layer 1, epoch 5, cost  -1350.08699423
Pre-training layer 1, epoch 6, cost  -1350.08660049
Pre-training layer 1, epoch 7, cost  -1350.08659041
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2308/2308, best error 0.234070 %, best_auc: 0.500000
Optimization complete with best validation score of 0.234070 %, obtained at iteration 2308, with test performance 0.234070 %, and best_auc: 0.500000
runtime: 38999.54 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 58
RunningOn slot6@opt-a005.discovery.wisc.edu
python th_deep_belief_net.py pcba 58
Running Theano Learn Deep Belief Net for pcba, target: aid540276.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.0301143101
Pre-training layer 0, epoch 1, cost  -50.2233052842
Pre-training layer 0, epoch 2, cost  -44.2819610651
Pre-training layer 0, epoch 3, cost  -40.5180741152
Pre-training layer 0, epoch 4, cost  -37.7508702676
Pre-training layer 0, epoch 5, cost  -35.5678397645
Pre-training layer 0, epoch 6, cost  -33.7802619385
Pre-training layer 0, epoch 7, cost  -32.247156515
Pre-training layer 1, epoch 0, cost  -1354.60313888
Pre-training layer 1, epoch 1, cost  -1351.45565863
Pre-training layer 1, epoch 2, cost  -1351.45471356
Pre-training layer 1, epoch 3, cost  -1351.4548696
Pre-training layer 1, epoch 4, cost  -1351.45472204
Pre-training layer 1, epoch 5, cost  -1351.45487493
Pre-training layer 1, epoch 6, cost  -1351.45426637
Pre-training layer 1, epoch 7, cost  -1351.45468276
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2469/2469, best error 32.002433 %, best_auc: 0.500000
Optimization complete with best validation score of 32.018248 %, obtained at iteration 2469, with test performance 32.002433 %, and best_auc: 0.500000
runtime: 65451.78 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 59
RunningOn slot4@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 59
Running Theano Learn Deep Belief Net for pcba, target: aid540317.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -64.2705144653
Pre-training layer 0, epoch 1, cost  -49.9205235713
Pre-training layer 0, epoch 2, cost  -44.2739600577
Pre-training layer 0, epoch 3, cost  -40.6505046281
Pre-training layer 0, epoch 4, cost  -38.0035405413
Pre-training layer 0, epoch 5, cost  -35.9116704409
Pre-training layer 0, epoch 6, cost  -34.1866381175
Pre-training layer 0, epoch 7, cost  -32.7372791938
Pre-training layer 1, epoch 0, cost  -1353.88847009
Pre-training layer 1, epoch 1, cost  -1350.92750328
Pre-training layer 1, epoch 2, cost  -1350.92665772
Pre-training layer 1, epoch 3, cost  -1350.92642799
Pre-training layer 1, epoch 4, cost  -1350.92669766
Pre-training layer 1, epoch 5, cost  -1350.92620827
Pre-training layer 1, epoch 6, cost  -1350.92640159
Pre-training layer 1, epoch 7, cost  -1350.92661499
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2671/2671, best error 14.323960 %, best_auc: 0.500000
Optimization complete with best validation score of 14.321710 %, obtained at iteration 2671, with test performance 14.323960 %, and best_auc: 0.500000
runtime: 193603.03 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 6
RunningOn slot6@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 6
Running Theano Learn Deep Belief Net for pcba, target: aid1460.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.0343071704
Pre-training layer 0, epoch 1, cost  -50.4684777527
Pre-training layer 0, epoch 2, cost  -44.61569536
Pre-training layer 0, epoch 3, cost  -40.8611250138
Pre-training layer 0, epoch 4, cost  -38.0495528715
Pre-training layer 0, epoch 5, cost  -35.8636803928
Pre-training layer 0, epoch 6, cost  -34.0579612207
Pre-training layer 0, epoch 7, cost  -32.4881112489
Pre-training layer 1, epoch 0, cost  -1352.43255986
Pre-training layer 1, epoch 1, cost  -1349.24152327
Pre-training layer 1, epoch 2, cost  -1349.24082048
Pre-training layer 1, epoch 3, cost  -1349.24065299
Pre-training layer 1, epoch 4, cost  -1349.24077046
Pre-training layer 1, epoch 5, cost  -1349.24056376
Pre-training layer 1, epoch 6, cost  -1349.24036366
Pre-training layer 1, epoch 7, cost  -1349.24069224
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2587/2587, best error 39.299304 %, best_auc: 0.500000
     new best AUC!: 0.500432927507
     new best AUC!: 0.500442469424
     new best AUC!: 0.500442477876
Optimization complete with best validation score of 39.301624 %, obtained at iteration 2587, with test performance 39.299304 %, and best_auc: 0.500442
runtime: 266141.56 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 60
RunningOn slot19@roy-exec-1.discovery.wisc.edu
python th_deep_belief_net.py pcba 60
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 61
RunningOn slot14@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 61
Running Theano Learn Deep Belief Net for pcba, target: aid588453.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -63.2481660385
Pre-training layer 0, epoch 1, cost  -48.6350631822
Pre-training layer 0, epoch 2, cost  -42.9505249073
Pre-training layer 0, epoch 3, cost  -39.2861803066
Pre-training layer 0, epoch 4, cost  -36.6247965297
Pre-training layer 0, epoch 5, cost  -34.5094647305
Pre-training layer 0, epoch 6, cost  -32.7845996424
Pre-training layer 0, epoch 7, cost  -31.3371888964
Pre-training layer 1, epoch 0, cost  -1353.17863045
Pre-training layer 1, epoch 1, cost  -1350.51213912
Pre-training layer 1, epoch 2, cost  -1350.51172213
Pre-training layer 1, epoch 3, cost  -1350.51149724
Pre-training layer 1, epoch 4, cost  -1350.51177305
Pre-training layer 1, epoch 5, cost  -1350.51119888
Pre-training layer 1, epoch 6, cost  -1350.5116968
Pre-training layer 1, epoch 7, cost  -1350.51155795
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2999/2999, best error 23.412412 %, best_auc: 0.500000
Optimization complete with best validation score of 23.412412 %, obtained at iteration 2999, with test performance 23.412412 %, and best_auc: 0.500000
runtime: 45064.08 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 62
RunningOn slot6@opt-a004.discovery.wisc.edu
python th_deep_belief_net.py pcba 62
Running Theano Learn Deep Belief Net for pcba, target: aid588456.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.7568078021
Pre-training layer 0, epoch 1, cost  -51.6426744203
Pre-training layer 0, epoch 2, cost  -46.0373063468
Pre-training layer 0, epoch 3, cost  -42.4893023281
Pre-training layer 0, epoch 4, cost  -39.8803070292
Pre-training layer 0, epoch 5, cost  -37.8223642735
Pre-training layer 0, epoch 6, cost  -36.1401955081
Pre-training layer 0, epoch 7, cost  -34.7141533329
Pre-training layer 1, epoch 0, cost  -1352.65139863
Pre-training layer 1, epoch 1, cost  -1349.06553681
Pre-training layer 1, epoch 2, cost  -1349.06481203
Pre-training layer 1, epoch 3, cost  -1349.06460592
Pre-training layer 1, epoch 4, cost  -1349.06441998
Pre-training layer 1, epoch 5, cost  -1349.06485965
Pre-training layer 1, epoch 6, cost  -1349.06419891
Pre-training layer 1, epoch 7, cost  -1349.06438751
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2326/2326, best error 0.385806 %, best_auc: 0.500000
Optimization complete with best validation score of 0.387097 %, obtained at iteration 2326, with test performance 0.385806 %, and best_auc: 0.500000
runtime: 276472.67 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 63
RunningOn slot15@wid-002.discovery.wisc.edu
python th_deep_belief_net.py pcba 63
Running Theano Learn Deep Belief Net for pcba, target: aid588579.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.222866195
Pre-training layer 0, epoch 1, cost  -50.7970329112
Pre-training layer 0, epoch 2, cost  -45.0821572897
Pre-training layer 0, epoch 3, cost  -41.4296222673
Pre-training layer 0, epoch 4, cost  -38.7292395752
Pre-training layer 0, epoch 5, cost  -36.6210668614
Pre-training layer 0, epoch 6, cost  -34.8750639114
Pre-training layer 0, epoch 7, cost  -33.3865028016
Pre-training layer 1, epoch 0, cost  -1350.03163721
Pre-training layer 1, epoch 1, cost  -1346.78834395
Pre-training layer 1, epoch 2, cost  -1346.7879731
Pre-training layer 1, epoch 3, cost  -1346.78772049
Pre-training layer 1, epoch 4, cost  -1346.78781793
Pre-training layer 1, epoch 5, cost  -1346.7873631
Pre-training layer 1, epoch 6, cost  -1346.78731687
Pre-training layer 1, epoch 7, cost  -1346.78781557
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2716/2716, best error 13.121547 %, best_auc: 0.500000
Optimization complete with best validation score of 13.119337 %, obtained at iteration 2716, with test performance 13.121547 %, and best_auc: 0.500000
runtime: 39955.62 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 64
RunningOn slot2@opt-a005.discovery.wisc.edu
python th_deep_belief_net.py pcba 64
Running Theano Learn Deep Belief Net for pcba, target: aid588590.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -63.8054112284
Pre-training layer 0, epoch 1, cost  -49.0055110109
Pre-training layer 0, epoch 2, cost  -43.1838955775
Pre-training layer 0, epoch 3, cost  -39.4478293974
Pre-training layer 0, epoch 4, cost  -36.7223310398
Pre-training layer 0, epoch 5, cost  -34.5897935047
Pre-training layer 0, epoch 6, cost  -32.8424214099
Pre-training layer 0, epoch 7, cost  -31.3767914675
Pre-training layer 1, epoch 0, cost  -1353.91366582
Pre-training layer 1, epoch 1, cost  -1351.31118812
Pre-training layer 1, epoch 2, cost  -1351.31085214
Pre-training layer 1, epoch 3, cost  -1351.31048659
Pre-training layer 1, epoch 4, cost  -1351.3108264
Pre-training layer 1, epoch 5, cost  -1351.31032899
Pre-training layer 1, epoch 6, cost  -1351.31050859
Pre-training layer 1, epoch 7, cost  -1351.31050974
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3000/3000, best error 23.579000 %, best_auc: 0.500000
     new best AUC!: 0.500636132316
     new best AUC!: 0.501265730593
     new best AUC!: 0.501265738916
Optimization complete with best validation score of 23.578000 %, obtained at iteration 3000, with test performance 23.579000 %, and best_auc: 0.501266
runtime: 87860.47 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 65
RunningOn slot3@opt-a016.discovery.wisc.edu
python th_deep_belief_net.py pcba 65
Running Theano Learn Deep Belief Net for pcba, target: aid588591.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -63.2924740348
Pre-training layer 0, epoch 1, cost  -48.1897481696
Pre-training layer 0, epoch 2, cost  -42.2971349309
Pre-training layer 0, epoch 3, cost  -38.5660799291
Pre-training layer 0, epoch 4, cost  -35.8461116109
Pre-training layer 0, epoch 5, cost  -33.7120554659
Pre-training layer 0, epoch 6, cost  -31.9577294224
Pre-training layer 0, epoch 7, cost  -30.4830490911
Pre-training layer 1, epoch 0, cost  -1354.26985907
Pre-training layer 1, epoch 1, cost  -1351.83257557
Pre-training layer 1, epoch 2, cost  -1351.83199586
Pre-training layer 1, epoch 3, cost  -1351.83187072
Pre-training layer 1, epoch 4, cost  -1351.83172526
Pre-training layer 1, epoch 5, cost  -1351.83182363
Pre-training layer 1, epoch 6, cost  -1351.83216528
Pre-training layer 1, epoch 7, cost  -1351.83180969
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3150/3150, best error 26.856190 %, best_auc: 0.500000
Optimization complete with best validation score of 26.857143 %, obtained at iteration 3150, with test performance 26.856190 %, and best_auc: 0.500000
runtime: 164098.95 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 66
RunningOn slot6@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 66
Running Theano Learn Deep Belief Net for pcba, target: aid588795.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.4515687764
Pre-training layer 0, epoch 1, cost  -51.0848331205
Pre-training layer 0, epoch 2, cost  -45.4319015868
Pre-training layer 0, epoch 3, cost  -41.7684495316
Pre-training layer 0, epoch 4, cost  -39.0914635305
Pre-training layer 0, epoch 5, cost  -37.0362479711
Pre-training layer 0, epoch 6, cost  -35.2965001899
Pre-training layer 0, epoch 7, cost  -33.8235549394
Pre-training layer 1, epoch 0, cost  -1352.62116698
Pre-training layer 1, epoch 1, cost  -1349.36980432
Pre-training layer 1, epoch 2, cost  -1349.36874044
Pre-training layer 1, epoch 3, cost  -1349.36875767
Pre-training layer 1, epoch 4, cost  -1349.3686353
Pre-training layer 1, epoch 5, cost  -1349.36842514
Pre-training layer 1, epoch 6, cost  -1349.36827624
Pre-training layer 1, epoch 7, cost  -1349.36856413
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2545/2545, best error 9.229953 %, best_auc: 0.500000
     new best AUC!: 0.503818429093
     new best AUC!: 0.507623969185
     new best AUC!: 0.507624018949
     new best AUC!: 0.507624068714
Optimization complete with best validation score of 9.231132 %, obtained at iteration 2545, with test performance 9.229953 %, and best_auc: 0.507624
runtime: 52971.45 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 67
RunningOn slot6@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 67
Running Theano Learn Deep Belief Net for pcba, target: aid588855.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -63.7452499302
Pre-training layer 0, epoch 1, cost  -48.7240431275
Pre-training layer 0, epoch 2, cost  -42.8514848029
Pre-training layer 0, epoch 3, cost  -39.1201799464
Pre-training layer 0, epoch 4, cost  -36.3598449357
Pre-training layer 0, epoch 5, cost  -34.2053119018
Pre-training layer 0, epoch 6, cost  -32.3951050499
Pre-training layer 0, epoch 7, cost  -30.9003808747
Pre-training layer 1, epoch 0, cost  -1353.79922854
Pre-training layer 1, epoch 1, cost  -1351.42366655
Pre-training layer 1, epoch 2, cost  -1351.42348542
Pre-training layer 1, epoch 3, cost  -1351.42326273
Pre-training layer 1, epoch 4, cost  -1351.42298689
Pre-training layer 1, epoch 5, cost  -1351.42329538
Pre-training layer 1, epoch 6, cost  -1351.42328239
Pre-training layer 1, epoch 7, cost  -1351.42302903
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3272/3272, best error 26.928440 %, best_auc: 0.500000
     new best AUC!: 0.500479326847
Optimization complete with best validation score of 26.931193 %, obtained at iteration 3272, with test performance 26.928440 %, and best_auc: 0.500479
runtime: 246866.64 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 68
RunningOn slot11@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 68
Running Theano Learn Deep Belief Net for pcba, target: aid602179.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.7935647692
Pre-training layer 0, epoch 1, cost  -51.4911300146
Pre-training layer 0, epoch 2, cost  -45.8481221995
Pre-training layer 0, epoch 3, cost  -42.2183746206
Pre-training layer 0, epoch 4, cost  -39.5704465053
Pre-training layer 0, epoch 5, cost  -37.45507427
Pre-training layer 0, epoch 6, cost  -35.7909930692
Pre-training layer 0, epoch 7, cost  -34.3287455557
Pre-training layer 1, epoch 0, cost  -1353.39592672
Pre-training layer 1, epoch 1, cost  -1350.01373615
Pre-training layer 1, epoch 2, cost  -1350.01266865
Pre-training layer 1, epoch 3, cost  -1350.01269051
Pre-training layer 1, epoch 4, cost  -1350.01227021
Pre-training layer 1, epoch 5, cost  -1350.01270937
Pre-training layer 1, epoch 6, cost  -1350.01225912
Pre-training layer 1, epoch 7, cost  -1350.01232382
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2389/2389, best error 2.713568 %, best_auc: 0.500000
Optimization complete with best validation score of 2.713568 %, obtained at iteration 2389, with test performance 2.713568 %, and best_auc: 0.500000
runtime: 36975.19 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 69
RunningOn slot6@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 69
Running Theano Learn Deep Belief Net for pcba, target: aid602233.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.9813170079
Pre-training layer 0, epoch 1, cost  -51.7518004749
Pre-training layer 0, epoch 2, cost  -46.1102756428
Pre-training layer 0, epoch 3, cost  -42.4837580546
Pre-training layer 0, epoch 4, cost  -39.8741786008
Pre-training layer 0, epoch 5, cost  -37.7763118351
Pre-training layer 0, epoch 6, cost  -36.0995727512
Pre-training layer 0, epoch 7, cost  -34.6315010951
Pre-training layer 1, epoch 0, cost  -1353.15530823
Pre-training layer 1, epoch 1, cost  -1349.60961948
Pre-training layer 1, epoch 2, cost  -1349.60874505
Pre-training layer 1, epoch 3, cost  -1349.60868837
Pre-training layer 1, epoch 4, cost  -1349.60853397
Pre-training layer 1, epoch 5, cost  -1349.608671
Pre-training layer 1, epoch 6, cost  -1349.60836949
Pre-training layer 1, epoch 7, cost  -1349.60849266
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2315/2315, best error 1.282750 %, best_auc: 0.500000
Optimization complete with best validation score of 1.281453 %, obtained at iteration 2315, with test performance 1.282750 %, and best_auc: 0.500000
runtime: 249356.72 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 7
RunningOn slot11@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 7
Running Theano Learn Deep Belief Net for pcba, target: aid1461.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -68.6258232017
Pre-training layer 0, epoch 1, cost  -54.4223608617
Pre-training layer 0, epoch 2, cost  -48.5453028078
Pre-training layer 0, epoch 3, cost  -44.7266318645
Pre-training layer 0, epoch 4, cost  -41.89014949
Pre-training layer 0, epoch 5, cost  -39.6441148263
Pre-training layer 0, epoch 6, cost  -37.805686996
Pre-training layer 0, epoch 7, cost  -36.2423535068
Pre-training layer 1, epoch 0, cost  -1355.28781305
Pre-training layer 1, epoch 1, cost  -1350.67497997
Pre-training layer 1, epoch 2, cost  -1350.67183235
Pre-training layer 1, epoch 3, cost  -1350.67147496
Pre-training layer 1, epoch 4, cost  -1350.67148533
Pre-training layer 1, epoch 5, cost  -1350.67124326
Pre-training layer 1, epoch 6, cost  -1350.67128927
Pre-training layer 1, epoch 7, cost  -1350.67140542
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1726/1726, best error 24.022609 %, best_auc: 0.500000
Optimization complete with best validation score of 24.031304 %, obtained at iteration 1726, with test performance 24.022609 %, and best_auc: 0.500000
runtime: 29935.67 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 70
RunningOn slot16@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 70
Running Theano Learn Deep Belief Net for pcba, target: aid602310.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.6995250836
Pre-training layer 0, epoch 1, cost  -51.4673013422
Pre-training layer 0, epoch 2, cost  -45.8433976073
Pre-training layer 0, epoch 3, cost  -42.2474367444
Pre-training layer 0, epoch 4, cost  -39.61609343
Pre-training layer 0, epoch 5, cost  -37.5499373568
Pre-training layer 0, epoch 6, cost  -35.8443575461
Pre-training layer 0, epoch 7, cost  -34.3978434021
Pre-training layer 1, epoch 0, cost  -1353.15264448
Pre-training layer 1, epoch 1, cost  -1349.83203467
Pre-training layer 1, epoch 2, cost  -1349.83080754
Pre-training layer 1, epoch 3, cost  -1349.83123367
Pre-training layer 1, epoch 4, cost  -1349.83089499
Pre-training layer 1, epoch 5, cost  -1349.83096844
Pre-training layer 1, epoch 6, cost  -1349.83058123
Pre-training layer 1, epoch 7, cost  -1349.83088362
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2467/2467, best error 2.260341 %, best_auc: 0.500000
Optimization complete with best validation score of 2.262774 %, obtained at iteration 2467, with test performance 2.260341 %, and best_auc: 0.500000
runtime: 39108.20 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 71
RunningOn slot5@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 71
Running Theano Learn Deep Belief Net for pcba, target: aid602313.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.5816495748
Pre-training layer 0, epoch 1, cost  -51.2211788029
Pre-training layer 0, epoch 2, cost  -45.536147351
Pre-training layer 0, epoch 3, cost  -41.9136771186
Pre-training layer 0, epoch 4, cost  -39.2613615011
Pre-training layer 0, epoch 5, cost  -37.1790001848
Pre-training layer 0, epoch 6, cost  -35.4782584764
Pre-training layer 0, epoch 7, cost  -33.986517849
Pre-training layer 1, epoch 0, cost  -1353.98117578
Pre-training layer 1, epoch 1, cost  -1350.71232376
Pre-training layer 1, epoch 2, cost  -1350.71139009
Pre-training layer 1, epoch 3, cost  -1350.71154478
Pre-training layer 1, epoch 4, cost  -1350.71106159
Pre-training layer 1, epoch 5, cost  -1350.71127341
Pre-training layer 1, epoch 6, cost  -1350.71079669
Pre-training layer 1, epoch 7, cost  -1350.71103353
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2435/2435, best error 5.620222 %, best_auc: 0.500000
Optimization complete with best validation score of 5.621455 %, obtained at iteration 2435, with test performance 5.620222 %, and best_auc: 0.500000
runtime: 230521.53 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 72
RunningOn slot7@wid-004.discovery.wisc.edu
python th_deep_belief_net.py pcba 72
Running Theano Learn Deep Belief Net for pcba, target: aid602332.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.0510908864
Pre-training layer 0, epoch 1, cost  -51.9810361122
Pre-training layer 0, epoch 2, cost  -46.3913719131
Pre-training layer 0, epoch 3, cost  -42.8681361151
Pre-training layer 0, epoch 4, cost  -40.2588513196
Pre-training layer 0, epoch 5, cost  -38.2017203251
Pre-training layer 0, epoch 6, cost  -36.4794017301
Pre-training layer 0, epoch 7, cost  -35.0594525594
Pre-training layer 1, epoch 0, cost  -1349.9871602
Pre-training layer 1, epoch 1, cost  -1346.43872627
Pre-training layer 1, epoch 2, cost  -1346.43789812
Pre-training layer 1, epoch 3, cost  -1346.43790585
Pre-training layer 1, epoch 4, cost  -1346.43754709
Pre-training layer 1, epoch 5, cost  -1346.43788807
Pre-training layer 1, epoch 6, cost  -1346.43722247
Pre-training layer 1, epoch 7, cost  -1346.43778631
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2507/2507, best error 0.467066 %, best_auc: 0.500000
Optimization complete with best validation score of 0.465868 %, obtained at iteration 2507, with test performance 0.467066 %, and best_auc: 0.500000
runtime: 39621.79 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 73
RunningOn slot10@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 73
Running Theano Learn Deep Belief Net for pcba, target: aid624170.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.6765707511
Pre-training layer 0, epoch 1, cost  -51.4102783622
Pre-training layer 0, epoch 2, cost  -45.7245573451
Pre-training layer 0, epoch 3, cost  -42.0831437353
Pre-training layer 0, epoch 4, cost  -39.4349223226
Pre-training layer 0, epoch 5, cost  -37.3279706814
Pre-training layer 0, epoch 6, cost  -35.6330297485
Pre-training layer 0, epoch 7, cost  -34.1539465946
Pre-training layer 1, epoch 0, cost  -1351.19361958
Pre-training layer 1, epoch 1, cost  -1347.85980583
Pre-training layer 1, epoch 2, cost  -1347.85914608
Pre-training layer 1, epoch 3, cost  -1347.85902426
Pre-training layer 1, epoch 4, cost  -1347.85886305
Pre-training layer 1, epoch 5, cost  -1347.85870482
Pre-training layer 1, epoch 6, cost  -1347.85883232
Pre-training layer 1, epoch 7, cost  -1347.85896356
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2577/2577, best error 5.831002 %, best_auc: 0.500000
Optimization complete with best validation score of 5.835664 %, obtained at iteration 2577, with test performance 5.831002 %, and best_auc: 0.500000
runtime: 41632.09 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 74
RunningOn slot6@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 74
Running Theano Learn Deep Belief Net for pcba, target: aid624171.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -64.6848563035
Pre-training layer 0, epoch 1, cost  -50.4336387765
Pre-training layer 0, epoch 2, cost  -44.7965192945
Pre-training layer 0, epoch 3, cost  -41.2113232409
Pre-training layer 0, epoch 4, cost  -38.5796621562
Pre-training layer 0, epoch 5, cost  -36.4866706394
Pre-training layer 0, epoch 6, cost  -34.7746963712
Pre-training layer 0, epoch 7, cost  -33.3481639064
Pre-training layer 1, epoch 0, cost  -1353.99678473
Pre-training layer 1, epoch 1, cost  -1351.00988647
Pre-training layer 1, epoch 2, cost  -1351.00889
Pre-training layer 1, epoch 3, cost  -1351.00891945
Pre-training layer 1, epoch 4, cost  -1351.00891357
Pre-training layer 1, epoch 5, cost  -1351.00882241
Pre-training layer 1, epoch 6, cost  -1351.00881166
Pre-training layer 1, epoch 7, cost  -1351.00880669
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2639/2639, best error 8.426621 %, best_auc: 0.500000
Optimization complete with best validation score of 8.428896 %, obtained at iteration 2639, with test performance 8.426621 %, and best_auc: 0.500000
runtime: 296664.50 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 75
RunningOn slot3@opt-a018.discovery.wisc.edu
python th_deep_belief_net.py pcba 75
Running Theano Learn Deep Belief Net for pcba, target: aid624173.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.5041260882
Pre-training layer 0, epoch 1, cost  -51.3568795433
Pre-training layer 0, epoch 2, cost  -45.7387495445
Pre-training layer 0, epoch 3, cost  -42.1408492122
Pre-training layer 0, epoch 4, cost  -39.523058808
Pre-training layer 0, epoch 5, cost  -37.4529212031
Pre-training layer 0, epoch 6, cost  -35.768233103
Pre-training layer 0, epoch 7, cost  -34.3147434744
Pre-training layer 1, epoch 0, cost  -1350.7663488
Pre-training layer 1, epoch 1, cost  -1347.3028743
Pre-training layer 1, epoch 2, cost  -1347.30212797
Pre-training layer 1, epoch 3, cost  -1347.30202715
Pre-training layer 1, epoch 4, cost  -1347.30177155
Pre-training layer 1, epoch 5, cost  -1347.30190025
Pre-training layer 1, epoch 6, cost  -1347.30160712
Pre-training layer 1, epoch 7, cost  -1347.30195791
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2525/2525, best error 3.457788 %, best_auc: 0.500000
Optimization complete with best validation score of 3.457788 %, obtained at iteration 2525, with test performance 3.457788 %, and best_auc: 0.500000
runtime: 266788.64 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 76
RunningOn slot4@opt-a004.discovery.wisc.edu
python th_deep_belief_net.py pcba 76
Running Theano Learn Deep Belief Net for pcba, target: aid624202.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -61.4909296489
Pre-training layer 0, epoch 1, cost  -47.2145282593
Pre-training layer 0, epoch 2, cost  -41.6644914366
Pre-training layer 0, epoch 3, cost  -38.1629588564
Pre-training layer 0, epoch 4, cost  -35.5614550388
Pre-training layer 0, epoch 5, cost  -33.5559019787
Pre-training layer 0, epoch 6, cost  -31.9089099704
Pre-training layer 0, epoch 7, cost  -30.4962524023
Pre-training layer 1, epoch 0, cost  -1355.27615019
Pre-training layer 1, epoch 1, cost  -1352.74207652
Pre-training layer 1, epoch 2, cost  -1352.74186769
Pre-training layer 1, epoch 3, cost  -1352.74164015
Pre-training layer 1, epoch 4, cost  -1352.74170295
Pre-training layer 1, epoch 5, cost  -1352.74144706
Pre-training layer 1, epoch 6, cost  -1352.74174133
Pre-training layer 1, epoch 7, cost  -1352.74151743
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2946/2946, best error 24.229358 %, best_auc: 0.500000
     new best AUC!: 0.500006719617
Optimization complete with best validation score of 24.223242 %, obtained at iteration 2946, with test performance 24.229358 %, and best_auc: 0.500007
runtime: 338741.64 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 77
RunningOn slot4@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 77
Running Theano Learn Deep Belief Net for pcba, target: aid624246.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.1718493869
Pre-training layer 0, epoch 1, cost  -51.9783678438
Pre-training layer 0, epoch 2, cost  -46.3379541237
Pre-training layer 0, epoch 3, cost  -42.7273199723
Pre-training layer 0, epoch 4, cost  -40.0912192317
Pre-training layer 0, epoch 5, cost  -38.0385073814
Pre-training layer 0, epoch 6, cost  -36.3252687506
Pre-training layer 0, epoch 7, cost  -34.8876770142
Pre-training layer 1, epoch 0, cost  -1353.80408307
Pre-training layer 1, epoch 1, cost  -1350.1604061
Pre-training layer 1, epoch 2, cost  -1350.15939266
Pre-training layer 1, epoch 3, cost  -1350.1592354
Pre-training layer 1, epoch 4, cost  -1350.15903607
Pre-training layer 1, epoch 5, cost  -1350.15948273
Pre-training layer 1, epoch 6, cost  -1350.15900559
Pre-training layer 1, epoch 7, cost  -1350.15871273
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2221/2221, best error 0.809459 %, best_auc: 0.500000
Optimization complete with best validation score of 0.809459 %, obtained at iteration 2221, with test performance 0.809459 %, and best_auc: 0.500000
runtime: 36493.49 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 78
RunningOn slot10@wid-002.discovery.wisc.edu
python th_deep_belief_net.py pcba 78
Running Theano Learn Deep Belief Net for pcba, target: aid624287.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.6020772089
Pre-training layer 0, epoch 1, cost  -51.4455350415
Pre-training layer 0, epoch 2, cost  -45.8937943315
Pre-training layer 0, epoch 3, cost  -42.3143370417
Pre-training layer 0, epoch 4, cost  -39.7450844017
Pre-training layer 0, epoch 5, cost  -37.6887663911
Pre-training layer 0, epoch 6, cost  -36.041061574
Pre-training layer 0, epoch 7, cost  -34.6056670662
Pre-training layer 1, epoch 0, cost  -1354.89547445
Pre-training layer 1, epoch 1, cost  -1351.11394914
Pre-training layer 1, epoch 2, cost  -1351.11277989
Pre-training layer 1, epoch 3, cost  -1351.11260811
Pre-training layer 1, epoch 4, cost  -1351.11225858
Pre-training layer 1, epoch 5, cost  -1351.11244892
Pre-training layer 1, epoch 6, cost  -1351.11263552
Pre-training layer 1, epoch 7, cost  -1351.1119788
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2082/2082, best error 3.634921 %, best_auc: 0.500000
Optimization complete with best validation score of 3.630592 %, obtained at iteration 2082, with test performance 3.634921 %, and best_auc: 0.500000
runtime: 33212.21 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 79
RunningOn slot4@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 79
Running Theano Learn Deep Belief Net for pcba, target: aid624288.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.3590382346
Pre-training layer 0, epoch 1, cost  -51.0044607971
Pre-training layer 0, epoch 2, cost  -45.3304459199
Pre-training layer 0, epoch 3, cost  -41.6868794026
Pre-training layer 0, epoch 4, cost  -39.0496636808
Pre-training layer 0, epoch 5, cost  -36.9468767652
Pre-training layer 0, epoch 6, cost  -35.2447700018
Pre-training layer 0, epoch 7, cost  -33.7911926011
Pre-training layer 1, epoch 0, cost  -1354.14910763
Pre-training layer 1, epoch 1, cost  -1350.61679581
Pre-training layer 1, epoch 2, cost  -1350.61539931
Pre-training layer 1, epoch 3, cost  -1350.61570066
Pre-training layer 1, epoch 4, cost  -1350.61528024
Pre-training layer 1, epoch 5, cost  -1350.61557779
Pre-training layer 1, epoch 6, cost  -1350.61529216
Pre-training layer 1, epoch 7, cost  -1350.61510549
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2260/2260, best error 10.791501 %, best_auc: 0.500000
Optimization complete with best validation score of 10.791501 %, obtained at iteration 2260, with test performance 10.791501 %, and best_auc: 0.500000
runtime: 235505.04 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 8
RunningOn slot3@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 8
Running Theano Learn Deep Belief Net for pcba, target: aid1468.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -68.5002050644
Pre-training layer 0, epoch 1, cost  -54.564737924
Pre-training layer 0, epoch 2, cost  -48.8711462969
Pre-training layer 0, epoch 3, cost  -45.257734711
Pre-training layer 0, epoch 4, cost  -42.5584489568
Pre-training layer 0, epoch 5, cost  -40.4566830412
Pre-training layer 0, epoch 6, cost  -38.6849623225
Pre-training layer 0, epoch 7, cost  -37.201145723
Pre-training layer 1, epoch 0, cost  -1351.60489859
Pre-training layer 1, epoch 1, cost  -1346.72534939
Pre-training layer 1, epoch 2, cost  -1346.72318071
Pre-training layer 1, epoch 3, cost  -1346.72270892
Pre-training layer 1, epoch 4, cost  -1346.72291711
Pre-training layer 1, epoch 5, cost  -1346.72211528
Pre-training layer 1, epoch 6, cost  -1346.72268456
Pre-training layer 1, epoch 7, cost  -1346.72250992
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1809/1809, best error 10.302326 %, best_auc: 0.500000
Optimization complete with best validation score of 10.299003 %, obtained at iteration 1809, with test performance 10.302326 %, and best_auc: 0.500000
runtime: 143775.47 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 80
RunningOn slot1@opt-a006.discovery.wisc.edu
python th_deep_belief_net.py pcba 80
Running Theano Learn Deep Belief Net for pcba, target: aid624291.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.1077790115
Pre-training layer 0, epoch 1, cost  -52.0268618909
Pre-training layer 0, epoch 2, cost  -46.4228307017
Pre-training layer 0, epoch 3, cost  -42.8680186801
Pre-training layer 0, epoch 4, cost  -40.2535150957
Pre-training layer 0, epoch 5, cost  -38.1957114362
Pre-training layer 0, epoch 6, cost  -36.5126768776
Pre-training layer 0, epoch 7, cost  -35.0870239255
Pre-training layer 1, epoch 0, cost  -1354.16883006
Pre-training layer 1, epoch 1, cost  -1350.37296525
Pre-training layer 1, epoch 2, cost  -1350.37207001
Pre-training layer 1, epoch 3, cost  -1350.37176466
Pre-training layer 1, epoch 4, cost  -1350.37157259
Pre-training layer 1, epoch 5, cost  -1350.37186884
Pre-training layer 1, epoch 6, cost  -1350.37160638
Pre-training layer 1, epoch 7, cost  -1350.37113681
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2113/2113, best error 1.872159 %, best_auc: 0.500000
Optimization complete with best validation score of 1.872159 %, obtained at iteration 2113, with test performance 1.872159 %, and best_auc: 0.500000
runtime: 55307.10 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 81
RunningOn slot11@wid-004.discovery.wisc.edu
python th_deep_belief_net.py pcba 81
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 82
RunningOn slot6@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 82
Running Theano Learn Deep Belief Net for pcba, target: aid624297.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -62.8681421754
Pre-training layer 0, epoch 1, cost  -48.0131160632
Pre-training layer 0, epoch 2, cost  -42.1928930768
Pre-training layer 0, epoch 3, cost  -38.4423071132
Pre-training layer 0, epoch 4, cost  -35.7022506711
Pre-training layer 0, epoch 5, cost  -33.5530901013
Pre-training layer 0, epoch 6, cost  -31.7797344054
Pre-training layer 0, epoch 7, cost  -30.2761824409
Pre-training layer 1, epoch 0, cost  -1353.91811884
Pre-training layer 1, epoch 1, cost  -1351.42676199
Pre-training layer 1, epoch 2, cost  -1351.42643942
Pre-training layer 1, epoch 3, cost  -1351.42620548
Pre-training layer 1, epoch 4, cost  -1351.4260594
Pre-training layer 1, epoch 5, cost  -1351.42597955
Pre-training layer 1, epoch 6, cost  -1351.42648342
Pre-training layer 1, epoch 7, cost  -1351.42625837
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3135/3135, best error 35.666667 %, best_auc: 0.500403
Optimization complete with best validation score of 35.662835 %, obtained at iteration 3135, with test performance 35.666667 %, and best_auc: 0.500403
runtime: 282985.92 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 83
RunningOn slot4@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 83
Running Theano Learn Deep Belief Net for pcba, target: aid624417.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -62.2181490205
Pre-training layer 0, epoch 1, cost  -47.195586599
Pre-training layer 0, epoch 2, cost  -41.3593886331
Pre-training layer 0, epoch 3, cost  -37.6256511201
Pre-training layer 0, epoch 4, cost  -34.9074736331
Pre-training layer 0, epoch 5, cost  -32.7785827349
Pre-training layer 0, epoch 6, cost  -31.0065807696
Pre-training layer 0, epoch 7, cost  -29.514685183
Pre-training layer 1, epoch 0, cost  -1355.28366199
Pre-training layer 1, epoch 1, cost  -1353.20404143
Pre-training layer 1, epoch 2, cost  -1353.20400149
Pre-training layer 1, epoch 3, cost  -1353.20403657
Pre-training layer 1, epoch 4, cost  -1353.20350431
Pre-training layer 1, epoch 5, cost  -1353.20372891
Pre-training layer 1, epoch 6, cost  -1353.2038454
Pre-training layer 1, epoch 7, cost  -1353.20386011
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3542/3542, best error 32.452542 %, best_auc: 0.500000
Optimization complete with best validation score of 32.449153 %, obtained at iteration 3542, with test performance 32.452542 %, and best_auc: 0.500000
runtime: 245109.66 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 84
RunningOn slot2@opt-a010.discovery.wisc.edu
python th_deep_belief_net.py pcba 84
Running Theano Learn Deep Belief Net for pcba, target: aid651635.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.2806835935
Pre-training layer 0, epoch 1, cost  -50.1305736143
Pre-training layer 0, epoch 2, cost  -44.1670547724
Pre-training layer 0, epoch 3, cost  -40.3713978283
Pre-training layer 0, epoch 4, cost  -37.6339282336
Pre-training layer 0, epoch 5, cost  -35.4431622996
Pre-training layer 0, epoch 6, cost  -33.6424754466
Pre-training layer 0, epoch 7, cost  -32.1404830865
Pre-training layer 1, epoch 0, cost  -1352.45875203
Pre-training layer 1, epoch 1, cost  -1349.74323773
Pre-training layer 1, epoch 2, cost  -1349.74310558
Pre-training layer 1, epoch 3, cost  -1349.74276387
Pre-training layer 1, epoch 4, cost  -1349.74291684
Pre-training layer 1, epoch 5, cost  -1349.74269723
Pre-training layer 1, epoch 6, cost  -1349.7429643
Pre-training layer 1, epoch 7, cost  -1349.74269728
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3008/3008, best error 22.631737 %, best_auc: 0.500000
     new best AUC!: 0.501316312896
     new best AUC!: 0.5032811246
     new best AUC!: 0.503281141655
     new best AUC!: 0.503929674574
     new best AUC!: 0.503929683102
Optimization complete with best validation score of 22.628743 %, obtained at iteration 3008, with test performance 22.631737 %, and best_auc: 0.503930
runtime: 133268.80 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 85
RunningOn slot3@opt-a016.discovery.wisc.edu
python th_deep_belief_net.py pcba 85
Running Theano Learn Deep Belief Net for pcba, target: aid651644.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.7539626406
Pre-training layer 0, epoch 1, cost  -51.6998357281
Pre-training layer 0, epoch 2, cost  -46.0951748643
Pre-training layer 0, epoch 3, cost  -42.5305298258
Pre-training layer 0, epoch 4, cost  -39.9145853345
Pre-training layer 0, epoch 5, cost  -37.8490353068
Pre-training layer 0, epoch 6, cost  -36.159832586
Pre-training layer 0, epoch 7, cost  -34.7132634535
Pre-training layer 1, epoch 0, cost  -1352.57332047
Pre-training layer 1, epoch 1, cost  -1348.9413135
Pre-training layer 1, epoch 2, cost  -1348.94012591
Pre-training layer 1, epoch 3, cost  -1348.94006804
Pre-training layer 1, epoch 4, cost  -1348.93988201
Pre-training layer 1, epoch 5, cost  -1348.94017125
Pre-training layer 1, epoch 6, cost  -1348.93979153
Pre-training layer 1, epoch 7, cost  -1348.9398325
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2301/2301, best error 5.826371 %, best_auc: 0.500000
Optimization complete with best validation score of 5.826371 %, obtained at iteration 2301, with test performance 5.826371 %, and best_auc: 0.500000
runtime: 132339.83 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 86
RunningOn slot6@opt-a005.discovery.wisc.edu
python th_deep_belief_net.py pcba 86
Running Theano Learn Deep Belief Net for pcba, target: aid651768.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.6432456301
Pre-training layer 0, epoch 1, cost  -51.258447945
Pre-training layer 0, epoch 2, cost  -45.5558383723
Pre-training layer 0, epoch 3, cost  -41.9140989748
Pre-training layer 0, epoch 4, cost  -39.2571981896
Pre-training layer 0, epoch 5, cost  -37.146352144
Pre-training layer 0, epoch 6, cost  -35.4023129981
Pre-training layer 0, epoch 7, cost  -33.9456955558
Pre-training layer 1, epoch 0, cost  -1353.27819307
Pre-training layer 1, epoch 1, cost  -1350.00581391
Pre-training layer 1, epoch 2, cost  -1350.00483255
Pre-training layer 1, epoch 3, cost  -1350.00482623
Pre-training layer 1, epoch 4, cost  -1350.0045614
Pre-training layer 1, epoch 5, cost  -1350.00483214
Pre-training layer 1, epoch 6, cost  -1350.0041246
Pre-training layer 1, epoch 7, cost  -1350.00466
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2476/2476, best error 12.180606 %, best_auc: 0.500000
     new best AUC!: 0.501492537313
     new best AUC!: 0.502943715953
Optimization complete with best validation score of 12.180606 %, obtained at iteration 2476, with test performance 12.180606 %, and best_auc: 0.502944
runtime: 88725.23 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 87
RunningOn slot2@opt-a008.discovery.wisc.edu
python th_deep_belief_net.py pcba 87
Running Theano Learn Deep Belief Net for pcba, target: aid651965.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -62.8364136797
Pre-training layer 0, epoch 1, cost  -47.7269551627
Pre-training layer 0, epoch 2, cost  -41.7833402044
Pre-training layer 0, epoch 3, cost  -38.0382993833
Pre-training layer 0, epoch 4, cost  -35.2883381893
Pre-training layer 0, epoch 5, cost  -33.1319303697
Pre-training layer 0, epoch 6, cost  -31.346018643
Pre-training layer 0, epoch 7, cost  -29.8593912153
Pre-training layer 1, epoch 0, cost  -1355.94144945
Pre-training layer 1, epoch 1, cost  -1353.60250076
Pre-training layer 1, epoch 2, cost  -1353.60214849
Pre-training layer 1, epoch 3, cost  -1353.60209657
Pre-training layer 1, epoch 4, cost  -1353.60184577
Pre-training layer 1, epoch 5, cost  -1353.60182083
Pre-training layer 1, epoch 6, cost  -1353.60206295
Pre-training layer 1, epoch 7, cost  -1353.6016262
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3134/3134, best error 36.441571 %, best_auc: 0.500000
Optimization complete with best validation score of 36.444444 %, obtained at iteration 3134, with test performance 36.441571 %, and best_auc: 0.500000
runtime: 136516.54 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 88
RunningOn slot12@wid-006.discovery.wisc.edu
python th_deep_belief_net.py pcba 88
Running Theano Learn Deep Belief Net for pcba, target: aid652025.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.352140371
Pre-training layer 0, epoch 1, cost  -52.265938471
Pre-training layer 0, epoch 2, cost  -46.6071403498
Pre-training layer 0, epoch 3, cost  -43.0354726057
Pre-training layer 0, epoch 4, cost  -40.4136484563
Pre-training layer 0, epoch 5, cost  -38.3567071005
Pre-training layer 0, epoch 6, cost  -36.7007914682
Pre-training layer 0, epoch 7, cost  -35.235921202
Pre-training layer 1, epoch 0, cost  -1352.92480368
Pre-training layer 1, epoch 1, cost  -1349.20640649
Pre-training layer 1, epoch 2, cost  -1349.20562427
Pre-training layer 1, epoch 3, cost  -1349.20554849
Pre-training layer 1, epoch 4, cost  -1349.2053255
Pre-training layer 1, epoch 5, cost  -1349.20571313
Pre-training layer 1, epoch 6, cost  -1349.20512586
Pre-training layer 1, epoch 7, cost  -1349.20521885
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2229/2229, best error 1.898922 %, best_auc: 0.500000
Optimization complete with best validation score of 1.898922 %, obtained at iteration 2229, with test performance 1.898922 %, and best_auc: 0.500000
runtime: 38016.89 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 89
RunningOn slot3@opt-a016.discovery.wisc.edu
python th_deep_belief_net.py pcba 89
Running Theano Learn Deep Belief Net for pcba, target: aid652104.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -60.4984397294
Pre-training layer 0, epoch 1, cost  -45.7698197674
Pre-training layer 0, epoch 2, cost  -40.0700552733
Pre-training layer 0, epoch 3, cost  -36.4346922518
Pre-training layer 0, epoch 4, cost  -33.7782007042
Pre-training layer 0, epoch 5, cost  -31.6783776409
Pre-training layer 0, epoch 6, cost  -30.0084210945
Pre-training layer 0, epoch 7, cost  -28.5436922962
Pre-training layer 1, epoch 0, cost  -1356.31112998
Pre-training layer 1, epoch 1, cost  -1354.3571801
Pre-training layer 1, epoch 2, cost  -1354.35705621
Pre-training layer 1, epoch 3, cost  -1354.357135
Pre-training layer 1, epoch 4, cost  -1354.3567334
Pre-training layer 1, epoch 5, cost  -1354.35694851
Pre-training layer 1, epoch 6, cost  -1354.3567302
Pre-training layer 1, epoch 7, cost  -1354.35693331
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3662/3662, best error 35.023770 %, best_auc: 0.500000
     new best AUC!: 0.500006304137
Optimization complete with best validation score of 35.023770 %, obtained at iteration 3662, with test performance 35.023770 %, and best_auc: 0.500006
runtime: 183134.38 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 9
RunningOn slot6@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 9
Running Theano Learn Deep Belief Net for pcba, target: aid1469.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -69.3996968835
Pre-training layer 0, epoch 1, cost  -55.88972255
Pre-training layer 0, epoch 2, cost  -50.3357458041
Pre-training layer 0, epoch 3, cost  -46.7442886187
Pre-training layer 0, epoch 4, cost  -44.1219183001
Pre-training layer 0, epoch 5, cost  -42.0233695669
Pre-training layer 0, epoch 6, cost  -40.2953273665
Pre-training layer 0, epoch 7, cost  -38.8455016987
Pre-training layer 1, epoch 0, cost  -1348.97425661
Pre-training layer 1, epoch 1, cost  -1343.28967555
Pre-training layer 1, epoch 2, cost  -1343.28508073
Pre-training layer 1, epoch 3, cost  -1343.28462142
Pre-training layer 1, epoch 4, cost  -1343.28512539
Pre-training layer 1, epoch 5, cost  -1343.28440225
Pre-training layer 1, epoch 6, cost  -1343.28449338
Pre-training layer 1, epoch 7, cost  -1343.28466012
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 1687/1687, best error 1.814947 %, best_auc: 0.500000
Optimization complete with best validation score of 1.814947 %, obtained at iteration 1687, with test performance 1.814947 %, and best_auc: 0.500000
runtime: 28399.48 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 90
RunningOn slot3@opt-a002.discovery.wisc.edu
python th_deep_belief_net.py pcba 90
Running Theano Learn Deep Belief Net for pcba, target: aid652105.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -64.9079399399
Pre-training layer 0, epoch 1, cost  -49.9625998846
Pre-training layer 0, epoch 2, cost  -44.0795362811
Pre-training layer 0, epoch 3, cost  -40.3128406455
Pre-training layer 0, epoch 4, cost  -37.5838284987
Pre-training layer 0, epoch 5, cost  -35.4448998888
Pre-training layer 0, epoch 6, cost  -33.6565946806
Pre-training layer 0, epoch 7, cost  -32.1904121113
Pre-training layer 1, epoch 0, cost  -1353.19422887
Pre-training layer 1, epoch 1, cost  -1350.18606004
Pre-training layer 1, epoch 2, cost  -1350.18528408
Pre-training layer 1, epoch 3, cost  -1350.18523512
Pre-training layer 1, epoch 4, cost  -1350.18543641
Pre-training layer 1, epoch 5, cost  -1350.18497296
Pre-training layer 1, epoch 6, cost  -1350.18500474
Pre-training layer 1, epoch 7, cost  -1350.1852463
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2681/2681, best error 27.323628 %, best_auc: 0.500000
     new best AUC!: 0.501228501229
     new best AUC!: 0.501812046404
     new best AUC!: 0.503639336778
Optimization complete with best validation score of 27.322508 %, obtained at iteration 2681, with test performance 27.323628 %, and best_auc: 0.503639
runtime: 318692.37 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 91
RunningOn slot3@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 91
Running Theano Learn Deep Belief Net for pcba, target: aid652106.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -66.0131611787
Pre-training layer 0, epoch 1, cost  -51.9458734424
Pre-training layer 0, epoch 2, cost  -46.3623661983
Pre-training layer 0, epoch 3, cost  -42.775717246
Pre-training layer 0, epoch 4, cost  -40.1587890344
Pre-training layer 0, epoch 5, cost  -38.0836783251
Pre-training layer 0, epoch 6, cost  -36.4017061704
Pre-training layer 0, epoch 7, cost  -34.9783173477
Pre-training layer 1, epoch 0, cost  -1352.05497697
Pre-training layer 1, epoch 1, cost  -1348.37125554
Pre-training layer 1, epoch 2, cost  -1348.37039498
Pre-training layer 1, epoch 3, cost  -1348.37030135
Pre-training layer 1, epoch 4, cost  -1348.37008926
Pre-training layer 1, epoch 5, cost  -1348.37047112
Pre-training layer 1, epoch 6, cost  -1348.36984487
Pre-training layer 1, epoch 7, cost  -1348.37020801
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2299/2299, best error 3.877285 %, best_auc: 0.500000
Optimization complete with best validation score of 3.873368 %, obtained at iteration 2299, with test performance 3.877285 %, and best_auc: 0.500000
runtime: 178555.77 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 92
RunningOn slot3@opt-a003.discovery.wisc.edu
python th_deep_belief_net.py pcba 92
Running Theano Learn Deep Belief Net for pcba, target: aid686970.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -63.5591988656
Pre-training layer 0, epoch 1, cost  -48.7150202888
Pre-training layer 0, epoch 2, cost  -42.8018173717
Pre-training layer 0, epoch 3, cost  -39.049026149
Pre-training layer 0, epoch 4, cost  -36.2543422018
Pre-training layer 0, epoch 5, cost  -34.0598041535
Pre-training layer 0, epoch 6, cost  -32.2756821326
Pre-training layer 0, epoch 7, cost  -30.7387502363
Pre-training layer 1, epoch 0, cost  -1354.07334045
Pre-training layer 1, epoch 1, cost  -1351.6737162
Pre-training layer 1, epoch 2, cost  -1351.67349781
Pre-training layer 1, epoch 3, cost  -1351.67338846
Pre-training layer 1, epoch 4, cost  -1351.67318862
Pre-training layer 1, epoch 5, cost  -1351.6731927
Pre-training layer 1, epoch 6, cost  -1351.67316375
Pre-training layer 1, epoch 7, cost  -1351.67327967
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3222/3222, best error 33.222740 %, best_auc: 0.500000
Optimization complete with best validation score of 33.219012 %, obtained at iteration 3222, with test performance 33.222740 %, and best_auc: 0.500000
runtime: 355077.95 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 93
RunningOn slot4@opt-a003.discovery.wisc.edu
python th_deep_belief_net.py pcba 93
Running Theano Learn Deep Belief Net for pcba, target: aid686978.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -62.4854037419
Pre-training layer 0, epoch 1, cost  -47.7334054091
Pre-training layer 0, epoch 2, cost  -42.0865147974
Pre-training layer 0, epoch 3, cost  -38.4872039742
Pre-training layer 0, epoch 4, cost  -35.8584550709
Pre-training layer 0, epoch 5, cost  -33.760173745
Pre-training layer 0, epoch 6, cost  -32.0708793538
Pre-training layer 0, epoch 7, cost  -30.6386958182
Pre-training layer 1, epoch 0, cost  -1352.51872351
Pre-training layer 1, epoch 1, cost  -1350.52007292
Pre-training layer 1, epoch 2, cost  -1350.51964068
Pre-training layer 1, epoch 3, cost  -1350.51963243
Pre-training layer 1, epoch 4, cost  -1350.51975722
Pre-training layer 1, epoch 5, cost  -1350.51962001
Pre-training layer 1, epoch 6, cost  -1350.51972269
Pre-training layer 1, epoch 7, cost  -1350.51950505
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3997/3997, best error 46.812312 %, best_auc: 0.500000
     new best AUC!: 0.500080160321
     new best AUC!: 0.500120240481
     new best AUC!: 0.500132116231
Optimization complete with best validation score of 46.803303 %, obtained at iteration 3997, with test performance 46.812312 %, and best_auc: 0.500132
runtime: 415977.41 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 94
RunningOn slot13@wid-005.discovery.wisc.edu
python th_deep_belief_net.py pcba 94
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 95
RunningOn slot6@opt-a004.discovery.wisc.edu
python th_deep_belief_net.py pcba 95
Running Theano Learn Deep Belief Net for pcba, target: aid720504.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -60.0021700096
Pre-training layer 0, epoch 1, cost  -45.3736155858
Pre-training layer 0, epoch 2, cost  -39.6423829972
Pre-training layer 0, epoch 3, cost  -35.9676077068
Pre-training layer 0, epoch 4, cost  -33.2982313432
Pre-training layer 0, epoch 5, cost  -31.1606189495
Pre-training layer 0, epoch 6, cost  -29.4228772424
Pre-training layer 0, epoch 7, cost  -27.9452309012
Pre-training layer 1, epoch 0, cost  -1355.26356062
Pre-training layer 1, epoch 1, cost  -1353.3979693
Pre-training layer 1, epoch 2, cost  -1353.39743309
Pre-training layer 1, epoch 3, cost  -1353.39731732
Pre-training layer 1, epoch 4, cost  -1353.39723553
Pre-training layer 1, epoch 5, cost  -1353.39724257
Pre-training layer 1, epoch 6, cost  -1353.39741061
Pre-training layer 1, epoch 7, cost  -1353.397441
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 3953/3953, best error 46.304480 %, best_auc: 0.500000
     new best AUC!: 0.500007064542
Optimization complete with best validation score of 46.299165 %, obtained at iteration 3953, with test performance 46.304480 %, and best_auc: 0.500007
runtime: 427265.27 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 96
RunningOn slot6@opt-a015.discovery.wisc.edu
python th_deep_belief_net.py pcba 96
Running Theano Learn Deep Belief Net for pcba, target: aid720532.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -98.63217982
Pre-training layer 0, epoch 1, cost  -80.20637501
Pre-training layer 0, epoch 2, cost  -75.0515326013
Pre-training layer 0, epoch 3, cost  -71.0925003395
Pre-training layer 0, epoch 4, cost  -67.6491147454
Pre-training layer 0, epoch 5, cost  -64.7718388831
Pre-training layer 0, epoch 6, cost  -62.3638577511
Pre-training layer 0, epoch 7, cost  -60.1887506503
Pre-training layer 1, epoch 0, cost  -1374.15353864
Pre-training layer 1, epoch 1, cost  -1350.56052914
Pre-training layer 1, epoch 2, cost  -1341.43833965
Pre-training layer 1, epoch 3, cost  -1338.59281478
Pre-training layer 1, epoch 4, cost  -1337.49451136
Pre-training layer 1, epoch 5, cost  -1336.95946709
Pre-training layer 1, epoch 6, cost  -1336.63307286
Pre-training layer 1, epoch 7, cost  -1336.40252673
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 169/169, best error 45.892857 %, best_auc: 0.558607
     new best AUC!: 0.559625334996
     epoch 2, minibatch 169/169, best error 44.035714 %, best_auc: 0.559625
     new best AUC!: 0.56318270668
     new best AUC!: 0.565278035751
     new best AUC!: 0.568102174932
     epoch 5, minibatch 169/169, best error 42.607143 %, best_auc: 0.568102
     new best AUC!: 0.569536798719
     epoch 6, minibatch 169/169, best error 43.178571 %, best_auc: 0.569537
     new best AUC!: 0.570436313141
     new best AUC!: 0.571820521665
     epoch 8, minibatch 169/169, best error 43.071429 %, best_auc: 0.571821
     new best AUC!: 0.573424080806
     new best AUC!: 0.575874085671
     epoch 10, minibatch 169/169, best error 42.857143 %, best_auc: 0.575874
     new best AUC!: 0.579905537719
     epoch 11, minibatch 169/169, best error 42.625000 %, best_auc: 0.579906
     new best AUC!: 0.581739061215
     epoch 12, minibatch 169/169, best error 42.339286 %, best_auc: 0.581739
     new best AUC!: 0.584174029948
     epoch 13, minibatch 169/169, best error 42.410714 %, best_auc: 0.584174
     new best AUC!: 0.587041508566
     epoch 14, minibatch 169/169, best error 42.464286 %, best_auc: 0.587042
     new best AUC!: 0.589167794376
     new best AUC!: 0.591570037413
     new best AUC!: 0.593547730871
     new best AUC!: 0.595444936804
     new best AUC!: 0.597007809943
     new best AUC!: 0.600091101264
     new best AUC!: 0.602243036945
     epoch 21, minibatch 169/169, best error 41.500000 %, best_auc: 0.602243
     new best AUC!: 0.605118475867
     epoch 22, minibatch 169/169, best error 41.178571 %, best_auc: 0.605118
     new best AUC!: 0.607935539222
     epoch 23, minibatch 169/169, best error 40.750000 %, best_auc: 0.607936
     new best AUC!: 0.611095780154
     new best AUC!: 0.612828473125
     new best AUC!: 0.61610634967
     epoch 26, minibatch 169/169, best error 39.285714 %, best_auc: 0.616106
     new best AUC!: 0.618681065973
     epoch 27, minibatch 169/169, best error 39.125000 %, best_auc: 0.618681
     new best AUC!: 0.62239233688
     epoch 28, minibatch 169/169, best error 38.714286 %, best_auc: 0.622392
     new best AUC!: 0.626016928914
     epoch 29, minibatch 169/169, best error 38.285714 %, best_auc: 0.626017
     new best AUC!: 0.629712279212
     epoch 30, minibatch 169/169, best error 38.089286 %, best_auc: 0.629712
     new best AUC!: 0.633532340949
     epoch 31, minibatch 169/169, best error 37.160714 %, best_auc: 0.633532
     new best AUC!: 0.638334173588
     epoch 32, minibatch 169/169, best error 36.821429 %, best_auc: 0.638334
     new best AUC!: 0.642376239375
     new best AUC!: 0.647280671496
     new best AUC!: 0.651859615606
     epoch 35, minibatch 169/169, best error 35.160714 %, best_auc: 0.651860
     new best AUC!: 0.656649065549
     epoch 36, minibatch 169/169, best error 34.875000 %, best_auc: 0.656649
     new best AUC!: 0.660914904344
     epoch 37, minibatch 169/169, best error 34.571429 %, best_auc: 0.660915
     new best AUC!: 0.665810491681
     epoch 38, minibatch 169/169, best error 34.553571 %, best_auc: 0.665810
     new best AUC!: 0.670484959447
     epoch 39, minibatch 169/169, best error 34.339286 %, best_auc: 0.670485
     new best AUC!: 0.675039138164
     epoch 40, minibatch 169/169, best error 34.000000 %, best_auc: 0.675039
     new best AUC!: 0.679964797764
     epoch 41, minibatch 169/169, best error 34.071429 %, best_auc: 0.679965
     new best AUC!: 0.684646341356
     new best AUC!: 0.689997435013
     epoch 43, minibatch 169/169, best error 33.500000 %, best_auc: 0.689997
     new best AUC!: 0.695860641601
     epoch 44, minibatch 169/169, best error 32.785714 %, best_auc: 0.695861
     new best AUC!: 0.702018379459
     epoch 45, minibatch 169/169, best error 32.857143 %, best_auc: 0.702018
     new best AUC!: 0.708295521886
     epoch 46, minibatch 169/169, best error 32.696429 %, best_auc: 0.708296
     new best AUC!: 0.714644307055
     epoch 47, minibatch 169/169, best error 32.214286 %, best_auc: 0.714644
     new best AUC!: 0.7214300245
     epoch 48, minibatch 169/169, best error 31.928571 %, best_auc: 0.721430
     new best AUC!: 0.728829569878
     epoch 49, minibatch 169/169, best error 30.964286 %, best_auc: 0.728830
     new best AUC!: 0.736973845977
     epoch 50, minibatch 169/169, best error 30.946429 %, best_auc: 0.736974
     new best AUC!: 0.74692157331
     epoch 51, minibatch 169/169, best error 30.000000 %, best_auc: 0.746922
     new best AUC!: 0.757293850222
     epoch 52, minibatch 169/169, best error 29.892857 %, best_auc: 0.757294
     new best AUC!: 0.76762190322
     epoch 53, minibatch 169/169, best error 28.660714 %, best_auc: 0.767622
     new best AUC!: 0.777399810722
     epoch 54, minibatch 169/169, best error 28.571429 %, best_auc: 0.777400
     new best AUC!: 0.785161992199
     epoch 55, minibatch 169/169, best error 27.732143 %, best_auc: 0.785162
     new best AUC!: 0.79239437118
     new best AUC!: 0.798823643874
     epoch 57, minibatch 169/169, best error 26.267857 %, best_auc: 0.798824
     new best AUC!: 0.804486073889
     new best AUC!: 0.809363087183
     epoch 59, minibatch 169/169, best error 25.589286 %, best_auc: 0.809363
     new best AUC!: 0.813456452711
     new best AUC!: 0.816511440727
     new best AUC!: 0.818126498085
     new best AUC!: 0.819309045559
     new best AUC!: 0.820393415944
     epoch 64, minibatch 169/169, best error 26.232143 %, best_auc: 0.820393
     new best AUC!: 0.820857767046
     epoch 65, minibatch 169/169, best error 26.946429 %, best_auc: 0.820858
     new best AUC!: 0.821514049938
     new best AUC!: 0.822554196407
     epoch 67, minibatch 169/169, best error 26.625000 %, best_auc: 0.822554
     new best AUC!: 0.823681906228
     epoch 68, minibatch 169/169, best error 26.285714 %, best_auc: 0.823682
     new best AUC!: 0.824191365723
     new best AUC!: 0.824779543786
     epoch 70, minibatch 169/169, best error 25.946429 %, best_auc: 0.824780
     new best AUC!: 0.824849417571
     new best AUC!: 0.824990049619
     epoch 73, minibatch 169/169, best error 25.571429 %, best_auc: 0.824990
Optimization complete with best validation score of 26.196429 %, obtained at iteration 12337, with test performance 25.571429 %, and best_auc: 0.824990
runtime: 58448.53 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 97
RunningOn slot2@roy-exec-1.discovery.wisc.edu
python th_deep_belief_net.py pcba 97
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 98
RunningOn slot5@opt-a001.discovery.wisc.edu
python th_deep_belief_net.py pcba 98
Running Theano Learn Deep Belief Net for pcba, target: aid720551.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -65.297855704
Pre-training layer 0, epoch 1, cost  -50.9358045304
Pre-training layer 0, epoch 2, cost  -45.2653323989
Pre-training layer 0, epoch 3, cost  -41.6612548839
Pre-training layer 0, epoch 4, cost  -39.019793568
Pre-training layer 0, epoch 5, cost  -36.9389727116
Pre-training layer 0, epoch 6, cost  -35.2280692629
Pre-training layer 0, epoch 7, cost  -33.8058165268
Pre-training layer 1, epoch 0, cost  -1353.81626088
Pre-training layer 1, epoch 1, cost  -1350.2876263
Pre-training layer 1, epoch 2, cost  -1350.28649656
Pre-training layer 1, epoch 3, cost  -1350.2865463
Pre-training layer 1, epoch 4, cost  -1350.28629893
Pre-training layer 1, epoch 5, cost  -1350.28663613
Pre-training layer 1, epoch 6, cost  -1350.2861726
Pre-training layer 1, epoch 7, cost  -1350.2861769
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2282/2282, best error 9.971053 %, best_auc: 0.500000
Optimization complete with best validation score of 9.982895 %, obtained at iteration 2282, with test performance 9.971053 %, and best_auc: 0.500000
runtime: 227265.92 secs.
_CONDOR_JOB_IWD /mnt/ws/home/jferiante
Cluster 14407
Process 99
RunningOn slot10@wid-001.discovery.wisc.edu
python th_deep_belief_net.py pcba 99
Running Theano Learn Deep Belief Net for pcba, target: aid720553.........
... building the model
... getting the pretraining functions
... pre-training the model
Pre-training layer 0, epoch 0, cost  -63.6205797408
Pre-training layer 0, epoch 1, cost  -48.8336627399
Pre-training layer 0, epoch 2, cost  -43.0755305553
Pre-training layer 0, epoch 3, cost  -39.4337127384
Pre-training layer 0, epoch 4, cost  -36.7699396579
Pre-training layer 0, epoch 5, cost  -34.6862258744
Pre-training layer 0, epoch 6, cost  -32.975221095
Pre-training layer 0, epoch 7, cost  -31.5451733466
Pre-training layer 1, epoch 0, cost  -1354.53586169
Pre-training layer 1, epoch 1, cost  -1351.57165424
Pre-training layer 1, epoch 2, cost  -1351.57075483
Pre-training layer 1, epoch 3, cost  -1351.57071301
Pre-training layer 1, epoch 4, cost  -1351.57092487
Pre-training layer 1, epoch 5, cost  -1351.57074262
Pre-training layer 1, epoch 6, cost  -1351.5705839
Pre-training layer 1, epoch 7, cost  -1351.57094868
... getting the finetuning functions
... finetuning the model
     epoch 1, minibatch 2619/2619, best error 22.371560 %, best_auc: 0.500000
Optimization complete with best validation score of 22.365826 %, obtained at iteration 2619, with test performance 22.371560 %, and best_auc: 0.500000
runtime: 40047.69 secs.
